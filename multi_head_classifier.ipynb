{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047819a2-1f2b-454d-80cb-2485193d5d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m fatal: not a git repository (or any of the parent directories): .git\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-install-lxb4501z/flash-attn_dbc0ba382e294b448d755348aa564708/setup.py:99: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-lxb4501z/flash-attn_dbc0ba382e294b448d755348aa564708/setup.py\", line 177, in <module>\n",
      "  \u001b[31m   \u001b[0m     CUDAExtension(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1076, in CUDAExtension\n",
      "  \u001b[31m   \u001b[0m     library_dirs += library_paths(cuda=True)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 1207, in library_paths\n",
      "  \u001b[31m   \u001b[0m     if (not os.path.exists(_join_cuda_home(lib_dir)) and\n",
      "  \u001b[31m   \u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ubuntu/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 2416, in _join_cuda_home\n",
      "  \u001b[31m   \u001b[0m     raise OSError('CUDA_HOME environment variable is not set. '\n",
      "  \u001b[31m   \u001b[0m OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.4.1+cu121\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cusparse-cu12 (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/home/ubuntu/.local/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q install \"torch==2.4.1\" tensorboard \n",
    "!pip -q install flash-attn \"setuptools<71.0.0\" scikit-learn \n",
    "!pip -q install --upgrade torch torchvision\n",
    "!pip -q install wandb\n",
    "!pip -q install matplotlib\n",
    "\n",
    "!pip -q install  --upgrade \\\n",
    "  \"datasets==3.1.0\" \\\n",
    "  \"accelerate==1.2.1\" \\\n",
    "  \"hf-transfer==0.1.8\" \\\n",
    "  \"transformers==4.47.1\" \\\n",
    " \n",
    "# ModernBERT is not yet available in an official release, so we need to install it from github\n",
    "!pip -q install \"git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07dcf9ef-15ac-433f-9c20-c4f3a0901869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from huggingface_hub import HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e03f70-aec1-4fd0-80f2-551c9225780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(token=\"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\", add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7632028b-b037-43c3-a9c0-380772b25f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 3270\n",
      "Dataset({\n",
      "    features: ['doc', 'doc_hash', 'doc_id', 'filter', 'filtered_resps', 'metrics', 'prompt_hash', 'resps', 'target', 'target_hash', 'input_text', 'input_text_modern_bert_embed', 'input_text_modern_bert_pca_{pca_dim}_dims', 'arguments', 'meta-llama__Llama-3.2-1B-Instruct_chosen', 'meta-llama__Llama-3.2-3B-Instruct_chosen', 'meta-llama__Llama-3.1-8B-Instruct_chosen', 'meta-llama__Llama-3.1-70B-Instruct_chosen', 'meta-llama__Llama-3.3-70B-Instruct_chosen', 'chosen_model'],\n",
      "    num_rows: 3270\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"datasets/modern-bert-embeddings/boolq_embeds.csv\")\n",
    "\n",
    "print(f\"Dataset size: {len(raw_dataset['train'])}\")\n",
    "print(raw_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827f3f26-0a7b-4f29-a77e-45ec09a37370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Value, Dataset, DatasetDict\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.model_max_length = 1024\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch['input_text'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "raw_dataset = raw_dataset.rename_column(\"meta-llama__Llama-3.2-1B-Instruct_chosen\", \"labels_0\")\n",
    "raw_dataset = raw_dataset.rename_column(\"meta-llama__Llama-3.1-8B-Instruct_chosen\", \"labels_1\")\n",
    "raw_dataset = raw_dataset.rename_column(\"meta-llama__Llama-3.2-3B-Instruct_chosen\", \"labels_2\")\n",
    "raw_dataset = raw_dataset.rename_column(\"meta-llama__Llama-3.1-70B-Instruct_chosen\", \"labels_3\")\n",
    "raw_dataset = raw_dataset.rename_column(\"meta-llama__Llama-3.3-70B-Instruct_chosen\", \"labels_4\")\n",
    "\n",
    "raw_dataset = raw_dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_dataset = raw_dataset.map(tokenize, batched=True, remove_columns=[\"input_text\"])\n",
    "\n",
    "for col in [\"labels_0\", \"labels_1\", \"labels_2\", \"labels_3\", \"labels_4\"]:\n",
    "    tokenized_dataset = tokenized_dataset.cast_column(col, Value(\"int64\"))\n",
    "\n",
    "def balance_all_label_cols_union(dataset, label_cols):\n",
    "    df = dataset.to_pandas()\n",
    "    if \"input_ids\" in df.columns:\n",
    "        df[\"input_ids\"] = df[\"input_ids\"].apply(tuple)\n",
    "    if \"attention_mask\" in df.columns:\n",
    "        df[\"attention_mask\"] = df[\"attention_mask\"].apply(tuple)\n",
    "    balanced_dfs = []\n",
    "    for col in label_cols:\n",
    "        m = df[df[col] == 1]\n",
    "        n = df[df[col] == 0]\n",
    "        if len(m) > len(n):\n",
    "            n_up = resample(n, replace=True, n_samples=len(m), random_state=42)\n",
    "            b = pd.concat([m, n_up], ignore_index=True)\n",
    "        else:\n",
    "            m_up = resample(m, replace=True, n_samples=len(n), random_state=42)\n",
    "            b = pd.concat([n, m_up], ignore_index=True)\n",
    "        balanced_dfs.append(b)\n",
    "    u = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    u.drop_duplicates(inplace=True)\n",
    "    u = u.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return Dataset.from_pandas(u)\n",
    "\n",
    "label_cols = [\"labels_0\",\"labels_1\",\"labels_2\",\"labels_3\",\"labels_4\"]\n",
    "balanced_train = balance_all_label_cols_union(tokenized_dataset[\"train\"], label_cols)\n",
    "balanced_test  = balance_all_label_cols_union(tokenized_dataset[\"test\"], label_cols)\n",
    "\n",
    "tokenized_dataset = DatasetDict({\"train\": balanced_train, \"test\": balanced_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "759977d2-e42b-48f3-8391-b861b1aeff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in balanced train: {'train': ['doc', 'doc_hash', 'doc_id', 'filter', 'filtered_resps', 'metrics', 'prompt_hash', 'resps', 'target', 'target_hash', 'input_text_modern_bert_embed', 'input_text_modern_bert_pca_{pca_dim}_dims', 'arguments', 'labels_0', 'labels_2', 'labels_1', 'labels_3', 'labels_4', 'chosen_model', 'input_ids', 'attention_mask'], 'test': ['doc', 'doc_hash', 'doc_id', 'filter', 'filtered_resps', 'metrics', 'prompt_hash', 'resps', 'target', 'target_hash', 'input_text_modern_bert_embed', 'input_text_modern_bert_pca_{pca_dim}_dims', 'arguments', 'labels_0', 'labels_2', 'labels_1', 'labels_3', 'labels_4', 'chosen_model', 'input_ids', 'attention_mask']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in balanced train:\", tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d715a9bc-fa9c-4711-8331-0d851df3daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['labels_0', 'labels_2', 'labels_1', 'labels_3', 'labels_4', 'input_ids', 'attention_mask']\n",
      "Test columns: ['labels_0', 'labels_2', 'labels_1', 'labels_3', 'labels_4', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = [\n",
    "    \"input_ids\",\n",
    "    \"attention_mask\",\n",
    "    \"labels_0\", \"labels_1\", \"labels_2\", \"labels_3\", \"labels_4\"\n",
    "]\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "    [col for col in tokenized_dataset[\"train\"].column_names if col not in columns_to_keep]\n",
    ")\n",
    "\n",
    "print(\"Train columns:\", tokenized_dataset[\"train\"].column_names)\n",
    "print(\"Test columns:\", tokenized_dataset[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c7926e-bcea-4c57-a3e1-45ddbaae65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution for labels_0:\n",
      "labels_0\n",
      "1    1793\n",
      "0     823\n",
      "Name: count, dtype: int64\n",
      "Test label distribution for labels_0:\n",
      "labels_0\n",
      "1    469\n",
      "0    185\n",
      "Name: count, dtype: int64\n",
      "Train label distribution for labels_1:\n",
      "labels_1\n",
      "1    2197\n",
      "0     419\n",
      "Name: count, dtype: int64\n",
      "Test label distribution for labels_1:\n",
      "labels_1\n",
      "1    553\n",
      "0    101\n",
      "Name: count, dtype: int64\n",
      "Train label distribution for labels_2:\n",
      "labels_2\n",
      "1    2034\n",
      "0     582\n",
      "Name: count, dtype: int64\n",
      "Test label distribution for labels_2:\n",
      "labels_2\n",
      "1    529\n",
      "0    125\n",
      "Name: count, dtype: int64\n",
      "Train label distribution for labels_3:\n",
      "labels_3\n",
      "1    2292\n",
      "0     324\n",
      "Name: count, dtype: int64\n",
      "Test label distribution for labels_3:\n",
      "labels_3\n",
      "1    579\n",
      "0     75\n",
      "Name: count, dtype: int64\n",
      "Train label distribution for labels_4:\n",
      "labels_4\n",
      "1    2312\n",
      "0     304\n",
      "Name: count, dtype: int64\n",
      "Test label distribution for labels_4:\n",
      "labels_4\n",
      "1    589\n",
      "0     65\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = tokenized_dataset[\"train\"].to_pandas()\n",
    "df_test  = tokenized_dataset[\"test\"].to_pandas()\n",
    "\n",
    "for label_col in [\"labels_0\", \"labels_1\", \"labels_2\", \"labels_3\", \"labels_4\"]:\n",
    "    print(f\"Train label distribution for {label_col}:\")\n",
    "    print(df_train[label_col].value_counts())\n",
    "\n",
    "    print(f\"Test label distribution for {label_col}:\")\n",
    "    print(df_test[label_col].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050581f-604f-45a7-9601-98b4ccc46c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultiHeadCollator(DataCollatorWithPadding):\n",
    "    \"\"\"\n",
    "    Collator that pulls out labels_0..labels_4 into a single tensor [batch_size, 5].\n",
    "    \"\"\"\n",
    "    def __call__(self, features):\n",
    "        # 1) Gather labels from features\n",
    "        labels_0 = [f[\"labels_0\"] for f in features]\n",
    "        labels_1 = [f[\"labels_1\"] for f in features]\n",
    "        labels_2 = [f[\"labels_2\"] for f in features]\n",
    "        labels_3 = [f[\"labels_3\"] for f in features]\n",
    "        labels_4 = [f[\"labels_4\"] for f in features]\n",
    "\n",
    "        # 2) Remove them from `features` so they won't become extra kwargs\n",
    "        for f in features:\n",
    "            for lbl_col in [\"labels_0\",\"labels_1\",\"labels_2\",\"labels_3\",\"labels_4\"]:\n",
    "                if lbl_col in f:\n",
    "                    del f[lbl_col]\n",
    "\n",
    "        # 3) Let the parent collator handle input_ids etc.\n",
    "        batch = super().__call__(features)\n",
    "\n",
    "        # 4) Create one \"labels\" field => shape [batch_size, 5]\n",
    "        batch[\"labels\"] = torch.tensor([labels_0, labels_1, labels_2, labels_3, labels_4], dtype=torch.long).T\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156d253-6a05-40ab-ad5b-457bc0593b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MultiHeadCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5605e7-61f8-49c4-b177-b2ec6525699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Each 'head' will be an instance of this.\"\"\"\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15dbdd-dc05-4b4a-9486-dc03afb41dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernBERTMultiHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single model with a ModernBERT backbone + 5 separate heads (MLP).\n",
    "    We'll do binary classification (num_labels=2) for each head.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_id, num_heads=5, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # 1) Load the *encoder* from \"answerdotai/ModernBERT-base\".\n",
    "        #    Using AutoModel instead of AutoModelForSequenceClassification,\n",
    "        #    because we want direct access to the hidden states.\n",
    "        self.backbone = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "        # 2) Freeze the entire backbone \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 3) Create one MLP for each head\n",
    "        self.heads = nn.ModuleList([\n",
    "            MLP(hidden_size=self.backbone.config.hidden_size, num_labels=num_labels)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels=None,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          labels: shape [batch_size, 5] for 5 separate binary classifications\n",
    "        Returns:\n",
    "          dict(loss=..., logits=...) \n",
    "            where logits has shape [batch_size, 5, num_labels]\n",
    "        \"\"\"\n",
    "        if \"num_items_in_batch\" in kwargs:\n",
    "            kwargs.pop(\"num_items_in_batch\")\n",
    "            \n",
    "        # 1) Run the shared backbone\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        # outputs.last_hidden_state => [batch_size, seq_len, hidden_size]\n",
    "        # We'll use the [CLS] token's embedding\n",
    "        pooled_cls = outputs.last_hidden_state[:, 0, :]  # shape [batch_size, hidden_size]\n",
    "\n",
    "        # 2) Pass it to each head\n",
    "        # We'll collect each head's logits => shape [batch_size, num_labels]\n",
    "        logits_list = []\n",
    "        for head_idx in range(self.num_heads):\n",
    "            head_logits = self.heads[head_idx](pooled_cls)\n",
    "            logits_list.append(head_logits)\n",
    "\n",
    "        # Stack them along axis=1 => shape [batch_size, 5, num_labels]\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # We do a separate cross-entropy per head, then average\n",
    "            # labels => shape [batch_size, 5]\n",
    "            # logits => [batch_size, 5, num_labels]\n",
    "            # We'll loop over heads => head_logits => [batch_size, num_labels]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            total_loss = 0.\n",
    "            for i in range(self.num_heads):\n",
    "                total_loss += loss_fct(logits[:, i, :], labels[:, i])\n",
    "            loss = total_loss / self.num_heads\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29765fbc-5fbc-4052-95b9-b40950c5e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_model = ModernBERTMultiHead(\n",
    "    model_id=model_id,\n",
    "    num_heads=5, \n",
    "    num_labels=2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a1d5f-9d2c-4a58-a4fa-c8df5f8f01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [name for name, p in multi_head_model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d49b1-0d78-4306-b427-08b3e329e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    eval_pred: (predictions, labels)\n",
    "      - predictions shape: [batch_size, 5, 2]\n",
    "      - labels shape: [batch_size, 5]\n",
    "    We'll compute accuracy & F1 per head, and maybe an average.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    metrics = {}\n",
    "    num_heads = predictions.shape[1]  # 5\n",
    "    for i in range(num_heads):\n",
    "        # For head i => predictions[:, i, :]\n",
    "        preds_head = np.argmax(predictions[:, i, :], axis=1)\n",
    "        true_head = labels[:, i]\n",
    "\n",
    "        head_acc = accuracy_score(true_head, preds_head)\n",
    "        head_f1  = f1_score(true_head, preds_head, average=\"weighted\")\n",
    "\n",
    "        metrics[f\"accuracy_head_{i}\"] = head_acc\n",
    "        metrics[f\"f1_head_{i}\"] = head_f1\n",
    "\n",
    "        cm = confusion_matrix(true_head, preds_head)\n",
    "        print(f\"Confusion matrix for head {i}:\\n{cm}\")\n",
    "        wandb.log({\n",
    "            f\"confusion_matrix_head_{i}\": wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=true_head,\n",
    "                preds=preds_head,\n",
    "                class_names=[\"Neg\", \"Pos\"]\n",
    "            )\n",
    "        })\n",
    "\n",
    "    # average across all 5 heads\n",
    "    avg_acc = np.mean([metrics[f\"accuracy_head_{i}\"] for i in range(num_heads)])\n",
    "    avg_f1  = np.mean([metrics[f\"f1_head_{i}\"] for i in range(num_heads)])\n",
    "    metrics[\"avg_accuracy\"] = float(avg_acc)\n",
    "    metrics[\"avg_f1\"] = float(avg_f1)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef1aa3-e5d2-4cd6-8438-30e6b7310979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"modernbert-llm-router-multihead\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_hf\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model=\"avg_f1\",  \n",
    "    report_to=\"wandb\",\n",
    "    push_to_hub=True,\n",
    "    hub_strategy=\"every_save\",\n",
    "    hub_token=HfFolder.get_token(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=multi_head_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,  \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4d4c3-871a-4521-8c85-15a44931df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"modernbert-llm-router\",\n",
    "    name=\"experiment-1\",\n",
    "    config=training_args.to_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8aa5c-e1d6-47a9-97a2-5e5d0bc1310b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfba125-a2ce-4788-b077-6e0442f49eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c856136-874c-4b91-961e-6992a706b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"modernbert-llm-router\")\n",
    "trainer.create_model_card()\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c968e-934a-47f3-b5ff-ed439f7739e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92732379-e857-4e09-b3f6-a64f91861ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
