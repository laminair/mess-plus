benchmark: social_iqa

# You need to provide models in ascending order by parameter count for MESS+ to work correctly.
model_zoo:
  meta-llama/Llama-3.2-1B-Instruct:
    category: "small"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.12
    quantization: null
#  meta-llama/Llama-3.2-3B-Instruct:
#    category: "small"
#    gpu_indices: [0]
#    max_seq_len: 2048
#    gpu_memory_utilization: 0.15
#    quantization: null
  unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit:
    category: "medium"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.15
    quantization: "bitsandbytes"
  unsloth/Llama-3.3-70B-Instruct-bnb-4bit:
    category: "large"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.53
    quantization: "bitsandbytes"

classifier_model:
  model_id: "answerdotai/ModernBERT-base"
  epochs: 5
  learning_rate: 0.0542
  weight_decay: 0.01
  momentum: 0.95
  batch_size: 64
  max_length: 64
  warmup_ratio: 0.1
  threshold: 0.5
  dropout_rate: 0.1
  freeze_bert_layers: true
  memory_size: 0
  memory_strategy: random # This only works when memory_size > 0
  reset_optimizer: false
  regularization_lambda: 0.0
  gpu_index: 0
  disable_tqdm: true
  approach: "online"
  # Path uses the "classifier" folder as root. Alternatively you can specify an absolute path.
  checkpoint_path: "checkpoints/social_iqa"
  use_pretrained_classifier: false
  scoring_method: raw
  validation_dataset_size: 0.1

# The "empirical" config will take effect when running the "main.py" script and doing actual inference on the LLM zoo.
empirical:
  seed: 42 # Can also be a list, if you want to capture multiple runs.
  alpha: 0.44
  c: 0.1
  V: 0.001
  feedback_sparsity: 1.0
  # Use the option below to capture the performance of all models in the zoo and use them with the simulator.
  # Important: Disables the decision-making process of MESS+ and can be used to generate data for the simulator.
  write_benchmark_data_to_disk: false

# The "simulated" config will take effect when running the "simulator.py" script and using previously generated results.
simulated:
  seed: [42, 43, 44]
  alpha_values: [0.42, 0.44, 0.46]
  c_values: [0.1] # [0.01, 0.1, 1.0]
  v_values: [0.000001, 0.00001, 0.0001] # [0.00001, 0.001, 0.01]
  feedback_sparsity: 1.0

lm_eval:
  num_repeats: 1
  limit_num_samples: null
  enforce_eager: true
  write_to_disk: false
