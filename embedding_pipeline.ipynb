{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a4fef8-9e57-4a6f-a4c7-92622f06f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efd7f60-80cc-4e29-a157-66fc1d94800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c4c022-39b6-445f-af12-60c64c9a395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NUM_SAMPLES = 1000\n",
    "MINIBATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9fe683-0b46-455b-8ff9-2f3438d9ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b2192f-8188-416d-b80e-82736acaef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('wmt14', 'de-en', split='train')\n",
    "dataset = dataset.select(range(NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2140a45e-6df3-4db5-b66e-d6d360fccb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(sentence_pair, tokenizer: AutoTokenizer = None):\n",
    "    return tokenizer(\n",
    "        sentence_pair[\"translation\"][\"de\"],        \t\n",
    "        truncation=True,   \n",
    "        padding='max_length',\n",
    "        return_tensors='pt',  \t\n",
    "        add_special_tokens=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# dataset = dataset.map(lambda sentence_pair: tokenize_data(sentence_pair, tokenizer=tokenizer), remove_columns=[\"translation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4fa622-0498-45cb-962b-8de07d649f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings Shape: torch.Size([64, 512, 768])\n",
      "PCA Embeddings Shape: tensor([[33581.3203,  4239.0229,   922.3452,  ...,   109.7223,    99.1819,\n",
      "            97.9879],\n",
      "        [51179.2930,  7132.0757,  1350.3467,  ...,   166.1138,   160.5107,\n",
      "           157.8768],\n",
      "        [41247.7383,  8174.0542,  1336.0531,  ...,   169.6024,   164.6267,\n",
      "           156.4999],\n",
      "        ...,\n",
      "        [40624.5977,  8774.3887,  1185.7632,  ...,   147.7459,   145.5760,\n",
      "           135.4269],\n",
      "        [36260.4844,  7496.4736,  1036.7146,  ...,   120.5133,   107.6837,\n",
      "            96.1345],\n",
      "        [29509.9902,  5759.0776,   993.5135,  ...,   120.3719,   110.7224,\n",
      "           106.2065]], device='cuda:0')\n",
      "Document Embeddings Shape: torch.Size([64, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_data = [data_point[\"translation\"][\"de\"] for data_point in dataset]\n",
    "tokenized_data = tokenizer.batch_encode_plus(\n",
    "    input_data,        \t\n",
    "    truncation=True,   \n",
    "    padding='max_length',\n",
    "    return_tensors='pt',  \t\n",
    "    add_special_tokens=True, \n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "input_ids_chunked = torch.split(tokenized_data[\"input_ids\"], MINIBATCH_SIZE)\n",
    "attn_mask_chunked = torch.split(tokenized_data[\"attention_mask\"], MINIBATCH_SIZE)\n",
    "\n",
    "for input_ids, attn_mask in zip(input_ids_chunked, attn_mask_chunked): \n",
    "    input_ids = input_ids.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, output_hidden_states=True)\n",
    "    \n",
    "    word_embeddings = outputs.hidden_states[-1]\n",
    "\n",
    "    # This is the PCA code. Depending on what we want to do, there are some interesting resources available: \n",
    "    #     - PyTorch PCA: https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html \n",
    "    #     - Discussion on an embedding approximation: https://stackoverflow.com/questions/75796047/how-to-evaluate-the-quality-of-pca-returned-by-torch-pca-lowrank\n",
    "    #     - Batched processing: https://github.com/pytorch/pytorch/issues/99705\n",
    "    U, S, V = torch.pca_lowrank(word_embeddings, q=25, center=True, niter=2)\n",
    "\n",
    "    # TODO: We may have to assess the PCA quality at some point.\n",
    "    \n",
    "    print(f\"Word Embeddings Shape: {word_embeddings.shape}\")\n",
    "    print(f\"PCA Embeddings Shape: {S}\")\n",
    "    # At this point we can either perform an avg_pool (procedure identical to SentenceTransformers) \n",
    "    # or we can run a PCA on the word embeddings (this might be helpful to identify \"interesting\" sequences).\n",
    "    document_embedding = torch.nanmean(word_embeddings, dim=1)\n",
    "    print(f\"Document Embeddings Shape: {document_embedding.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2016a106-bef4-4467-a3a1-d774fdf394e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "# \n",
    "#     encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n",
    "#     \n",
    "#     outputs = model(**encoding, output_hidden_states=True)\n",
    "#     word_embeddings = outputs.hidden_states[-1]  \n",
    "# \n",
    "#     document_embedding = torch.nanmean(word_embeddings,dim = 1)\n",
    "#     hidden_state = torch.nan_to_num(word_embeddings,nan = 0)\n",
    "#     \n",
    "# print(f\"Word Embeddings Shape: {document_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb316d-0935-43bb-8144-1151bb789cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
