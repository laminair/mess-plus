{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18860d56-34a4-44fd-9412-ed4e00a63a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertPreTrainedModel,\n",
    "    DistilBertModel,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e4b9bd-02c8-4882-8243-ad1cfc814c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3760282/2468758156.py\", line 1, in <module>\n",
      "    df = pd.read_csv('data_logs/wmt14_bleu_threshold.csv')\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 457, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/usr/lib/python3/dist-packages/pandas/io/parsers.py\", line 1917, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'data_logs/wmt14_bleu_threshold.csv' does not exist: b'data_logs/wmt14_bleu_threshold.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_logs/wmt14_bleu_threshold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ea519-52bc-49de-9d68-7aefa0d89db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1b = (df['1b'] - df['1b'].min()).astype(int).tolist()\n",
    "labels_3b = (df['3b'] - df['3b'].min()).astype(int).tolist()\n",
    "labels_8b = (df['8b'] - df['8b'].min()).astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529400bd-0c02-4e46-9946-74d08881a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels for 1b:\", set(labels_1b))\n",
    "print(\"Labels for 3b:\", set(labels_3b))\n",
    "print(\"Labels for 8b:\", set(labels_8b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20af639-8dec-4fd0-a85d-160d14a92850",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['input_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607cb3a-7702-414b-b471-f6c7dc6f058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels_texts, val_labels_texts = train_test_split(\n",
    "    texts,\n",
    "    list(zip(labels_1b, labels_3b, labels_8b)),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192385e-5327-49f3-96ca-21ed47a542b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_1b, train_labels_3b, train_labels_8b = zip(*train_labels_texts)\n",
    "val_labels_1b, val_labels_3b, val_labels_8b = zip(*val_labels_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e00571-8af4-405f-8eaf-7403dd626ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45cbb5-919b-4933-baeb-8f809f44029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba7328-2c0d-4e8e-bb1f-6b8b94defcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ea616-a267-4656-89d6-d148a395bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels_list):\n",
    "        self.encodings = encodings\n",
    "        self.labels_list = labels_list  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        labels = torch.tensor([label[idx] for label in self.labels_list], dtype=torch.long)\n",
    "        item['labels'] = labels\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3817f-1d48-49ab-a498-acbad601d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiHeadDataset(\n",
    "    train_encodings,\n",
    "    [list(train_labels_1b), list(train_labels_3b)] # list(train_labels_8b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133818bf-d626-41dc-8e0d-70cbb6cc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MultiHeadDataset(\n",
    "    val_encodings,\n",
    "    [list(val_labels_1b), list(val_labels_3b)] # list(val_labels_8b)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad31cb5-be10-4f61-99f8-ab575bbe5b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertMultiHeadClassification(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels_per_head):\n",
    "        super().__init__(config)\n",
    "        self.num_heads = len(num_labels_per_head)\n",
    "        self.num_labels_per_head = num_labels_per_head\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "\n",
    "        self.classifier_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(config.hidden_size, 128),  \n",
    "                nn.ReLU(),                         \n",
    "                nn.Linear(128, 64),                 \n",
    "                nn.ReLU(),                          \n",
    "                nn.Linear(64, num_labels)          \n",
    "            )\n",
    "            for num_labels in num_labels_per_head\n",
    "        ])\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Get the outputs from DistilBERT backbone\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "        pooled_output = hidden_state[:, 0]  # Take the representation of [CLS] token\n",
    "\n",
    "        # Compute logits for each head\n",
    "        logits = [classifier(pooled_output) for classifier in self.classifier_heads]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = []\n",
    "            # labels: Tensor of shape (batch_size, num_heads)\n",
    "            for i in range(self.num_heads):\n",
    "                # Extract labels for the current head\n",
    "                head_labels = labels[:, i]  # Shape: (batch_size,)\n",
    "                # Compute loss for the current head\n",
    "                losses.append(loss_fct(logits[i], head_labels))\n",
    "            loss = sum(losses) / self.num_heads  # Average the loss over all heads\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fd4af-e269-4f8c-b207-6b21a9462fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransformerMultiHeadClassification(nn.Module):\n",
    "    def __init__(self, model_name, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.sentence_transformer = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.sentence_transformer.config.hidden_size\n",
    "\n",
    "        # self.classifier_heads = nn.ModuleList([\n",
    "        #     nn.Linear(hidden_size, 1)  \n",
    "        #     for _ in range(num_heads)\n",
    "        # ])\n",
    "\n",
    "        # Each head outputs a single logit\n",
    "        self.classifier_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1)  \n",
    "            )\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        # Get model outputs\n",
    "        outputs = self.sentence_transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Perform mean pooling to get sentence embeddings\n",
    "        pooled_output = self.mean_pooling(outputs, attention_mask)\n",
    "\n",
    "        # Compute logits for each head\n",
    "        logits = [head(pooled_output) for head in self.classifier_heads]\n",
    "        logits = torch.stack(logits, dim=1).squeeze(-1)  # Shape: (batch_size, num_heads)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # labels shape: (batch_size, num_heads)\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element contains token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef75feec-f161-4225-b321-8ff04462c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels_per_head = [\n",
    "    max(labels_1b)+1,\n",
    "    max(labels_3b)+1,\n",
    "    # max(labels_8b)+1,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90066e6-d642-4586-9dae-1bb752682be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of classes per head:\", num_labels_per_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e11a5-1a2d-4768-a367-03c1fe6ef2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DistilBertMultiHeadClassification.from_pretrained(\n",
    "#     'distilbert-base-uncased',\n",
    "#     num_labels_per_head=num_labels_per_head\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9bf04-a76f-4f28-b275-a35eae410bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2  \n",
    "\n",
    "model = SentenceTransformerMultiHeadClassification(\n",
    "    model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "    num_heads=num_heads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226ef32-4edb-4d4a-ae11-e60f174e3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Hidden size: {model.sentence_transformer.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ea6db-7403-4ce1-a93f-0da33cf9ff82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for param in model.distilbert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "for param in model.sentence_transformer.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449a4d4-5dfd-4e55-b973-ffa2d972b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    if not isinstance(logits, torch.Tensor):\n",
    "        logits = torch.tensor(logits)\n",
    "\n",
    "    # Apply sigmoid to logits to get probabilities\n",
    "    probs = torch.sigmoid(logits)\n",
    "\n",
    "    # Convert probabilities to binary predictions (threshold at 0.5)\n",
    "    preds = (probs >= 0.5).int().numpy()\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    accuracies = []\n",
    "    # precisions = []\n",
    "    # recalls = []\n",
    "    # f1s = []\n",
    "\n",
    "    num_heads = labels.shape[1]\n",
    "\n",
    "    for i in range(num_heads): \n",
    "        pred = preds[:, i]\n",
    "        label = labels[:, i]\n",
    "\n",
    "        accuracy = accuracy_score(label, pred)\n",
    "        # precision = precision_score(label, pred, zero_division=0)\n",
    "        # recall = recall_score(label, pred, zero_division=0)\n",
    "        # f1 = f1_score(label, pred, zero_division=0)\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        # precisions.append(precision)\n",
    "        # recalls.append(recall)\n",
    "        # f1s.append(f1)\n",
    "\n",
    "    # Compute average metrics across heads\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    # avg_precision = np.mean(precisions)\n",
    "    # avg_recall = np.mean(recalls)\n",
    "    # avg_f1 = np.mean(f1s)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': avg_accuracy,\n",
    "        # 'precision': avg_precision,\n",
    "        # 'recall': avg_recall,\n",
    "        # 'f1': avg_f1\n",
    "    }\n",
    "\n",
    "    # Add per-head metrics\n",
    "    # for i, (acc, prec, rec, f1) in enumerate(zip(accuracies, precisions, recalls, f1s)):\n",
    "    #     metrics[f'accuracy_head_{i+1}'] = acc\n",
    "    #     metrics[f'precision_head_{i+1}'] = prec\n",
    "    #     metrics[f'recall_head_{i+1}'] = rec\n",
    "    #     metrics[f'f1_head_{i+1}'] = f1\n",
    "\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        metrics[f'accuracy_head_{i+1}'] = acc\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de440d7-fc72-43b5-9d80-a06a626eea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./data_logs/sentence_transformer_multihead',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    logging_dir='./data_logs/sentence_transformer_multihead',\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a175f6b-d5d7-41d3-8149-54d851e62345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"MESS+\", name=\"sentence-transformer-multihead-run\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# eval_results = trainer.evaluate(eval_dataset=val_dataset, metric_key_prefix=\"eval\")\n",
    "# print(\"Validation Results:\")\n",
    "# for key, value in eval_results.items():\n",
    "#     if key.startswith(\"eval_accuracy\"):\n",
    "#         print(f\"{key}: {value}\")\n",
    "\n",
    "# train_results = trainer.evaluate(eval_dataset=train_dataset, metric_key_prefix=\"train\")\n",
    "# print(\"Training Results:\")\n",
    "# for key, value in train_results.items():\n",
    "#     if key.startswith(\"train_accuracy\"):\n",
    "#         print(f\"{key}: {value}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b009b4-fa20-487f-99ba-11df13625ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc42829-ef9b-48c4-833b-df423b5fd37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0e6ab-5002-4c4b-8cda-348c862c168d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
