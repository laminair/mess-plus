{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c8fa74-f12a-4983-8dba-8bf42519b18a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:17:24.686352Z",
     "start_time": "2025-01-21T09:17:21.757048Z"
    }
   },
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy.random import binomial\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel, destroy_distributed_environment\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from lm_eval.evaluator_utils import (\n",
    "\tconsolidate_group_results,\n",
    "\tconsolidate_results,\n",
    "\tget_sample_size,\n",
    "\tget_subtask_list,\n",
    "\tget_task_list,\n",
    "\tprepare_print_tasks,\n",
    "\tprint_writeout,\n",
    "\trun_task_tests,\n",
    ")\n",
    "from lm_eval.tasks import (\n",
    "    Task,\n",
    "    TaskManager,\n",
    "    get_task_dict,\n",
    ")\n",
    "\n",
    "from mess_plus import MessPlusAutomaticModelSelector\n",
    "from utils.mess_lm_eval_harness.vllm_v2 import MessLMEvalVLLM\n",
    "from utils.modelling_messplus_classifier import make_mlp\n",
    "from classifier.utils.lit_trainer import MESSPlusTrainer\n",
    "\n",
    "from zeus.monitor import ZeusMonitor\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Optional, Type, Callable\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'mess_plus.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "NUM_GPUS = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58187b71-505e-4319-8632-5d7cb8c3e42c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:17:24.709645Z",
     "start_time": "2025-01-21T09:17:24.699271Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlaBlaBla(object):\n",
    "\n",
    "    def __init__(self, config_file_path: str):\n",
    "        self.config = yaml.safe_load(open(config_file_path, \"r\"))\n",
    "        self.lm_eval_config = self.config[\"lm_eval\"]\n",
    "        self.algorithm_config = self.config[\"algorithm\"]\n",
    "        self.dataset = None\n",
    "        self.input_column_name = None\n",
    "        self.expected_response_column_name = None\n",
    "\n",
    "        self.__warm_up_inference_models()\n",
    "\n",
    "        # Classifier model\n",
    "        # self.__warmup_classifier_model()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Loggers\n",
    "        # When using Zeus, you must disable RAPL CPU monitoring as this will cause the program to fail.\n",
    "        # Change \"True\" to \"False\" in file venv/lib/python3.12/site-packages/zeus/device/cpu/rapl.py (l. 137)\n",
    "        self.measurements = {d[\"category\"]: [] for i, d in self.config[\"model_zoo\"].items()}\n",
    "        self.scores = {d[\"category\"]: [] for i, d in self.config[\"model_zoo\"].items()}\n",
    "        self.energy_monitor = ZeusMonitor(gpu_indices=[i for i in range(NUM_GPUS)], approx_instant_energy=True)\n",
    "\n",
    "        # Algorithm config\n",
    "        # Q is a virtual queue, i.e., we only keep the sum of all violations, no history.\n",
    "        self.Q = 0.0\n",
    "\n",
    "        # LM Eval Config\n",
    "        task_manager = TaskManager(verbosity=\"INFO\")\n",
    "\n",
    "        self.task_dict = get_task_dict(self.lm_eval_config[\"benchmarks\"], task_manager)\n",
    "        self.task_dict = self.__adjust_config(\n",
    "            self.task_dict,\n",
    "            gen_kwargs=self.lm_eval_config[\"gen_kwargs\"] if \"gen_kwargs\" in self.config.keys() else None,\n",
    "            predict_only=False,\n",
    "            num_fewshot=0,\n",
    "            fewshot_random_seed=self.config[\"seed\"]\n",
    "        )\n",
    "\n",
    "        self.eval_tasks = get_task_list(self.task_dict)\n",
    "\n",
    "    def launch(\n",
    "        self,\n",
    "        limit_num_samples: int = None,\n",
    "        cache_requests: bool = False,\n",
    "        rewrite_requests_cache: bool = False,\n",
    "        system_instruction: Optional[str] = None,\n",
    "        apply_chat_template: bool = False,\n",
    "        fewshot_as_multiturn: bool = False,\n",
    "        chat_template: Optional[Callable] = None,\n",
    "        tokenizer_name: str = \"\",\n",
    "        write_out: bool = False,\n",
    "        log_samples: bool = False\n",
    "    ): \n",
    "        if apply_chat_template:\n",
    "            logger.warning(\n",
    "                \"Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.\"\n",
    "            )\n",
    "\n",
    "        # tracks all Instances/requests a model must generate output on.\n",
    "        requests = defaultdict(list)\n",
    "        # stores the amount to pad out reqs per req. type so that\n",
    "        # number of fwd passes per distributed rank is equal\n",
    "        padding_requests = defaultdict(int)\n",
    "    \n",
    "        # get lists of group hierarchy and each type of request\n",
    "        eval_tasks = get_task_list(self.task_dict)\n",
    "        if not log_samples:\n",
    "            if not all(\n",
    "                    \"bypass\" not in getattr(task_output.task, \"_metric_fn_list\", {}).keys()\n",
    "                    for task_output in eval_tasks\n",
    "            ):\n",
    "                raise ValueError(\"log_samples must be True for 'bypass' metric-only tasks\")\n",
    "    \n",
    "        limit_arg = limit_num_samples\n",
    "        limits = []\n",
    "        for task_output in self.eval_tasks:\n",
    "            self.run_benchmark(task_output, limit_arg)\n",
    "        \n",
    "\n",
    "    def run_benchmark(task_output, limit_arg): \n",
    "        task: Task = task_output.task\n",
    "        smallest_model_category, smallest_model_instance = next(iter(self.vllm_models.items()))\n",
    "        smallest_model_instance = smallest_model_instance[\"vllm_eval_instance\"]\n",
    "\n",
    "        limit = get_sample_size(task, limit_arg)\n",
    "        # limits.append(limit)\n",
    "        task.build_all_requests(\n",
    "            limit=limit,\n",
    "            rank=0,\n",
    "            world_size=1,\n",
    "            cache_requests=cache_requests,\n",
    "            rewrite_requests_cache=rewrite_requests_cache,\n",
    "            system_instruction=system_instruction,\n",
    "            apply_chat_template=bool(apply_chat_template),\n",
    "            fewshot_as_multiturn=fewshot_as_multiturn,\n",
    "            chat_template=getattr(smallest_model_instance, \"apply_chat_template\") if apply_chat_template else None,\n",
    "            tokenizer_name=getattr(smallest_model_instance, \"tokenizer_name\", \"\") if apply_chat_template else \"\",\n",
    "        )\n",
    "        logger.debug(\n",
    "            f\"Task: {task_output.task_name}; number of requests on this rank: {len(task.instances)}\"\n",
    "        )\n",
    "        \n",
    "        if write_out:\n",
    "            print_writeout(task)\n",
    "            \n",
    "        # aggregate Instances by LM method requested to get output.\n",
    "        requests = defaultdict(list)\n",
    "        for instance in task.instances:\n",
    "            reqtype = instance.request_type\n",
    "            requests[reqtype].append(instance)\n",
    "\n",
    "        ### Run LM on inputs, get all outputs ###\n",
    "        # execute each type of request        \n",
    "        for reqtype, reqs in requests.items():\n",
    "            logger.info(f\"Processing {reqtype} requests for {task.task_name} dataset with a total of {len(requests[reqtype])} samples.\")\n",
    "            # create `K` copies of each request `req` based off `K = req.repeats`\n",
    "            cloned_reqs = []\n",
    "            for req in reqs:\n",
    "                cloned_reqs.extend([req] * req.repeats)\n",
    "    \n",
    "            display(cloned_reqs[0])\n",
    "    \n",
    "            for idx, request in enumerate(cloned_reqs): \n",
    "                model_resps_mapping = self.run_request(request, timestamp=idx)\n",
    "            \n",
    "                display(model_resps_mapping)\n",
    "    \n",
    "                if idx == 2: \n",
    "                    break\n",
    "\n",
    "    def query_model(model, request, model_category): \n",
    "        self.energy_monitor.begin_window(f\"pass_{model_category}\")\n",
    "        output = getattr(model[\"vllm_eval_instance\"], request.request_type)([request] if type(request) != list else request, disable_tqdm=True)\n",
    "        measurement = self.energy_monitor.end_window(f\"pass_{model_category}\")\n",
    "\n",
    "        model_resps_mapping[model_category] = output\n",
    "        self.measurements[model_category].append(measurement)\n",
    "        self.scores[model_category].append(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def run_request(self, request, c: int = 3, timestamp: int = 0): \n",
    "        x_t = self.__sample_from_bernoulli(c=c, timestamp=timestamp)\n",
    "    \n",
    "        if x_t == 1: \n",
    "            logger.info(f\"Exploring during step {timestamp}.\")\n",
    "            \n",
    "            model_resps_mapping = {i: [] for i in self.vllm_models.keys()}\n",
    "            for model_category, model in self.vllm_models.items():\n",
    "                model_resps_mapping[model_category] = self.query_model(\n",
    "                    model, \n",
    "                    request, \n",
    "                    model_category\n",
    "                )\n",
    "    \n",
    "            # the final response\n",
    "            label = []\n",
    "            for model_category, data in model_resps_mapping.items(): \n",
    "                label.append(0 if sum(map(lambda x: x[1], data)) / len(data) < 0.5 else 1)\n",
    "\n",
    "            label = torch.tensor(label)\n",
    "            response_idx = label[torch.argmax(label)]\n",
    "            model_category = [i for i in self.vllm_models.keys()][choice_idx]\n",
    "            response = label[model_category]\n",
    "        \n",
    "            # Prepare the classifier training environment\n",
    "            # self.__evict_vllm_models()\n",
    "\n",
    "            # Train the classifier\n",
    "            # self.train_classifier(request=request, label=label)\n",
    "\n",
    "            # Need to re-initiate inference models\n",
    "            # self.__warm_up_inference_models()\n",
    "    \n",
    "        else: \n",
    "            logger.info(f\"Estimating during step {timestamp}.\")  \n",
    "\n",
    "            preference_estimation = [0.4, 0.7, 0.85]\n",
    "            preference_estimation = torch.tensor(preference_estimation).reshape(-1, 1)\n",
    "\n",
    "            energy = []\n",
    "            for model_category in selector.measurements.keys(): \n",
    "                energy.append(\n",
    "                    sum(map(lambda x: sum([i for i in x.gpu_energy.values()]) / len(selector.measurements[model_category]), selector.measurements[model_category]))\n",
    "                )\n",
    "            \n",
    "            energy = torch.tensor(energy).view(-1, 1)\n",
    "            # Energy and Preference Estimation are vectors.\n",
    "            cost_fn = self.algorithm_config[\"V\"] * energy + self.Q * (self.algorithm_config[\"alpha\"] - preference_estimation)\n",
    "            choice_idx = torch.argmax(cost_fn)\n",
    "            model_category = [i for i in self.vllm_models.keys()][choice_idx]\n",
    "\n",
    "            # Query the model of choice\n",
    "            response = self.query_model(\n",
    "                model[model_category], \n",
    "                request, \n",
    "                model_category\n",
    "            )\n",
    "\n",
    "        return response, model_category\n",
    "\n",
    "    @staticmethod\n",
    "    def __sample_from_bernoulli(c: float, timestamp: int):\n",
    "\n",
    "        p_t = min(\n",
    "            1.0, c / np.cbrt(1 if timestamp == 0 else timestamp)\n",
    "        )\n",
    "\n",
    "        x_t = binomial(n=1, p=p_t, size=1)\n",
    "\n",
    "        return x_t.item()\n",
    "\n",
    "    def __warm_up_inference_models(self):\n",
    "        self.vllm_models = {}\n",
    "        self.tokenizers = {}\n",
    "\n",
    "        # This is a safeguard so that we do not over-commit GPUs and cause unexpected OOMs.\n",
    "        max_memory_utilization_per_model = 0.45 # min(1.0, torch.cuda.device_count() / len(self.config[\"model_zoo\"].keys()))\n",
    "        logger.info(f\"Found {len(self.config[\"model_zoo\"].keys())} models in zoo: {self.config[\"model_zoo\"].keys()}\")\n",
    "        for model, data in self.config[\"model_zoo\"].items():\n",
    "\n",
    "            if data[\"category\"] not in self.vllm_models.keys():\n",
    "                self.vllm_models[data[\"category\"]] = {}\n",
    "\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(data[\"gpu_indices\"]).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            self.vllm_models[data[\"category\"]] = {\n",
    "                \"model_name\": model,\n",
    "                \"vllm_eval_instance\": MessLMEvalVLLM(\n",
    "                    model,\n",
    "                    max_length=data[\"max_seq_len\"],  # data[\"max_seq_len\"],\n",
    "                    gpu_indices=data[\"gpu_indices\"],\n",
    "                    trust_remote_code=True,\n",
    "                    tensor_parallel_size=len(data[\"gpu_indices\"]),\n",
    "                    gpu_memory_utilization=data[\"gpu_memory_utilization\"], \n",
    "                    seed=42\n",
    "                ),\n",
    "                # \"tokenize\": AutoTokenizer.from_pretrained(model)\n",
    "            }\n",
    "\n",
    "            logger.info(f\"vLLM model {model} loaded on rank {data['gpu_indices']}. Tensor parallel size: {len(data['gpu_indices'])}\")\n",
    "\n",
    "        logger.info(f\"All models loaded.\")\n",
    "\n",
    "    def __warmup_classifier_model(self):\n",
    "        self.classifier_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.config[\"classifier_model\"][\"model_id\"],\n",
    "            num_labels=self.config[\"classifier_model\"][\"num_labels\"]\n",
    "        )\n",
    "\n",
    "        self.__make_mlp_classifier()\n",
    "\n",
    "        self.classifier_tokenizer = AutoTokenizer.from_pretrained(self.config[\"classifier_model\"][\"model_id\"])\n",
    "        self.classifier_tokenizer.model_max_length = self.config[\"classifier_model\"][\"max_seq_len\"]\n",
    "\n",
    "        logger.info(f\"Classification model {self.config['classifier_model']['model_id']} loaded and ready to use.\")\n",
    "\n",
    "    def __evict_vllm_models(self):\n",
    "\n",
    "        destroy_model_parallel()\n",
    "        destroy_distributed_environment()\n",
    "\n",
    "        models = [i for i in self.vllm_models.keys()]\n",
    "\n",
    "        for model_category in models:\n",
    "            del self.vllm_models[model_category][\"vllm_eval_instance\"].model.llm_engine.model_executor\n",
    "            del self.vllm_models[model_category]\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        with contextlib.suppress(AssertionError):\n",
    "            torch.distributed.destroy_process_group()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    @staticmethod\n",
    "    def __prepare_classifier_training_labels(preference_scores: list):\n",
    "        \"\"\"\n",
    "        Returns the index of the smallest model that was able to respond correctly to the question.\n",
    "        :param preference_scores:\n",
    "        :return: Tensor with the smallest model index that was able to solve the question.\n",
    "        \"\"\"\n",
    "        if 1 in preference_scores:\n",
    "            up_to_first_one = preference_scores[:preference_scores.index(1) + 1]\n",
    "            after_first_one = [0 if x == 1 else x for x in preference_scores[preference_scores.index(1) + 1:]]\n",
    "\n",
    "            return up_to_first_one + after_first_one\n",
    "        else:\n",
    "            return preference_scores.copy()\n",
    "\n",
    "    # @staticmethod\n",
    "    # def __append_label_for_human_annotation(preference_scores: list, val: int = 1):\n",
    "    #     # We do not consider human annotation an option at this point.\n",
    "    #     preference_scores += [val]\n",
    "    #     return preference_scores\n",
    "\n",
    "    def __make_mlp_classifier(self):\n",
    "\n",
    "        # We freeze the backbone model parameters\n",
    "        for param in self.classifier_model.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier_model.classifier = make_mlp(\n",
    "            base_model=self.classifier_model,\n",
    "            config=self.config\n",
    "        )\n",
    "\n",
    "        trainable_parameters = filter(lambda p: p.requires_grad, self.classifier_model.parameters())\n",
    "        param_count = sum([np.prod(p.size()) for p in trainable_parameters])\n",
    "\n",
    "        logger.info(f\"Using a classification MLP with {param_count} trainable parameters.\")\n",
    "\n",
    "    def __adjust_config(self, task_dict, gen_kwargs, predict_only, num_fewshot, fewshot_random_seed):\n",
    "        adjusted_task_dict = {}\n",
    "        for task_name, task_obj in task_dict.items():\n",
    "            if isinstance(task_obj, dict):\n",
    "                adjusted_task_dict = {\n",
    "                    **adjusted_task_dict,\n",
    "                    **{task_name: self.__adjust_config(task_obj)},\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                if task_obj.get_config(\"output_type\") == \"generate_until\":\n",
    "                    if gen_kwargs is not None:\n",
    "                        task_obj.set_config(\n",
    "                            key=\"generation_kwargs\", value=gen_kwargs, update=True\n",
    "                        )\n",
    "\n",
    "                if predict_only:\n",
    "                    logger.info(\n",
    "                        f\"Processing {task_name} in output-only mode. Metrics will not be calculated!\"\n",
    "                    )\n",
    "                    # we have to change the class properties post-hoc. This is pretty hacky.\n",
    "                    task_obj.override_metric(metric_name=\"bypass\")\n",
    "\n",
    "                # override tasks' fewshot values to the provided num_fewshot arg value\n",
    "                # except if tasks have it set to 0 manually in their configs--then we should never overwrite that\n",
    "                if num_fewshot is not None:\n",
    "                    if (default_num_fewshot := task_obj.get_config(\"num_fewshot\")) == 0:\n",
    "                        logger.info(\n",
    "                            f\"num_fewshot has been set to 0 for {task_name} in its config. Manual configuration will be ignored.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            f\"Overwriting default num_fewshot of {task_name} from {default_num_fewshot} to {num_fewshot}\"\n",
    "                        )\n",
    "                        task_obj.set_config(key=\"num_fewshot\", value=num_fewshot)\n",
    "                else:\n",
    "                    # if num_fewshot not provided, and the task does not define a default one, default to 0\n",
    "                    if (\n",
    "                            default_num_fewshot := task_obj.get_config(\"num_fewshot\")\n",
    "                    ) is None:\n",
    "                        task_obj.set_config(key=\"num_fewshot\", value=0)\n",
    "                # fewshot_random_seed set for tasks, even with a default num_fewshot (e.g. in the YAML file)\n",
    "                task_obj.set_fewshot_seed(seed=fewshot_random_seed)\n",
    "\n",
    "                adjusted_task_dict[task_name] = task_obj\n",
    "\n",
    "        return adjusted_task_dict\n",
    "\n",
    "    def validate_tasks(self, lm, eval_tasks, confirm_run_unsafe_code: bool = True):\n",
    "        # validation checks:\n",
    "        # 1.are we running multimodal task <-> non-multimodal model class, or vice-versa.\n",
    "        # 2.are we running code that is marked as unsafe.\n",
    "        incompatible_tasks = []\n",
    "        for task_output in eval_tasks:\n",
    "            task: Task = task_output.task\n",
    "\n",
    "            if getattr(lm, \"MULTIMODAL\", False) != getattr(task, \"MULTIMODAL\", False):\n",
    "                incompatible_tasks.append(task_output.task_name)\n",
    "            elif getattr(task, \"UNSAFE_CODE\", False) and not confirm_run_unsafe_code:\n",
    "                raise ValueError(\n",
    "                    f\"Attempted to run task: {task_output.task_name} which is marked as unsafe. Set confirm_run_unsafe_code=True to run this task.\"\n",
    "                )\n",
    "        if len(incompatible_tasks) > 0:\n",
    "            if not getattr(lm, \"MULTIMODAL\", False):\n",
    "                raise ValueError(\n",
    "                    f\"Attempted to run tasks: {incompatible_tasks} which require multimodal input, but the selected model type does not currently implement this. Multimodal support is currently restricted to the ['hf-multimodal', 'vllm-vlm'] model type.\"\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Attempted to run tasks: {incompatible_tasks} which are text-only, but used a model type which only currently supports multimodal tasks.\"\n",
    "                )\n",
    "\n",
    "    def train_classifier(self, data):\n",
    "        self.classifier_model = self.classifier_model.to(self.device)\n",
    "        self.classifier_model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"messplus_modernbert\",\n",
    "            per_device_train_batch_size=1,\n",
    "            per_device_eval_batch_size=1,\n",
    "            learning_rate=5e-5,\n",
    "            num_train_epochs=5,\n",
    "            bf16=True,\n",
    "            optim=\"sgd\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=1,\n",
    "            eval_strategy=\"steps\",\n",
    "            report_to=\"wandb\",\n",
    "        )\n",
    "\n",
    "        trainer = WeightedLossTrainer(\n",
    "            model=self.classifier_model,\n",
    "            args=training_args,\n",
    "            train_dataset=data,\n",
    "            eval_dataset=data,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        self.classifier_model = self.classifier_model.to(\"cpu\")\n",
    "\n",
    "    def estimate_user_preferences(self, req):\n",
    "        tokenized_inputs = self.classifier_tokenizer(\n",
    "            [req],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.classifier_model(**tokenized_inputs)\n",
    "\n",
    "        preds = outputs.logits\n",
    "        probs = torch.softmax(preds, dim=-1)\n",
    "\n",
    "        estimated_user_preferences = []\n",
    "        for idx in range(len(self.config[\"model_zoo\"].keys())):\n",
    "            # For now, this only works for single requests. When using batched processing, we need to introduce another\n",
    "            # dimension here.\n",
    "            if probs.shape != torch.Size([1, probs.shape[-1]]):\n",
    "                logger.warning(f\"It seems you are trying to use batched inputs. Currently the system only supports \"\n",
    "                               f\"single requests. Change the way probs are summed up.\")\n",
    "\n",
    "            estimated_user_preferences.append(\n",
    "                torch.sum(probs[:, 0:(idx + 1)])\n",
    "            )\n",
    "\n",
    "        return estimated_user_preferences\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(eval_preds):\n",
    "        predictions, labels = eval_preds\n",
    "        print(predictions, labels)\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "        metrics = {\n",
    "            \"f1\": float(f1) if f1 == 1 else f1,\n",
    "            \"accuracy\": float(accuracy),\n",
    "        }\n",
    "\n",
    "        logger.info(f\"Evaluated model with metrics: {metrics}\")\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3afc55-7226-4ba8-9e2a-6fcab7888893",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:17:50.633170Z",
     "start_time": "2025-01-21T09:17:24.785504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21:15:14:57,661 INFO     [mess_plus.py:285] Found 2 models in zoo: dict_keys(['meta-llama/Llama-3.2-1B-Instruct', 'meta-llama/Llama-3.2-3B-Instruct'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-21 15:15:02 config.py:478] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 01-21 15:15:02 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-21 15:15:03 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-21 15:15:03 model_runner.py:1092] Starting to load model meta-llama/Llama-3.2-1B-Instruct...\n",
      "INFO 01-21 15:15:04 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 01-21 15:15:04 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e072e8b544f64934bd0109ff43e05351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-21 15:15:04 model_runner.py:1097] Loading model weights took 2.3185 GB\n",
      "INFO 01-21 15:15:05 worker.py:241] Memory profiling takes 0.35 seconds\n",
      "INFO 01-21 15:15:05 worker.py:241] the current vLLM instance can use total_gpu_memory (23.64GiB) x gpu_memory_utilization (0.90) = 21.28GiB\n",
      "INFO 01-21 15:15:05 worker.py:241] model weights take 2.32GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 17.67GiB.\n",
      "INFO 01-21 15:15:05 gpu_executor.py:76] # GPU blocks: 36194, # CPU blocks: 8192\n",
      "INFO 01-21 15:15:05 gpu_executor.py:80] Maximum concurrency for 1024 tokens per request: 565.53x\n",
      "INFO 01-21 15:15:06 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 01-21 15:15:06 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 01-21 15:15:18 model_runner.py:1527] Graph capturing finished in 11 secs, took 0.15 GiB\n",
      "INFO 01-21 15:15:18 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 13.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21:15:15:19,670 INFO     [mess_plus.py:306] vLLM model meta-llama/Llama-3.2-1B-Instruct loaded on rank [0]. Tensor parallel size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-21 15:15:21 config.py:478] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 01-21 15:15:21 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=42, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-21 15:15:21 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-21 15:15:21 model_runner.py:1092] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 75.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig/messplus/boolq_baseline.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[43mMessPlusAutomaticModelSelector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_file_path\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/mess_plus.py:76\u001b[0m, in \u001b[0;36mMessPlusAutomaticModelSelector.__init__\u001b[0;34m(self, config_file_path)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_response_column_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__warm_up_inference_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Classifier model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# self.__warmup_classifier_model()\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/mess_plus.py:294\u001b[0m, in \u001b[0;36mMessPlusAutomaticModelSelector.__warm_up_inference_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_models[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    291\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_models[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m--> 294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm_eval_instance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mMessLMEvalVLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_seq_len\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# data[\"max_seq_len\"],\u001b[39;49;00m\n\u001b[1;32m    297\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgpu_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu_memory_utilization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# \"tokenize\": AutoTokenizer.from_pretrained(model)\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     }\n\u001b[1;32m    306\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loaded on rank \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Tensor parallel size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu_indices\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll models loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/utils/mess_lm_eval_harness/vllm.py:60\u001b[0m, in \u001b[0;36mMessLMEvalVLLM.__init__\u001b[0;34m(self, pretrained, trust_remote_code, revision, interleave, max_images, max_seq_len, gpu_indices, max_memory_utilization, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m         pretrained: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     42\u001b[0m ):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m: pretrained,\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_memory_utilization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(max_memory_utilization),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     58\u001b[0m     }\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterleave\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/lm_eval/models/vllm_vlms.py:49\u001b[0m, in \u001b[0;36mVLLM_VLM.__init__\u001b[0;34m(self, pretrained, trust_remote_code, revision, interleave, max_images, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit_mm_per_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_images}\n\u001b[1;32m     48\u001b[0m     eval_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetting limit_mm_per_prompt[image] to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_images\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterleave \u001b[38;5;241m=\u001b[39m interleave\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_images \u001b[38;5;241m=\u001b[39m max_images\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/lm_eval/models/vllm_causallms.py:108\u001b[0m, in \u001b[0;36mVLLM.__init__\u001b[0;34m(self, pretrained, dtype, revision, trust_remote_code, tokenizer, tokenizer_mode, tokenizer_revision, add_bos_token, prefix_token_id, tensor_parallel_size, quantization, max_gen_toks, swap_space, batch_size, max_batch_size, max_length, max_model_len, seed, gpu_memory_utilization, device, data_parallel_size, lora_local_path, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch_size\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(batch_size)\n\u001b[1;32m    106\u001b[0m )\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     eval_logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou might experience occasional issues with model weight downloading when data_parallel is in use. To ensure stable performance, run with data_parallel_size=1 until the weights are downloaded and cached.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/utils.py:990\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    986\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m    987\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m    988\u001b[0m         )\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:230\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:532\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    530\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:288\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/executor/executor_base.py:36\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/executor/gpu_executor.py:35\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/worker/worker.py:155\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1094\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1097\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1098\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py:12\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     11\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:361\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[0;32m--> 361\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     weights_to_load \u001b[38;5;241m=\u001b[39m {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()}\n\u001b[1;32m    364\u001b[0m     loaded_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_all_weights(model_config, model))\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py:114\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(vllm_config, prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvllm_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_params:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# new-style model class\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config):\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the design and update the model class accordingly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    121\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:517\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config \u001b[38;5;241m=\u001b[39m lora_config\n\u001b[0;32m--> 517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpadded_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:554\u001b[0m, in \u001b[0;36mLlamaForCausalLM._init_model\u001b[0;34m(self, vllm_config, prefix)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/compilation/decorators.py:147\u001b[0m, in \u001b[0;36m_support_torch_compile.<locals>.__init__\u001b[0;34m(self, vllm_config, prefix, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 147\u001b[0m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config \u001b[38;5;241m=\u001b[39m vllm_config\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# for CompilationLevel.DYNAMO_AS_IS , the upper level model runner\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# will handle the compilation, so we don't need to do anything here.\u001b[39;00m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:318\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, vllm_config, prefix, layer_type)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[0;32m--> 318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:551\u001b[0m, in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    546\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    547\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    548\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    549\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    550\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 551\u001b[0m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    553\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:320\u001b[0m, in \u001b[0;36mLlamaModel.__init__.<locals>.<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m PPMissingLayer()\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m make_layers(\n\u001b[1;32m    319\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m--> 320\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    324\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    325\u001b[0m )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:247\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    231\u001b[0m attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    232\u001b[0m     config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LlamaAttention(\n\u001b[1;32m    234\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    235\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.self_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    246\u001b[0m )\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    256\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    258\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/models/llama.py:72\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config, bias, prefix)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     64\u001b[0m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     prefix: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     70\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.gate_up_proj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(\n\u001b[1;32m     80\u001b[0m         input_size\u001b[38;5;241m=\u001b[39mintermediate_size,\n\u001b[1;32m     81\u001b[0m         output_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m         prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.down_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:426\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix)\u001b[0m\n\u001b[1;32m    424\u001b[0m tp_size \u001b[38;5;241m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size \u001b[38;5;241m%\u001b[39m tp_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 426\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:306\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     output_sizes \u001b[38;5;241m=\u001b[39m [output_size]\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader_v2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mWEIGHT_LOADER_V2_SUPPORTED\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    318\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    319\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py:124\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    120\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    121\u001b[0m                    output_partition_sizes: List[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    122\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    123\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m--> 124\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    127\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    128\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m    129\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m~/code/Energy-Optimal-Inferencing/venv/lib/python3.12/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 80.06 MiB is free. Including non-PyTorch memory, this process has 23.55 GiB memory in use. Of the allocated memory 22.92 GiB is allocated by PyTorch, with 24.00 MiB allocated in private pools (e.g., CUDA Graphs), and 75.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "config_file_path = \"config/messplus/boolq_baseline.yaml\"\n",
    "\n",
    "selector = MessPlusAutomaticModelSelector(\n",
    "    config_file_path=config_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a4754-5e86-4244-a875-414fda1144fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T09:17:50.633890Z",
     "start_time": "2025-01-21T09:17:50.633848Z"
    }
   },
   "outputs": [],
   "source": [
    "selector.launch(\n",
    "    limit_num_samples=None,\n",
    "    cache_requests=False,\n",
    "    rewrite_requests_cache=False,\n",
    "    system_instruction=None,\n",
    "    apply_chat_template=False,\n",
    "    fewshot_as_multiturn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab5884-e3ca-4f75-ae7b-75917c9f235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(selector.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d7bee-b6f6-4822-820a-1e63a9446fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = []\n",
    "for model_category in selector.scores.keys(): \n",
    "    energy.append(\n",
    "        sum(map(lambda x: x[0][1] / len(selector.scores[model_category]), selector.scores[model_category])) #  / len(selector.scores[model_category])\n",
    "    )\n",
    "\n",
    "# energy = torch.tensor(energy).view(-1, 1)\n",
    "display(energy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa63499-892d-4be6-9074-4c790e0712b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
