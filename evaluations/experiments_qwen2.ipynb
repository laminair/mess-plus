{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "from plots.utils.plotting import write_figure_to_disk\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"experiments_llama3.ipynb\").absolute().parent\n",
    "\n",
    "DATA_DIR = f\"{NOTEBOOK_PATH}/data/qwen2_online\"\n",
    "\n",
    "BENCHMARK_NAMES = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\", \"non_iid_chained\"]\n",
    "MODEL_LABEL_NAMES = [\"xsmall\", \"small\", \"medium\", \"large\"]\n",
    "MODEL_FAMILY = \"qwen2\"\n",
    "\n",
    "# BENCHMARK_NAMES = [\"winogrande\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess-plus_3-qwen2_online_vFINAL\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")"
   ],
   "id": "440a498da98688a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(run_summary_df)\n",
    "run_df = load_all_histories_to_dataframe(DATA_DIR)\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"benchmark_name\"] = name\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"] = run_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"].str.replace(f\"{name}_\", \"\")\n",
    "\n",
    "run_df[[\"V\", \"alpha\", \"c\", \"seed\"]] = run_df[\"run_name\"].str.split(\"_\", expand=True)\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].str.replace(\"a=\", \"\")\n",
    "run_df[\"V\"] = run_df[\"V\"].str.replace(\"V=\", \"\")\n",
    "run_df[\"c\"] = run_df[\"c\"].str.replace(\"c=\", \"\")\n",
    "run_df[\"seed\"] = run_df[\"seed\"].str.replace(\"seed=\", \"\")\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].astype(float)\n",
    "run_df[\"V\"] = run_df[\"V\"].astype(float)\n",
    "run_df[\"c\"] = run_df[\"c\"].astype(float)\n",
    "run_df[\"seed\"] = run_df[\"seed\"].astype(int)\n",
    "\n",
    "run_df[\"models/small_chosen\"] = run_df[\"models/small_chosen\"].astype(float)\n",
    "run_df[\"models/medium_chosen\"] = run_df[\"models/medium_chosen\"].astype(float)\n",
    "run_df[\"models/large_chosen\"] = run_df[\"models/large_chosen\"].astype(float)\n",
    "\n",
    "display(run_df.head())"
   ],
   "id": "52264c7618a8265f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(run_df.columns)\n",
    "analysis_df = run_df.loc[(run_df[\"c\"] == 1.0) & (run_df[\"benchmark_name\"] == \"winogrande\")].pivot_table(index=[\"benchmark_name\", \"alpha\", \"V\", \"c\"], values=[\"avg_accuracy\", \"mess_plus/total_energy_incl_classifier\", \"mess_plus/q_length\", \"total_runtime\"], aggfunc={\"avg_accuracy\": \"mean\", \"mess_plus/total_energy_incl_classifier\": \"sum\", \"mess_plus/q_length\": \"mean\", \"total_runtime\": \"max\"})"
   ],
   "id": "e20b51f6fc67780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_value_labels(axx, spacing=5, convert_to_mj: bool = True):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in axx.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        if convert_to_mj:\n",
    "            label = f'{y_value / 1_000_000:.1f}' # MJ conversion\n",
    "        else:\n",
    "            label = f'{y_value:.2f}'\n",
    "\n",
    "        # Create annotation\n",
    "        axx.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "def fmt_to_megajoules(x, pos):\n",
    "    return f'{(x / 1_000_000):.0f}'\n"
   ],
   "id": "a38483bf31450f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load raw inference data\n",
    "\n",
    "infer_df = pd.DataFrame()\n",
    "def get_inference_data(benchmark_name):\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{NOTEBOOK_PATH.parent}/data/{MODEL_FAMILY}/inference_outputs/{benchmark_name}\")\n",
    "\t\tinput_df[\"idx_original\"] = input_df.index\n",
    "\t\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\treturn input_df\n",
    "\texcept ValueError:\n",
    "\t\treturn pd.DataFrame()\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\tinfer_df = pd.concat([infer_df, get_inference_data(name)], ignore_index=True)\n",
    "\n",
    "infer_df.reset_index(inplace=True)\n",
    "\n",
    "# Get baseline dataframe\n",
    "BASELINE_DATA_DIR = f\"{NOTEBOOK_PATH}/data/random_qwen2_baseline_v01\"\n",
    "baseline_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess_plus_qwen2_random_baseline_with_constraint_v01\",\n",
    "    save_dir=BASELINE_DATA_DIR,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "baseline_df = load_all_histories_to_dataframe(BASELINE_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"] = baseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_alpha=\", \"\")\n",
    "\tbaseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"alpha\"] = baseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"run_name\"]\n",
    "\n",
    "\tbaseline_df[\"alpha\"] = baseline_df[\"alpha\"].astype(float)\n",
    "\n",
    "\n",
    "print(baseline_df[\"benchmark_name\"].unique())\n"
   ],
   "id": "ec68ca6f1bfad7d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the RouteLLM evaluations\n",
    "ROUTELLM_DATA_DIR = f\"{NOTEBOOK_PATH}/data/routellm_baseline_v01\"\n",
    "routellm_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routellm_baseline_v01\",\n",
    "    save_dir=ROUTELLM_DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routellm_bs_df = load_all_histories_to_dataframe(ROUTELLM_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\troutellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\troutellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"run_name_updated\"] = routellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_\", \"\")\n",
    "\n",
    "\n",
    "routellm_bs_df[[\"alpha\", \"threshold\"]] = routellm_bs_df[\"run_name_updated\"].str.split(\"_\", expand=True)\n",
    "routellm_bs_df[\"alpha\"] = routellm_bs_df[\"alpha\"].str.replace(\"alpha=\", \"\")\n",
    "routellm_bs_df[\"threshold\"] = routellm_bs_df[\"threshold\"].str.replace(\"thres=\", \"\")\n",
    "routellm_bs_df[\"alpha\"] = routellm_bs_df[\"alpha\"].astype(float)\n",
    "routellm_bs_df[\"threshold\"] = routellm_bs_df[\"threshold\"].astype(float)\n"
   ],
   "id": "3a45eef546795658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the RouterDC evaluations\n",
    "ROUTERDC_DATA_DIR = f\"{NOTEBOOK_PATH}/data/routerdc_baseline_v01\"\n",
    "routellm_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routerdc_baseline_v01\",\n",
    "    save_dir=ROUTERDC_DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routerdc_bs_df = load_all_histories_to_dataframe(ROUTERDC_DATA_DIR)\n",
    "routerdc_bs_df[[\"benchmark_name\", \"alpha\"]] = routerdc_bs_df[\"run_name\"].str.replace(\"routerdc-\", \"\").str.split(\"-\", expand=True)\n",
    "routerdc_bs_df[\"alpha\"] = routerdc_bs_df[\"alpha\"].str.replace(\"alpha=\", \"\").astype(float)\n",
    "\n",
    "display(routerdc_bs_df)"
   ],
   "id": "4f88a5c39cd1ae93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ROUTELLM_THRESHOLD_ALPHA_MAP = {}\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\troutellm_subset = routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == benchmark)]\n",
    "\n",
    "\tif len(routellm_subset) == 0:\n",
    "\t\tcontinue\n",
    "\n",
    "\t# sns.set_style(\"whitegrid\")\n",
    "\t# sns.set_theme(context='paper', style='whitegrid', palette='colorblind', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\t#\n",
    "\t# # Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "\t# fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "\n",
    "\talpha_vals_bm = routellm_subset[\"alpha\"].unique().tolist()\n",
    "\tthreshold_vals = routellm_subset[\"threshold\"].sort_values(ascending=False).unique().tolist()\n",
    "\n",
    "\tthres_acc_mapping = {i: 0.0 for i in threshold_vals}\n",
    "\talpha_thresh_mapping = {a: 0.0 for a in alpha_vals_bm}\n",
    "\t# Get final accuracy for each threshold\n",
    "\tfor alpha in alpha_vals_bm:\n",
    "\t\talpha_thres_value_match = False\n",
    "\t\tfor thresh in threshold_vals:\n",
    "\n",
    "\t\t\tif alpha_thres_value_match is True:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tdata = routellm_subset.loc[(routellm_subset[\"alpha\"] == alpha) & (routellm_subset[\"threshold\"] == thresh)]\n",
    "\t\t\tthreshold_accuracy = data.loc[data[\"_step\"] > data[\"_step\"].max() - 30, [\"avg_accuracy\"]].mean()\n",
    "\n",
    "\t\t\tif threshold_accuracy.item() >= alpha:\n",
    "\t\t\t\talpha_thresh_mapping[alpha] = thresh\n",
    "\t\t\t\talpha_thres_value_match = True\n",
    "\n",
    "\tROUTELLM_THRESHOLD_ALPHA_MAP[benchmark] = alpha_thresh_mapping\n",
    "\n",
    "display(ROUTELLM_THRESHOLD_ALPHA_MAP)\n"
   ],
   "id": "315de4977bc810af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v_values_per_benchmark = {\n",
    "    \"arc_challenge\": [0.001, 0.0001, 0.00001],\n",
    "    \"arc_easy\": [0.01, 0.001, 0.0001],\n",
    "    \"boolq\": [0.01, 0.001, 0.0001],\n",
    "    # \"lambada_standard\": [0.01, 0.001, 0.0001],\n",
    "    \"logiqa\": [0.001, 0.0001, 0.00001],\n",
    "    # \"logiqa2\": [0.01, 0.001, 0.0001],\n",
    "    \"piqa\": [0.01, 0.001, 0.0001],\n",
    "    \"sciq\": [0.0001, 0.00001, 0.000001],\n",
    "    \"social_iqa\": [0.001, 0.0001, 0.00001],\n",
    "    \"winogrande\": [0.01, 0.001, 0.0001],\n",
    "    \"non_iid_chained\": [0.001, 0.0001, 0.00001],\n",
    "}"
   ],
   "id": "57657572dfc3321f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.1, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(15, 5), gridspec_kw={'width_ratios': [4, 1, 1, 1, 1, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"arc_challenge\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# alpha = target_alpha_per_benchmark[name]\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\"]+[f\"label_{name}\" for name in MODEL_LABEL_NAMES]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\tfor model in MODEL_LABEL_NAMES:\n",
    "\t\taxes[iterator][0].text(s=\"L1B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[f\"label_{model}\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\t\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[f\"label_{model}\"], color='gray', linestyle='--')\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][0],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\", \"#7c7c7c\"][:len(MODEL_LABEL_NAMES)],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "\n",
    "\taxes[iterator][0].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\t# Add RouteLLM baseline\n",
    "\t# axes[iterator][0].plot(\n",
    "\t# \troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"],\n",
    "\t# \troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"avg_accuracy\"],\n",
    "\t# \tcolor=\"brown\", linestyle=\"dotted\", label=\"RouteLLM\"\n",
    "\t# )\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][0].legend(loc=\"center\", ncol=2, title=\"Method\", fontsize=8, title_fontsize=8)\n",
    "\n",
    "\taxes[iterator][0].axhline(y=alpha, color='red', linestyle='-', label=\"alpha\")\n",
    "\taxes[iterator][0].set(ylim=[0.97 * raw_inference_accuracies_per_model[f\"label_{MODEL_LABEL_NAMES[0]}\"], 1.15 * raw_inference_accuracies_per_model[f\"label_{MODEL_LABEL_NAMES[-1]}\"]])\n",
    "\taxes[iterator][0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator][0].text(s=r\"$ \\alpha = {alpha_val} $ (red line)\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=1.15 * raw_inference_accuracies_per_model[f\"label_{MODEL_LABEL_NAMES[-1]}\"] - 0.04, color='red', fontsize=9, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\t# Stackplot for Model Call Ratio\n",
    "\tv_values_per_benchmark[name] = sorted(v_values_per_benchmark[name], reverse=False)\n",
    "\t# v_values_per_benchmark[name].reverse()\n",
    "\tfor jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "\t\tstack_df = subset.loc[\n",
    "\t\t\t(run_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(run_df[\"V\"] == V) &\n",
    "\t\t\t(subset[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\"] + [f\"models/{name}_chosen\" for name in MODEL_LABEL_NAMES]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t\tx = stack_df[\"_step\"]\n",
    "\t\ty = stack_df[[f\"models/{name}_chosen\" for name in MODEL_LABEL_NAMES]]\n",
    "\t\ty_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].set(xlabel=f\"Requests @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "\t\taxes[iterator][1 + jdx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t\taxes[iterator][1 + jdx].set(xlim=[0, stack_df[\"_step\"].max()])\n",
    "\n",
    "\t\tif iterator == 0 and jdx == 0:\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L70B\", x=70, y=0.80, color=\"black\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L8B\", x=70, y=0.40, color=\"white\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L1B\", x=70, y=0.10, color=\"white\")\n",
    "\n",
    "\t# Add area plot for random baseline with constraint.\n",
    "\t# baseline_stack_df = baseline_df.loc[\n",
    "\t# \t\t(baseline_df[\"benchmark_name\"] == name) &\n",
    "\t# \t\t(baseline_df[\"alpha\"] == alpha),\n",
    "\t# \t\t[\"_step\"] + [f\"models/{name}_chosen\" for name in MODEL_LABEL_NAMES]\n",
    "\t# \t].groupby([\"_step\"]).mean().reset_index()\n",
    "\t#\n",
    "\t# x_base = baseline_stack_df[\"_step\"]\n",
    "\t# y_base = baseline_stack_df[[f\"models/{name}_chosen\" for name in MODEL_LABEL_NAMES]]\n",
    "\t# y_stack_base = np.cumsum(y_base, axis=1)\n",
    "\t#\n",
    "\t# axes[iterator][4].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\t# axes[iterator][4].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "\t# axes[iterator][4].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\t# axes[iterator][4].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\t# axes[iterator][4].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t# axes[iterator][4].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\t## axes[iterator][6].get_yaxis().set_visible(False)\n",
    "\n",
    "\t# Add area plot for RouteLLM baseline\n",
    "\t# routellm_stack_df = routellm_bs_df.loc[\n",
    "\t# \t\t(routellm_bs_df[\"benchmark_name\"] == name) &\n",
    "\t# \t\t(routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]),\n",
    "\t# \t\t[\"_step\", \"models/small_chosen\", \"models/large_chosen\"]\n",
    "\t# \t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t# x_rllm = routellm_stack_df[\"_step\"]\n",
    "\t# y_rllm = routellm_stack_df[[\"models/small_chosen\", \"models/large_chosen\"]]\n",
    "\t# y_stack_rllm = np.cumsum(y_rllm, axis=1)\n",
    "\n",
    "\t# axes[iterator][5].fill_between(x_rllm, 0, y_stack_rllm.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\t# axes[iterator][5].fill_between(x_rllm, y_stack_rllm.iloc[:, 0], y_stack_rllm.iloc[:, 1], color=\"#69cf81\", alpha=0.95)\n",
    "\t# axes[iterator][6].fill_between(x_rllm, y_stack_rllm.iloc[:, 1], y_stack_rllm.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\t# axes[iterator][5].set(xlabel=f\"Req. (RouteLLM)\", xlim=[0, routellm_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\t# axes[iterator][5].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t# axes[iterator][5].set(xlim=[0, routellm_stack_df[\"_step\"].max()])\n",
    "\n",
    "\taxes[iterator][0].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][0].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes[iterator], [r\"Avg. User Satisfaction Rate Over Time (varying $\\alpha$)\".format(alpha_val=alpha), \"Model Call Ratio (MCR)\", \"\", \"\"]):\n",
    "\n",
    "\t\tif iterator == 1:\n",
    "\t\t\tax.set_ylabel(col, rotation=90, size=10)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_{MODEL_FAMILY}_all_alpha\", chapter_name=\"evaluations\")\n"
   ],
   "id": "7045943bad636e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(4, 7))\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"arc_challenge\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# Q Plot for SLA violations\n",
    "\tsubset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\tbaseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"sla_violations\"] = baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"mess_plus/q_length\"] / baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\t# routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"sla_violations\"] = routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"mess_plus/q_length\"] / routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"sla_violations\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "\n",
    "\taxes[iterator].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"sla_violations\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\t# Add RouteLLM baseline\n",
    "\t# axes[iterator].plot(\n",
    "\t# \troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"],\n",
    "\t# \troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"sla_violations\"],\n",
    "\t# \tcolor=\"brown\", linestyle=\"dotted\", label=\"RouteLLM\"\n",
    "\t# )\n",
    "\n",
    "\taxes[iterator].set_ylim([0, 0.2])\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator].text(s=r\"$ \\alpha = {alpha_val} $\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=0.17, color='red', fontsize=14, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator].legend(ncols=1, title=\"Method\", fontsize=9, title_fontsize=9, loc='upper left')\n",
    "\n",
    "\taxes[iterator].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\n",
    "\taxes[iterator].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes, [\"\", r\"Avg. SLA Violations Over Time (varying $\\alpha$ values)\", \"\"]):\n",
    "\t\tax.set_ylabel(col, rotation=90, size=18)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_sla_violations\", chapter_name=\"evaluations\")"
   ],
   "id": "a48f9f1d3de21409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(infer_df.columns)\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_large\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_medium\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_small\"].mean())"
   ],
   "id": "3ef80eb1dca358dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If there is a non-stationary benchmark named \"non_iid_chained\", we need to compute the scores for it\n",
    "\n",
    "\n",
    "non_stat_bm = infer_df.loc[infer_df[\"benchmark_name\"].isin([\"arc_challenge\", \"piqa\", \"winogrande\"])].copy(deep=True)\n",
    "non_stat_bm[\"benchmark_name\"] = \"non_iid_chained\"\n",
    "display(non_stat_bm[\"benchmark_name\"].unique())\n",
    "\n",
    "infer_df = pd.concat([infer_df, non_stat_bm], ignore_index=True)\n"
   ],
   "id": "999cdcb282e356da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot generator\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    # \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    # \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "\t\"non_iid_chained\": \"Non-Stationary Benchmark\"\n",
    "}\n",
    "\n",
    "# Create a list of all benchmark-alpha combinations\n",
    "benchmark_alpha_combinations = []\n",
    "for name in v_values_per_benchmark.keys():\n",
    "    config_path = Path(f\"{NOTEBOOK_PATH.parent}/config/{MODEL_FAMILY}/online/{name}.yaml\")\n",
    "    with config_path.open(\"r\") as f:\n",
    "        import yaml\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    algorithm_config = CONFIG[\"algorithm\"]\n",
    "    for alpha in algorithm_config[\"alpha_values\"]:\n",
    "        benchmark_alpha_combinations.append((name, alpha))\n",
    "\n",
    "# Initialize plotting variables\n",
    "plot_num = 0\n",
    "col_count = 0\n",
    "\n",
    "# Iterate through all benchmark-alpha combinations\n",
    "for combo_idx, (name, alpha) in enumerate(benchmark_alpha_combinations):\n",
    "\n",
    "    # Create new figure every 6 columns\n",
    "    if col_count == 0:\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        fig, axes = plt.subplots(nrows=7, ncols=6, figsize=(20, 12))\n",
    "        plot_num += 1\n",
    "\n",
    "    # Get current column index\n",
    "    col_idx = col_count\n",
    "\n",
    "    # Skip if this benchmark doesn't have V values configured\n",
    "    if name not in v_values_per_benchmark.keys():\n",
    "        continue\n",
    "\n",
    "    # Filter data for current benchmark and alpha\n",
    "    subset = run_df.loc[(run_df[\"benchmark_name\"] == name) &\n",
    "                       (run_df[\"c\"] == 0.1) &\n",
    "                       (run_df[\"V\"].isin(v_values_per_benchmark[name])) &\n",
    "                       (run_df[\"_step\"] > 10) &\n",
    "                       (run_df[\"alpha\"] == alpha)]\n",
    "\n",
    "    v_values = subset[\"V\"].unique().tolist()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    raw_inference_accuracies_per_model = infer_df[[\"benchmark_name\"] + [f\"label_{model}\" for model in MODEL_LABEL_NAMES]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.025, color='gray', fontsize=8, ha=\"left\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.025, color='gray', fontsize=8, ha=\"center\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.025, color='gray', fontsize=8, ha=\"right\")\n",
    "\n",
    "    for model in MODEL_LABEL_NAMES:\n",
    "\t    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[f\"label_{model}\"], color='gray', linestyle='--')\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"avg_accuracy\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[0][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\", \"#7c7c7c\"][:len(MODEL_LABEL_NAMES)],\n",
    "    )\n",
    "\n",
    "    axes[0][col_idx].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "    axes[0][col_idx].axhline(y=alpha, color='red', linestyle='-')\n",
    "    axes[0][col_idx].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=8, ha=\"right\")\n",
    "\n",
    "    axes[0][col_idx].set(ylim=[0.97 * raw_inference_accuracies_per_model[f\"label_{MODEL_LABEL_NAMES[0]}\"], 1.15 * raw_inference_accuracies_per_model[f\"label_{MODEL_LABEL_NAMES[-1]}\"]])\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[0][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Q Plot for SLA violations\n",
    "    subset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"sla_violations\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[1][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "    )\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[1][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Energy consumption plot\n",
    "    random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "    random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "    random_baseline_energy.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "    raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "    raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "    raw_inference_energy_data.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"70B\", \"energy_consumption_medium\": \"8B\", \"energy_consumption_small\": \"1B\"}, inplace=False)\n",
    "\n",
    "    raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "    energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "\n",
    "    energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "\n",
    "    energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "    energy_data.reset_index(inplace=True)\n",
    "    energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "\n",
    "    sns.barplot(\n",
    "        data=energy_data,\n",
    "        x=\"V\",\n",
    "        y=\"mess_plus/energy\",\n",
    "        ax=axes[2][col_idx],\n",
    "        errorbar=(\"ci\", 0.95),\n",
    "    )\n",
    "\n",
    "    add_value_labels(axes[2][col_idx])\n",
    "    axes[2][col_idx].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "    axes[2][col_idx].set(ylim=[0, 2 * energy_data[\"mess_plus/energy\"].max()])\n",
    "    axes[2][col_idx].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "    # Stackplot for Model Call Ratio\n",
    "    for jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "        stack_df = subset.loc[\n",
    "            (run_df[\"benchmark_name\"] == name) &\n",
    "            (run_df[\"V\"] == V) &\n",
    "            (subset[\"alpha\"] == alpha),\n",
    "            [\"_step\"] + [f\"models/{model}_chosen\" for model in MODEL_LABEL_NAMES]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "        x = stack_df[\"_step\"]\n",
    "        y = stack_df[[f\"models/{model}_chosen\" for model in MODEL_LABEL_NAMES]]\n",
    "        y_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "        axes[3 + jdx][col_idx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 2], y_stack.iloc[:, 3], color=\"#7c7c7c\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].set(xlabel=f\"Request @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "        axes[3 + jdx][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "        # if jdx == 0 and col_idx == 0:\n",
    "        #     axes[3 + jdx][col_idx].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"])\n",
    "\n",
    "    # Add area plot for random baseline with constraint.\n",
    "    baseline_stack_df = baseline_df.loc[\n",
    "            (baseline_df[\"benchmark_name\"] == name) &\n",
    "            (baseline_df[\"alpha\"] == alpha),\n",
    "            [\"_step\"] + [f\"models/{model}_chosen\" for model in MODEL_LABEL_NAMES]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "    x_base = baseline_stack_df[\"_step\"]\n",
    "    y_base = baseline_stack_df[[f\"models/{model}_chosen\" for model in MODEL_LABEL_NAMES]]\n",
    "    y_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "    axes[6][col_idx].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 2], y_stack_base.iloc[:, 3], color=\"#7c7c7c\", alpha=0.95)\n",
    "    axes[6][col_idx].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "    axes[6][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axes[6][col_idx].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\n",
    "    # Set axis properties\n",
    "    axes[0][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[1][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[2][col_idx].set(xlabel=\"\")\n",
    "\n",
    "    # Remove y-labels for columns after the first\n",
    "    if col_idx > 0:\n",
    "        axes[0][col_idx].set(ylabel=None)\n",
    "        axes[1][col_idx].set(ylabel=None)\n",
    "        axes[2][col_idx].set(ylabel=None)\n",
    "\n",
    "    # Set title for each column\n",
    "    axes[0][col_idx].set_title(r\"{bm_name} ($\\alpha = {alpha_val} $)\".format(bm_name=BENCHMARK_NAME_DICT[name], alpha_val=alpha))\n",
    "\n",
    "    # Increment column counter\n",
    "    col_count += 1\n",
    "\n",
    "    # Check if we need to save the current figure and start a new one\n",
    "    if col_count == 6 or combo_idx == len(benchmark_alpha_combinations) - 1:\n",
    "        # Add row labels\n",
    "        for idx, (ax, row) in enumerate(zip(axes[:,0], [\"User Satisfaction\", \"SLA Violations\", \"Cost (in MJ energy)\", \"\", \"\", \"\", \"\"])):\n",
    "            if idx == 5:\n",
    "                fig.text(0.003, 0.225, \"Model Call Ratio (MCR)\", ha=\"center\", rotation='vertical', fontsize=plt.rcParams['axes.labelsize'])\n",
    "            else:\n",
    "                ax.set_ylabel(row, rotation=90, size='large')\n",
    "\n",
    "        # Save the figure\n",
    "        fig.tight_layout()\n",
    "        write_figure_to_disk(plt, file_name=f\"benchmark_performance_plot_{plot_num}\", chapter_name=\"evaluations\")\n",
    "\n",
    "        # Reset column counter for next figure\n",
    "        col_count = 0"
   ],
   "id": "989db16e594ce6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_pivot_table_for_main_results(input_df: pd.DataFrame, model_cols: list):\n",
    "\tlatest_steps = input_df.groupby(['benchmark_name', 'alpha'])['_step'].transform('max')\n",
    "\tis_last_step = input_df['_step'] == latest_steps\n",
    "\n",
    "\tfor col in model_cols:\n",
    "\t\tinput_df = input_df.rename(columns={col: f\"final_{col}\"})\n",
    "\n",
    "\t# Create new columns with the final values\n",
    "\t# for col in model_cols:\n",
    "\t#     final_values = input_df.loc[is_last_step, ['benchmark_name', 'alpha', col]]\n",
    "\t#     final_values = final_values.drop_duplicates(['benchmark_name', 'alpha'])\n",
    "\t#     input_df = pd.merge(\n",
    "\t#         input_df,\n",
    "\t# \t    final_values.rename(columns={col: f\"final_{col}\"}),\n",
    "\t#         on=['benchmark_name', 'alpha'],\n",
    "\t#         how='left'\n",
    "\t#     )\n",
    "\t#\n",
    "\t# display(input_df.columns)\n",
    "\n",
    "\t# Add the final model values to the pivot table\n",
    "\tmerged_pvt_table = input_df.loc[:, [\"benchmark_name\", \"alpha\", \"V\", \"avg_accuracy\", \"mess_plus/energy\"] + [f\"final_{col}\" for col in model_cols]].pivot_table(\n",
    "\t    index=[\"benchmark_name\", \"alpha\"],\n",
    "\t    columns=[\"V\"],\n",
    "\t    values=[\"avg_accuracy\", \"mess_plus/energy\"] + [f\"final_{col}\" for col in model_cols],\n",
    "\t    aggfunc={\n",
    "\t        \"avg_accuracy\": [\"mean\", \"std\"],\n",
    "\t        \"mess_plus/energy\": [\"sum\", \"std\"],\n",
    "\t        **{f\"final_{col}\": ['mean'] for col in model_cols}\n",
    "\t    }\n",
    "\t)\n",
    "\n",
    "\treturn merged_pvt_table\n",
    "\n"
   ],
   "id": "77532abbf7c205f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create pivot tables for our random baseline, RouteLLM, and MESS+\n",
    "\n",
    "# SINGLE MODEL\n",
    "pvt_base_model = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"energy_consumption_xsmall\", \"label_xsmall\", \"label_small\", \"label_medium\", \"label_large\"]].pivot_table(\n",
    "    index=[\"benchmark_name\"],\n",
    "    values=[\"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"energy_consumption_xsmall\", \"label_xsmall\", \"label_small\", \"label_medium\", \"label_large\"],\n",
    "    aggfunc={\n",
    "        \"energy_consumption_large\": [\"sum\", \"std\"],\n",
    "        \"energy_consumption_medium\": [\"sum\", \"std\"],\n",
    "        \"energy_consumption_small\": [\"sum\", \"std\"],\n",
    "        \"energy_consumption_xsmall\": [\"sum\", \"std\"],\n",
    "        \"label_xsmall\": [\"mean\", \"std\"],\n",
    "        \"label_small\": [\"mean\", \"std\"],\n",
    "        \"label_medium\": [\"mean\", \"std\"],\n",
    "        \"label_large\": [\"mean\", \"std\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "pvt_base_model.columns = pd.MultiIndex.from_tuples(\n",
    "    map(lambda x: (x[0], x[1], 2), pvt_base_model.columns)\n",
    ")\n",
    "\n",
    "new_cols = []\n",
    "for col in pvt_base_model.columns:\n",
    "\tif col[0] == \"energy_consumption_xsmall\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 2))\n",
    "\telif col[0] == \"energy_consumption_small\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 3))\n",
    "\telif col[0] == \"energy_consumption_medium\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 4))\n",
    "\telif col[0] == \"energy_consumption_large\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 5))\n",
    "\telif col[0] == \"label_xsmall\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 2))\n",
    "\telif col[0] == \"label_small\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 3))\n",
    "\telif col[0] == \"label_medium\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 4))\n",
    "\telif col[0] == \"label_large\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 5))\n",
    "\n",
    "pvt_base_model.columns = pd.MultiIndex.from_tuples(new_cols, names=[None, None, \"V\"])\n",
    "\n",
    "# Add the new \"final_models/{size}_chosen\" columns with values of 1.0\n",
    "for size in MODEL_LABEL_NAMES:\n",
    "\tnum_size = 2\n",
    "\tif size == 'xsmall':\n",
    "\t\tnum_size = 2\n",
    "\telif size == 'small':\n",
    "\t\tnum_size = 3\n",
    "\telif size == 'medium':\n",
    "\t\tnum_size = 4\n",
    "\telif size == \"large\":\n",
    "\t\tnum_size = 5\n",
    "\n",
    "\tpvt_base_model[(f'final_models/{size}_chosen'), \"first\", num_size] = 1.0\n",
    "\n",
    "# Sort the columns for better organization (by size group)\n",
    "pvt_base_model = pvt_base_model.sort_index(axis=1, level=0)\n",
    "\n",
    "# RouteLLM\n",
    "# filtered_routellm_df = pd.DataFrame()\n",
    "# for benchmark_name, val_dict in ROUTELLM_THRESHOLD_ALPHA_MAP.items():\n",
    "# \tfor alpha, threshold in val_dict.items():\n",
    "# \t\tfiltered_routellm_df = pd.concat([\n",
    "# \t\t\tfiltered_routellm_df,\n",
    "# \t\t\troutellm_bs_df.loc[(routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == threshold) & (routellm_bs_df[\"benchmark_name\"] == benchmark_name)]\n",
    "# \t\t], ignore_index=True)\n",
    "\n",
    "# Convert to Megajoule\n",
    "# filtered_routellm_df[\"mess_plus/energy\"] = filtered_routellm_df[\"mess_plus/energy\"] / 1_000_000\n",
    "# filtered_routellm_df[\"V\"] = 1000 # Dummy to identify RouteLLM in the final latex output.\n",
    "# pvt_routellm = build_pivot_table_for_main_results(filtered_routellm_df, [\"models/small_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "# RouterDC baseline\n",
    "# routerdc_bs_df[\"V\"] = 10000\n",
    "# pvt_routerdc_baseline = build_pivot_table_for_main_results(routerdc_bs_df, [\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "baseline_df[\"V\"] = 100\n",
    "pvt_rand_baseline = build_pivot_table_for_main_results(baseline_df, [f\"models/{model}_chosen\" for model in MODEL_LABEL_NAMES])\n",
    "\n",
    "# This is an intermediary step to average across seeds.\n",
    "pvt_mess_plus = build_pivot_table_for_main_results(run_df, [f\"models/{name}_chosen\" for name in MODEL_LABEL_NAMES])\n",
    "\n",
    "combined_pivot = None\n",
    "combined_pivot = pd.concat([pvt_mess_plus, pvt_rand_baseline], axis=1) # pvt_routerdc_baseline, pvt_routellm\n",
    "\n",
    "display(combined_pivot.loc[(combined_pivot.index.get_level_values(0) == \"non_iid_chained\") & (combined_pivot.index.get_level_values(1) == 0.67), (combined_pivot.columns.get_level_values(2) == 0.0001)])\n",
    "display(pvt_rand_baseline)\n",
    "\n",
    "# Select alpha values per benchmark\n",
    "# TODO: @Ryan, make sure to adjust the values in the 'replace_level_0()' function as well, if you change it.\n",
    "bm_alpha = {\n",
    "\t\"arc_challenge\": 0.55,\n",
    "\t\"arc_easy\": 0.77,\n",
    "\t\"boolq\": 0.80,\n",
    "\t\"logiqa\": 0.33,\n",
    "\t\"piqa\": 0.79,\n",
    "\t\"sciq\": 0.93,\n",
    "\t\"social_iqa\": 0.47,\n",
    "\t\"winogrande\": 0.71,\n",
    "\t\"non_iid_chained\": 0.67\n",
    "}\n",
    "\n",
    "display(f\"Mean alpha across benchmarks: {np.mean([i for i in bm_alpha.values()])}\")\n",
    "\n",
    "selected_pivot = pd.DataFrame()\n",
    "for k, v in bm_alpha.items():\n",
    "\tselected_pivot = pd.concat([\n",
    "\t\tselected_pivot,\n",
    "\t\tcombined_pivot.loc[(combined_pivot.index.get_level_values(0) == k) & (combined_pivot.index.get_level_values(1) == v)]\n",
    "\t])\n",
    "\n",
    "\n",
    "# if combined_pivot is not None:\n",
    "# \tselected_pivot = combined_pivot.iloc[1::3]\n",
    "# else:\n",
    "# \tselected_pivot = pvt_mess_plus\n",
    "\n",
    "selected_pivot = selected_pivot.query('benchmark_name != \"lambada_standard\" & benchmark_name != \"logiqa2\"')\n",
    "\n",
    "print(\"All experiments combined\")\n",
    "# display(selected_pivot.index)\n",
    "def replace_level_0(idx_tuple):\n",
    "\t# Replace only 'A' with 'X', leave others unchanged\n",
    "\tbenchmark_name_mapping = {\n",
    "\t\t\"arc_challenge\": r\"ARC Challenge ($\\alpha = 55\\%$)\",\n",
    "\t\t\"arc_easy\": r\"ARC Easy ($\\alpha = 77\\%$)\",\n",
    "\t\t\"boolq\": r\"BoolQ  ($\\alpha = 80\\%$)\",\n",
    "\t\t\"logiqa\": r\"LogiQA ($\\alpha = 33\\%$)\",\n",
    "\t\t\"piqa\": r\"PiQA ($\\alpha = 79\\%$)\",\n",
    "\t\t\"sciq\": r\"SciQ ($\\alpha = 93\\%$)\",\n",
    "\t\t\"social_iqa\": r\"SocialIQA ($\\alpha = 47\\%$)\",\n",
    "\t\t\"winogrande\": r\"Winogrande ($\\alpha = 71\\%$)\",\n",
    "\t\t\"non_iid_chained\": r\"Non-stationary Benchmark ($\\alpha = 67\\%$)\"\n",
    "\t}\n",
    "\n",
    "\tif type(idx_tuple) == tuple:\n",
    "\t\treturn (benchmark_name_mapping[idx_tuple[0]], idx_tuple[1])\n",
    "\telse:\n",
    "\t\treturn benchmark_name_mapping[idx_tuple]\n",
    "\n",
    "# Apply the replacement function\n",
    "new_index = selected_pivot.index.map(replace_level_0)\n",
    "selected_pivot.index = pd.MultiIndex.from_tuples(new_index, names=selected_pivot.index.names)\n",
    "# display(selected_pivot.columns.get_level_values(2))\n",
    "selected_pivot = selected_pivot.droplevel('alpha', axis=0)\n",
    "selected_pivot = selected_pivot.loc[:, (selected_pivot.columns.get_level_values(2) == 0.000001) | (selected_pivot.columns.get_level_values(2) == 100) | (selected_pivot.columns.get_level_values(2) == 1000) | (selected_pivot.columns.get_level_values(2) == 10000)]\n",
    "\n",
    "# Merge pivot from single model experiments\n",
    "pvt_base_model.index = pvt_base_model.index.map(replace_level_0)\n",
    "\n",
    "if combined_pivot is not None:\n",
    "\tselected_pivot = pd.concat([selected_pivot, pvt_base_model], axis=1)\n",
    "else:\n",
    "\tselected_pivot = pvt_mess_plus\n",
    "\n",
    "benchmark_names = selected_pivot.index\n",
    "\n",
    "# Step 2: Create a new MultiIndex from the stacked data\n",
    "stacked_data = []\n",
    "for v_value in selected_pivot.columns.get_level_values('V').unique():\n",
    "\tfor b_name in benchmark_names:\n",
    "\t\tfor metric in selected_pivot.columns.get_level_values(0).unique():\n",
    "\t\t\tfor agg_type in selected_pivot.columns.get_level_values(1).unique():\n",
    "\t\t\t\tif (metric, agg_type, v_value) in selected_pivot.columns:\n",
    "\t\t\t\t\tvalue = selected_pivot.loc[b_name, (metric, agg_type, v_value)]\n",
    "\t\t\t\t\tstacked_data.append((v_value, b_name, metric, agg_type, value))\n",
    "\n",
    "\n",
    "# Step 3: Create a new DataFrame from the stacked data\n",
    "selected_pivot = pd.DataFrame(stacked_data, columns=['V', 'benchmark_name', 'metric', 'aggregation_type', 'value'])\n",
    "\n",
    "# Step 4: Pivot to get the desired format\n",
    "selected_pivot = selected_pivot.pivot(\n",
    "\tindex='V',\n",
    "\tcolumns=['benchmark_name', 'metric', 'aggregation_type'],\n",
    "\tvalues='value'\n",
    ")\n",
    "\n",
    "# Re-order the rows\n",
    "idx_new = []\n",
    "for row in selected_pivot.index:\n",
    "\tif row < 1:\n",
    "\t\tidx_new.append(100000)\n",
    "\telse:\n",
    "\t\tidx_new.append(row)\n",
    "\n",
    "selected_pivot.index = idx_new\n",
    "selected_pivot.sort_index(inplace=True)\n",
    "\n",
    "selected_pivot.index = [\"Qwen2 0.5B\", \"Qwen2 1.5B\", \"Qwen2 7B\", \"Qwen2.5 32B\", \"Random w. Constraint\", r\"\\textbf{MESS+ (ours)}\"]\n",
    "selected_pivot = selected_pivot.fillna(0)\n",
    "\n",
    "def multiply_if_small(x):\n",
    "\tif isinstance(x, (int, float)) and x <= 1 and x != 0:  # Exclude zero if needed\n",
    "\t\treturn x * 100\n",
    "\treturn x\n",
    "\n",
    "# Apply the function to all elements in the DataFrame\n",
    "selected_pivot = selected_pivot.applymap(multiply_if_small)\n",
    "\n",
    "selected_pivot.loc[:, (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"sum\")] /= 1_000_000\n",
    "selected_pivot.loc[:, (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"std\")] /= 1_000_000\n",
    "selected_pivot.loc[selected_pivot.index == r\"\\textbf{MESS+ (ours)}\", (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"sum\")] /= 9\n",
    "\n",
    "# We add a new \"AVERAGE L1 ITEM\n",
    "level2_level3_means = selected_pivot.groupby(level=[1, 2], axis=1).mean()\n",
    "new_column_tuples = [(\"Mean\",) + col for col in level2_level3_means.columns]\n",
    "new_columns = pd.MultiIndex.from_tuples(new_column_tuples)\n",
    "\n",
    "# Create a DataFrame with the new columns\n",
    "means_df = pd.DataFrame(level2_level3_means.values, index=selected_pivot.index, columns=new_columns)\n",
    "\n",
    "# Concatenate the original DataFrame with the means DataFrame\n",
    "selected_pivot = pd.concat([selected_pivot, means_df], axis=1)\n",
    "\n",
    "selected_pivot.loc[(selected_pivot.index == \"Qwen 2 0.5B\") | (selected_pivot.index == \"Qwen2 1.5B\") | (selected_pivot.index == \"Qwen2 7B\") | (selected_pivot.index == \"Qwen2.5 32B\"), (selected_pivot.columns.get_level_values(1) == \"avg_accuracy\") & (selected_pivot.columns.get_level_values(2) == \"std\")] /= 9 # We ran raw inference calls on 3 seeds and then ran our experiments on 3 seeds again, i.e., 3x3 = 9\n",
    "\n",
    "selected_pivot.loc[(selected_pivot.index == \"Qwen2 0.5B\"), (selected_pivot.columns.get_level_values(1) == \"final_models/xsmall_chosen\") & (selected_pivot.columns.get_level_values(2) == \"mean\")] = 100.0\n",
    "selected_pivot.loc[(selected_pivot.index == \"Qwen2 1.5B\"), (selected_pivot.columns.get_level_values(1) == \"final_models/small_chosen\") & (selected_pivot.columns.get_level_values(2) == \"mean\")] = 100.0\n",
    "selected_pivot.loc[(selected_pivot.index == \"Qwen2 7B\"), (selected_pivot.columns.get_level_values(1) == \"final_models/medium_chosen\") & (selected_pivot.columns.get_level_values(2) == \"mean\")] = 100.0\n",
    "selected_pivot.loc[(selected_pivot.index == \"Qwen2.5 32B\"), (selected_pivot.columns.get_level_values(1) == \"final_models/large_chosen\") & (selected_pivot.columns.get_level_values(2) == \"mean\")] = 100.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# display(selected_pivot)"
   ],
   "id": "e9db668c366e50e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "# CREATE LATEX TABLE\n",
    "def multiindex_df_to_latex_chunked(\n",
    "\t\tdf,\n",
    "\t\tchunk_size=3,\n",
    "\t\tlevel2_order=None,\n",
    "\t\tcaption_template=\"Results Table Part {}\",\n",
    "\t\tlabel_template=\"tab:results_part{}\", include_index=True\n",
    "):\n",
    "\t# Get unique level 1 items\n",
    "\tlevel1_items = df.columns.get_level_values(0).unique()\n",
    "\n",
    "\t# Get level 2 items (either in specified order or existing order)\n",
    "\tif level2_order is None:\n",
    "\t\tlevel2_items = df.columns.get_level_values(1).unique()\n",
    "\telse:\n",
    "\t\t# Verify all specified level2 items exist in the DataFrame\n",
    "\t\texisting_level2 = df.columns.get_level_values(1).unique()\n",
    "\t\tfor item in level2_order:\n",
    "\t\t\tif item not in existing_level2:\n",
    "\t\t\t\traise ValueError(f\"Level 2 item '{item}' not found in DataFrame\")\n",
    "\t\tlevel2_items = level2_order\n",
    "\n",
    "\t# Calculate number of chunks\n",
    "\tnum_chunks = math.ceil(len(level1_items) / chunk_size)\n",
    "\n",
    "\tlatex_tables = []\n",
    "\n",
    "\t# Process each chunk\n",
    "\tfor chunk_idx in range(num_chunks):\n",
    "\t\tstart_idx = chunk_idx * chunk_size\n",
    "\t\tend_idx = min((chunk_idx + 1) * chunk_size, len(level1_items))\n",
    "\n",
    "\t\t# Get level 1 items for this chunk\n",
    "\t\tchunk_level1_items = level1_items[start_idx:end_idx]\n",
    "\n",
    "\t\t# Filter DataFrame to only include these level 1 items\n",
    "\t\tchunk_columns = [col for col in df.columns if col[0] in chunk_level1_items]\n",
    "\t\tchunk_df = df[chunk_columns]\n",
    "\n",
    "\t\t# Create a new DataFrame for the LaTeX output with the same higher-level structure\n",
    "\t\t# Get unique combinations of first two levels in this chunk\n",
    "\t\thigher_levels = chunk_df.columns.droplevel(2).unique()\n",
    "\n",
    "\t\t# Create new DataFrame with appropriate multi-index\n",
    "\t\tresult_df = pd.DataFrame(index=df.index)\n",
    "\t\tresult_cols = []\n",
    "\n",
    "\t\t# For each combination of higher levels, combine mean and std\n",
    "\t\tall_alph = []\n",
    "\t\tfor level1 in chunk_level1_items:\n",
    "\t\t\tpattern = r\"\\\\alpha\\s*=\\s*(\\d+)\\\\%\"\n",
    "\t\t\tmatch = re.search(pattern, level1)\n",
    "\t\t\tif match:\n",
    "\t\t\t    number = match.group(1)  # This will be \"50\" as a string\n",
    "\t\t\t    number_int = int(number)  # Convert to integer if needed\n",
    "\t\t\telse:\n",
    "\t\t\t    print(\"No number found\")\n",
    "\t\t\t    number_int = 0\n",
    "\n",
    "\t\t\talph = number_int\n",
    "\t\t\tall_alph.append(alph)\n",
    "\n",
    "\t\t\tfor level2 in level2_items:\n",
    "\t\t\t\tif \"final_models/large\" in level2:\n",
    "\t\t\t\t\tname = r\"\\thead{Model Call Ratio \\\\ (Q32B/Q7B/Q1.5B/Q0.5B)}\"\n",
    "\t\t\t\t\tresult_df[(level1, name)] = [f\"{w:.0f}\\\\% / {x:.0f}\\\\% / {y:.0f}\\\\% / {z:.0f}\\\\%\" for w, x, y, z in zip(df[(level1, \"final_models/large_chosen\", \"mean\")], df[(level1, \"final_models/medium_chosen\", \"mean\")], df[(level1, \"final_models/small_chosen\", \"mean\")], df[(level1, \"final_models/xsmall_chosen\", \"mean\")])]\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif level2 == \"avg_accuracy\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Request. \\\\ Satisfaction}\"\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tfor m, s in zip(mean_val, std_val):\n",
    "\t\t\t\t\t\t\tif m >= alph:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{darkgreen}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{red}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\n",
    "\t\t\t\t\telif level2 == \"mess_plus/energy\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Operating \\\\ Cost}\"\n",
    "\t\t\t\t\t\tlevel3 = \"sum\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tmin_mean_val = mean_val[3:].min()\n",
    "\n",
    "\t\t\t\t\t\tmean_acc = df[level1, \"avg_accuracy\", \"mean\"]\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tacc_match = mean_acc[:4] >= alph\n",
    "\t\t\t\t\t\tis_min_single_satisfying = np.where(acc_match == True)[0]\n",
    "\t\t\t\t\t\tfor idx, (m, s) in enumerate(zip(mean_val, std_val)):\n",
    "\t\t\t\t\t\t\tif m == mean_val[4:].min():\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textbf{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telif len(is_min_single_satisfying) > 0 and idx == is_min_single_satisfying[0]:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\underline{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\")\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tname = level2\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tvals = [f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\" for m, s in zip(mean_val, std_val)]\n",
    "\n",
    "\t\t\t\t\tresult_df[(level1, name)] = vals\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t# Skip if mean or std not available\n",
    "\t\t\t\t\tprint(f\"Warning: Missing mean or std for {level1}, {level2}\")\n",
    "\n",
    "\t\t# Set the columns with multi-index (preserving top 2 levels)\n",
    "\t\tresult_df.columns = pd.MultiIndex.from_tuples(result_cols, names=['Category', 'Subcategory'])\n",
    "\n",
    "\t\t# Convert to LaTeX with multi-index\n",
    "\t\tcaption = caption_template.format(chunk_idx + 1)\n",
    "\t\tlabel = label_template.format(chunk_idx + 1)\n",
    "\n",
    "\t\tlatex_str = result_df.to_latex(escape=False, multicolumn=True, multicolumn_format='c', index=include_index)\n",
    "\n",
    "\t\t# Add caption and label\n",
    "\t\tlatex_str = latex_str.replace('\\\\begin{tabular}',\n",
    "\t\t                              f'\\\\begin{{table}}\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{{tabular}}')\n",
    "\t\tlatex_str = latex_str + '\\\\end{table}'\n",
    "\t\tlatex_str = latex_str.replace(\"\\\\toprule\", \"\\\\cmidrule(lr){1-1}\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-10}\")\n",
    "\t\tlatex_str = latex_str.replace(\"\\\\midrule\", \"\\\\cmidrule(lr){1-1}\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-10}\")\n",
    "\t\tlatex_str = latex_str.replace(\"\\\\bottomrule\", \"\\\\cmidrule(lr){1-1}\\cmidrule(lr){2-4}\\cmidrule(lr){5-7}\\cmidrule(lr){8-10}\")\n",
    "\n",
    "\t\tlatex_tables.append(latex_str)\n",
    "\n",
    "\treturn latex_tables\n",
    "\n",
    "\n",
    "latex_tables = multiindex_df_to_latex_chunked(\n",
    "\tselected_pivot,\n",
    "\tchunk_size=3,\n",
    "\tcaption_template=\"Results Table Part {}: Categories\",\n",
    "\tlabel_template=\"tab:results_part{}\",\n",
    "\tlevel2_order=[\"mess_plus/energy\", \"avg_accuracy\", \"final_models/large_chosen\"]\n",
    ")\n",
    "\n",
    "for t in latex_tables:\n",
    "\tprint(t)"
   ],
   "id": "19a5c28b0517cd23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7473856edc48c547",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
