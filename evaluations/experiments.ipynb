{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "from plots.utils.plotting import write_figure_to_disk\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"experiments.ipynb\").absolute().parent\n",
    "\n",
    "DATA_DIR = f\"{NOTEBOOK_PATH}/data/online\"\n",
    "\n",
    "BENCHMARK_NAMES = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\"]\n",
    "# BENCHMARK_NAMES = [\"winogrande\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess-plus_3-models_online_vFINAL\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")"
   ],
   "id": "440a498da98688a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(run_summary_df)\n",
    "run_df = load_all_histories_to_dataframe(DATA_DIR)\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"benchmark_name\"] = name\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"] = run_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"].str.replace(f\"{name}_\", \"\")\n",
    "\n",
    "run_df[[\"V\", \"alpha\", \"c\", \"seed\"]] = run_df[\"run_name\"].str.split(\"_\", expand=True)\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].str.replace(\"a=\", \"\")\n",
    "run_df[\"V\"] = run_df[\"V\"].str.replace(\"V=\", \"\")\n",
    "run_df[\"c\"] = run_df[\"c\"].str.replace(\"c=\", \"\")\n",
    "run_df[\"seed\"] = run_df[\"seed\"].str.replace(\"seed=\", \"\")\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].astype(float)\n",
    "run_df[\"V\"] = run_df[\"V\"].astype(float)\n",
    "run_df[\"c\"] = run_df[\"c\"].astype(float)\n",
    "run_df[\"seed\"] = run_df[\"seed\"].astype(int)\n",
    "\n",
    "run_df[\"models/small_chosen\"] = run_df[\"models/small_chosen\"].astype(float)\n",
    "run_df[\"models/medium_chosen\"] = run_df[\"models/medium_chosen\"].astype(float)\n",
    "run_df[\"models/large_chosen\"] = run_df[\"models/large_chosen\"].astype(float)\n",
    "\n",
    "display(run_df.head())"
   ],
   "id": "52264c7618a8265f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_df = run_df.loc[(run_df[\"c\"] == 1.0) & (run_df[\"benchmark_name\"] == \"winogrande\")].pivot_table(index=[\"benchmark_name\", \"alpha\", \"V\", \"c\"], values=[\"avg_accuracy\", \"mess_plus/energy\", \"mess_plus/q_length\", \"total_runtime\"], aggfunc={\"avg_accuracy\": \"mean\", \"mess_plus/energy\": \"sum\", \"mess_plus/q_length\": \"mean\", \"total_runtime\": \"max\"})",
   "id": "e20b51f6fc67780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_value_labels(axx, spacing=5, convert_to_mj: bool = True):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in axx.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        if convert_to_mj:\n",
    "            label = f'{y_value / 1_000_000:.1f}' # MJ conversion\n",
    "        else:\n",
    "            label = f'{y_value:.1f}'\n",
    "\n",
    "        # Create annotation\n",
    "        axx.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "def fmt_to_megajoules(x, pos):\n",
    "    return f'{(x / 1_000_000):.0f}'\n"
   ],
   "id": "a38483bf31450f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load raw inference data\n",
    "\n",
    "infer_df = pd.DataFrame()\n",
    "def get_inference_data(benchmark_name):\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{NOTEBOOK_PATH.parent}/data/inference_outputs/{benchmark_name}\")\n",
    "\t\tinput_df[\"idx_original\"] = input_df.index\n",
    "\t\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\treturn input_df\n",
    "\texcept ValueError:\n",
    "\t\treturn pd.DataFrame()\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\tinfer_df = pd.concat([infer_df, get_inference_data(name)], ignore_index=True)\n",
    "\n",
    "infer_df.reset_index(inplace=True)\n",
    "\n",
    "# Get baseline dataframe\n",
    "BASELINE_DATA_DIR = f\"{NOTEBOOK_PATH}/data/random_baseline_v02\"\n",
    "baseline_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess_plus_random_baseline_with_constraint_v02\",\n",
    "    save_dir=BASELINE_DATA_DIR,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "baseline_df = load_all_histories_to_dataframe(BASELINE_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"] = baseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_alpha=\", \"\")\n",
    "\tbaseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"alpha\"] = baseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"run_name\"]\n",
    "\n",
    "\tbaseline_df[\"alpha\"] = baseline_df[\"alpha\"].astype(float)\n",
    "\n",
    "\n",
    "print(baseline_df.head())\n"
   ],
   "id": "ec68ca6f1bfad7d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the RouteLLM evaluations\n",
    "ROUTELLM_DATA_DIR = f\"{NOTEBOOK_PATH}/data/routellm_baseline_v01\"\n",
    "routellm_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routellm_baseline_v01\",\n",
    "    save_dir=ROUTELLM_DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routellm_bs_df = load_all_histories_to_dataframe(ROUTELLM_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\troutellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\troutellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"run_name_updated\"] = routellm_bs_df.loc[routellm_bs_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_\", \"\")\n",
    "\n",
    "\n",
    "routellm_bs_df[[\"alpha\", \"threshold\"]] = routellm_bs_df[\"run_name_updated\"].str.split(\"_\", expand=True)\n",
    "routellm_bs_df[\"alpha\"] = routellm_bs_df[\"alpha\"].str.replace(\"alpha=\", \"\")\n",
    "routellm_bs_df[\"threshold\"] = routellm_bs_df[\"threshold\"].str.replace(\"thres=\", \"\")\n",
    "routellm_bs_df[\"alpha\"] = routellm_bs_df[\"alpha\"].astype(float)\n",
    "routellm_bs_df[\"threshold\"] = routellm_bs_df[\"threshold\"].astype(float)\n"
   ],
   "id": "3a45eef546795658",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the RouterDC evaluations\n",
    "ROUTERDC_DATA_DIR = f\"{NOTEBOOK_PATH}/data/routerdc_baseline_v01\"\n",
    "routellm_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routerdc_baseline_v01\",\n",
    "    save_dir=ROUTERDC_DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routerdc_bs_df = load_all_histories_to_dataframe(ROUTERDC_DATA_DIR)\n",
    "routerdc_bs_df[[\"benchmark_name\", \"alpha\"]] = routerdc_bs_df[\"run_name\"].str.replace(\"routerdc-\", \"\").str.split(\"-\", expand=True)\n",
    "routerdc_bs_df[\"alpha\"] = routerdc_bs_df[\"alpha\"].str.replace(\"alpha=\", \"\").astype(float)\n",
    "\n",
    "display(routerdc_bs_df)"
   ],
   "id": "4f88a5c39cd1ae93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1267a961aec3b537",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ROUTELLM_THRESHOLD_ALPHA_MAP = {}\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\troutellm_subset = routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == benchmark)]\n",
    "\n",
    "\tif len(routellm_subset) == 0:\n",
    "\t\tcontinue\n",
    "\n",
    "\t# sns.set_style(\"whitegrid\")\n",
    "\t# sns.set_theme(context='paper', style='whitegrid', palette='colorblind', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\t#\n",
    "\t# # Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "\t# fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\n",
    "\n",
    "\talpha_vals_bm = routellm_subset[\"alpha\"].unique().tolist()\n",
    "\tthreshold_vals = routellm_subset[\"threshold\"].sort_values(ascending=False).unique().tolist()\n",
    "\n",
    "\tthres_acc_mapping = {i: 0.0 for i in threshold_vals}\n",
    "\talpha_thresh_mapping = {a: 0.0 for a in alpha_vals_bm}\n",
    "\t# Get final accuracy for each threshold\n",
    "\tfor alpha in alpha_vals_bm:\n",
    "\t\talpha_thres_value_match = False\n",
    "\t\tfor thresh in threshold_vals:\n",
    "\n",
    "\t\t\tif alpha_thres_value_match is True:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tdata = routellm_subset.loc[(routellm_subset[\"alpha\"] == alpha) & (routellm_subset[\"threshold\"] == thresh)]\n",
    "\t\t\tthreshold_accuracy = data.loc[data[\"_step\"] > data[\"_step\"].max() - 30, [\"avg_accuracy\"]].mean()\n",
    "\n",
    "\t\t\tif threshold_accuracy.item() >= alpha:\n",
    "\t\t\t\talpha_thresh_mapping[alpha] = thresh\n",
    "\t\t\t\talpha_thres_value_match = True\n",
    "\n",
    "\tROUTELLM_THRESHOLD_ALPHA_MAP[benchmark] = alpha_thresh_mapping\n",
    "\n",
    "display(ROUTELLM_THRESHOLD_ALPHA_MAP)\n"
   ],
   "id": "315de4977bc810af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v_values_per_benchmark = {\n",
    "    \"arc_challenge\": [0.001, 0.0001, 0.00001],\n",
    "    \"arc_easy\": [0.01, 0.001, 0.0001],\n",
    "    \"boolq\": [0.01, 0.001, 0.0001],\n",
    "    # \"lambada_standard\": [0.01, 0.001, 0.0001],\n",
    "    \"logiqa\": [0.001, 0.0001, 0.00001],\n",
    "    # \"logiqa2\": [0.01, 0.001, 0.0001],\n",
    "    \"piqa\": [0.01, 0.001, 0.0001],\n",
    "    \"sciq\": [0.0001, 0.00001, 0.000001],\n",
    "    \"social_iqa\": [0.001, 0.0001, 0.00001],\n",
    "    \"winogrande\": [0.01, 0.001, 0.0001],\n",
    "}"
   ],
   "id": "57657572dfc3321f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.1, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(15, 5), gridspec_kw={'width_ratios': [4, 1, 1, 1, 1, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"winogrande\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# alpha = target_alpha_per_benchmark[name]\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\taxes[iterator][0].text(s=\"L1B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].text(s=\"L8B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].text(s=\"L70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][0],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "\n",
    "\taxes[iterator][0].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\t# Add RouteLLM baseline\n",
    "\taxes[iterator][0].plot(\n",
    "\t\troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"],\n",
    "\t\troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"avg_accuracy\"],\n",
    "\t\tcolor=\"brown\", linestyle=\"dotted\", label=\"RouteLLM\"\n",
    "\t)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][0].legend(loc=\"center\", ncol=2, title=\"Method\", fontsize=8, title_fontsize=8)\n",
    "\n",
    "\taxes[iterator][0].axhline(y=alpha, color='red', linestyle='-', label=\"alpha\")\n",
    "\taxes[iterator][0].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\taxes[iterator][0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator][0].text(s=r\"$ \\alpha = {alpha_val} $ (red line)\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=1.15 * raw_inference_accuracies_per_model[\"label_large\"] - 0.04, color='red', fontsize=9, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\t# Stackplot for Model Call Ratio\n",
    "\tv_values_per_benchmark[name] = sorted(v_values_per_benchmark[name], reverse=False)\n",
    "\t# v_values_per_benchmark[name].reverse()\n",
    "\tfor jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "\t\tstack_df = subset.loc[\n",
    "\t\t\t(run_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(run_df[\"V\"] == V) &\n",
    "\t\t\t(subset[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t\tx = stack_df[\"_step\"]\n",
    "\t\ty = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\t\ty_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].set(xlabel=f\"Requests @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "\t\taxes[iterator][1 + jdx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t\taxes[iterator][1 + jdx].set(xlim=[0, stack_df[\"_step\"].max()])\n",
    "\n",
    "\t\tif iterator == 0 and jdx == 0:\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L70B\", x=70, y=0.80, color=\"black\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L8B\", x=70, y=0.40, color=\"white\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L1B\", x=70, y=0.10, color=\"white\")\n",
    "\n",
    "\t# Add area plot for random baseline with constraint.\n",
    "\tbaseline_stack_df = baseline_df.loc[\n",
    "\t\t\t(baseline_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(baseline_df[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\tx_base = baseline_stack_df[\"_step\"]\n",
    "\ty_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\ty_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "\taxes[iterator][4].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\taxes[iterator][4].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "\taxes[iterator][4].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\taxes[iterator][4].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\taxes[iterator][4].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\taxes[iterator][4].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\t# axes[iterator][6].get_yaxis().set_visible(False)\n",
    "\n",
    "\t# Add area plot for RouteLLM baseline\n",
    "\troutellm_stack_df = routellm_bs_df.loc[\n",
    "\t\t\t(routellm_bs_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\tx_rllm = routellm_stack_df[\"_step\"]\n",
    "\ty_rllm = routellm_stack_df[[\"models/small_chosen\", \"models/large_chosen\"]]\n",
    "\ty_stack_rllm = np.cumsum(y_rllm, axis=1)\n",
    "\n",
    "\taxes[iterator][5].fill_between(x_rllm, 0, y_stack_rllm.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\taxes[iterator][5].fill_between(x_rllm, y_stack_rllm.iloc[:, 0], y_stack_rllm.iloc[:, 1], color=\"#69cf81\", alpha=0.95)\n",
    "\t# axes[iterator][6].fill_between(x_rllm, y_stack_rllm.iloc[:, 1], y_stack_rllm.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\taxes[iterator][5].set(xlabel=f\"Req. (RouteLLM)\", xlim=[0, routellm_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\taxes[iterator][5].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\taxes[iterator][5].set(xlim=[0, routellm_stack_df[\"_step\"].max()])\n",
    "\n",
    "\taxes[iterator][0].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][0].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes[iterator], [r\"Avg. User Satisfaction Rate Over Time (varying $\\alpha$)\".format(alpha_val=alpha), \"Model Call Ratio (MCR)\", \"\", \"\"]):\n",
    "\n",
    "\t\tif iterator == 1:\n",
    "\t\t\tax.set_ylabel(col, rotation=90, size=10)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_all_alpha\", chapter_name=\"evaluations\")\n"
   ],
   "id": "7045943bad636e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(4, 7))\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"winogrande\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# Q Plot for SLA violations\n",
    "\tsubset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\tbaseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"sla_violations\"] = baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"mess_plus/q_length\"] / baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"sla_violations\"] = routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"mess_plus/q_length\"] / routellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"sla_violations\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "\n",
    "\taxes[iterator].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"sla_violations\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\t# Add RouteLLM baseline\n",
    "\taxes[iterator].plot(\n",
    "\t\troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"_step\"],\n",
    "\t\troutellm_bs_df.loc[(routellm_bs_df[\"benchmark_name\"] == name) & (routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == ROUTELLM_THRESHOLD_ALPHA_MAP[name][alpha]), \"sla_violations\"],\n",
    "\t\tcolor=\"brown\", linestyle=\"dotted\", label=\"RouteLLM\"\n",
    "\t)\n",
    "\n",
    "\taxes[iterator].set_ylim([0, 0.2])\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator].text(s=r\"$ \\alpha = {alpha_val} $\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=0.17, color='red', fontsize=14, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator].legend(ncols=1, title=\"Method\", fontsize=9, title_fontsize=9, loc='upper left')\n",
    "\n",
    "\taxes[iterator].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\n",
    "\taxes[iterator].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes, [\"\", r\"Avg. SLA Violations Over Time (varying $\\alpha$ values)\", \"\"]):\n",
    "\t\tax.set_ylabel(col, rotation=90, size=18)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_sla_violations\", chapter_name=\"evaluations\")"
   ],
   "id": "a48f9f1d3de21409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(infer_df.columns)\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_large\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_medium\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_small\"].mean())"
   ],
   "id": "3ef80eb1dca358dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Plot generator\n",
    "#\n",
    "# BENCHMARK_NAME_DICT = {\n",
    "#     \"arc_challenge\": \"ARC Challenge\",\n",
    "#     \"arc_easy\": \"ARC Easy\",\n",
    "#     \"boolq\": \"BoolQ\",\n",
    "#     # \"lambada_standard\": \"Lambada\",\n",
    "#     \"logiqa\": \"LogiQA\",\n",
    "#     # \"logiqa2\": \"LogiQA2\",\n",
    "#     \"piqa\": \"PiQA\",\n",
    "#     \"sciq\": \"SciQ\",\n",
    "#     \"social_iqa\": \"SocialIQA\",\n",
    "#     \"winogrande\": \"WinoGrande\",\n",
    "# }\n",
    "#\n",
    "# # Create a list of all benchmark-alpha combinations\n",
    "# benchmark_alpha_combinations = []\n",
    "# for name in v_values_per_benchmark.keys():\n",
    "#     config_path = Path(f\"{NOTEBOOK_PATH.parent}/config/online/{name}.yaml\")\n",
    "#     with config_path.open(\"r\") as f:\n",
    "#         import yaml\n",
    "#         CONFIG = yaml.safe_load(f)\n",
    "#\n",
    "#     algorithm_config = CONFIG[\"algorithm\"]\n",
    "#     for alpha in algorithm_config[\"alpha_values\"]:\n",
    "#         benchmark_alpha_combinations.append((name, alpha))\n",
    "#\n",
    "# # Initialize plotting variables\n",
    "# plot_num = 0\n",
    "# col_count = 0\n",
    "#\n",
    "# # Iterate through all benchmark-alpha combinations\n",
    "# for combo_idx, (name, alpha) in enumerate(benchmark_alpha_combinations):\n",
    "#\n",
    "#     # Create new figure every 6 columns\n",
    "#     if col_count == 0:\n",
    "#         sns.set(style=\"whitegrid\")\n",
    "#         fig, axes = plt.subplots(nrows=7, ncols=6, figsize=(20, 12))\n",
    "#         plot_num += 1\n",
    "#\n",
    "#     # Get current column index\n",
    "#     col_idx = col_count\n",
    "#\n",
    "#     # Skip if this benchmark doesn't have V values configured\n",
    "#     if name not in v_values_per_benchmark.keys():\n",
    "#         continue\n",
    "#\n",
    "#     # Filter data for current benchmark and alpha\n",
    "#     subset = run_df.loc[(run_df[\"benchmark_name\"] == name) &\n",
    "#                        (run_df[\"c\"] == 0.1) &\n",
    "#                        (run_df[\"V\"].isin(v_values_per_benchmark[name])) &\n",
    "#                        (run_df[\"_step\"] > 10) &\n",
    "#                        (run_df[\"alpha\"] == alpha)]\n",
    "#\n",
    "#     v_values = subset[\"V\"].unique().tolist()\n",
    "#\n",
    "#     # Accuracy Plot\n",
    "#     raw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "#\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.025, color='gray', fontsize=8, ha=\"left\")\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.025, color='gray', fontsize=8, ha=\"center\")\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.025, color='gray', fontsize=8, ha=\"right\")\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "#\n",
    "#     sns.lineplot(\n",
    "#         data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "#         x=\"_step\",\n",
    "#         y=\"avg_accuracy\",\n",
    "#         hue=\"V\",\n",
    "#         errorbar=None,\n",
    "#         ax=axes[0][col_idx],\n",
    "#         legend=True if col_idx == 0 else False,\n",
    "# \t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "#     )\n",
    "#\n",
    "#     axes[0][col_idx].plot(\n",
    "# \t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "# \t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "# \t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "# \t)\n",
    "#\n",
    "#     axes[0][col_idx].axhline(y=alpha, color='red', linestyle='-')\n",
    "#     axes[0][col_idx].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=8, ha=\"right\")\n",
    "#\n",
    "#     axes[0][col_idx].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "#\n",
    "#     if col_idx == 0:\n",
    "#         axes[0][col_idx].legend(ncols=2)\n",
    "#\n",
    "#     # Q Plot for SLA violations\n",
    "#     subset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "#     sns.lineplot(\n",
    "#         data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "#         x=\"_step\",\n",
    "#         y=\"sla_violations\",\n",
    "#         hue=\"V\",\n",
    "#         errorbar=None,\n",
    "#         ax=axes[1][col_idx],\n",
    "#         legend=True if col_idx == 0 else False,\n",
    "# \t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "#     )\n",
    "#\n",
    "#     if col_idx == 0:\n",
    "#         axes[1][col_idx].legend(ncols=2)\n",
    "#\n",
    "#     # Energy consumption plot\n",
    "#     random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "#     random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "#     random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "#     random_baseline_energy.reset_index(inplace=True)\n",
    "#\n",
    "#     raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "#     raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "#     raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "#     raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "#     raw_inference_energy_data.reset_index(inplace=True)\n",
    "#\n",
    "#     raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"70B\", \"energy_consumption_medium\": \"8B\", \"energy_consumption_small\": \"1B\"}, inplace=False)\n",
    "#\n",
    "#     raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "#     energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "#\n",
    "#     energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "#\n",
    "#     energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "#     energy_data.reset_index(inplace=True)\n",
    "#     energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "#\n",
    "#     sns.barplot(\n",
    "#         data=energy_data,\n",
    "#         x=\"V\",\n",
    "#         y=\"mess_plus/energy\",\n",
    "#         ax=axes[2][col_idx],\n",
    "#         errorbar=(\"ci\", 0.95),\n",
    "#     )\n",
    "#\n",
    "#     add_value_labels(axes[2][col_idx])\n",
    "#     axes[2][col_idx].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "#     axes[2][col_idx].set(ylim=[0, 2 * energy_data[\"mess_plus/energy\"].max()])\n",
    "#     axes[2][col_idx].tick_params(axis='x', labelrotation=45)\n",
    "#\n",
    "#     # Stackplot for Model Call Ratio\n",
    "#     for jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "#\n",
    "#         stack_df = subset.loc[\n",
    "#             (run_df[\"benchmark_name\"] == name) &\n",
    "#             (run_df[\"V\"] == V) &\n",
    "#             (subset[\"alpha\"] == alpha),\n",
    "#             [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "#         ].groupby([\"_step\"]).mean().reset_index()\n",
    "#\n",
    "#         x = stack_df[\"_step\"]\n",
    "#         y = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "#         y_stack = np.cumsum(y, axis=1)\n",
    "#\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].set(xlabel=f\"Request @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "#         axes[3 + jdx][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "#\n",
    "#         if jdx == 0 and col_idx == 0:\n",
    "#             axes[3 + jdx][col_idx].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"])\n",
    "#\n",
    "#     # Add area plot for random baseline with constraint.\n",
    "#     baseline_stack_df = baseline_df.loc[\n",
    "#             (baseline_df[\"benchmark_name\"] == name) &\n",
    "#             (baseline_df[\"alpha\"] == alpha),\n",
    "#             [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "#         ].groupby([\"_step\"]).mean().reset_index()\n",
    "#\n",
    "#     x_base = baseline_stack_df[\"_step\"]\n",
    "#     y_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "#     y_stack_base = np.cumsum(y_base, axis=1)\n",
    "#\n",
    "#     axes[6][col_idx].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "#     axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "#     axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "#     axes[6][col_idx].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "#     axes[6][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "#     axes[6][col_idx].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "#\n",
    "#     # Set axis properties\n",
    "#     axes[0][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "#     axes[1][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "#     axes[2][col_idx].set(xlabel=\"\")\n",
    "#\n",
    "#     # Remove y-labels for columns after the first\n",
    "#     if col_idx > 0:\n",
    "#         axes[0][col_idx].set(ylabel=None)\n",
    "#         axes[1][col_idx].set(ylabel=None)\n",
    "#         axes[2][col_idx].set(ylabel=None)\n",
    "#\n",
    "#     # Set title for each column\n",
    "#     axes[0][col_idx].set_title(r\"{bm_name} ($\\alpha = {alpha_val} $)\".format(bm_name=BENCHMARK_NAME_DICT[name], alpha_val=alpha))\n",
    "#\n",
    "#     # Increment column counter\n",
    "#     col_count += 1\n",
    "#\n",
    "#     # Check if we need to save the current figure and start a new one\n",
    "#     if col_count == 6 or combo_idx == len(benchmark_alpha_combinations) - 1:\n",
    "#         # Add row labels\n",
    "#         for idx, (ax, row) in enumerate(zip(axes[:,0], [\"User Satisfaction\", \"SLA Violations\", \"Cost (in MJ energy)\", \"\", \"\", \"\", \"\"])):\n",
    "#             if idx == 5:\n",
    "#                 fig.text(0.003, 0.225, \"Model Call Ratio (MCR)\", ha=\"center\", rotation='vertical', fontsize=plt.rcParams['axes.labelsize'])\n",
    "#             else:\n",
    "#                 ax.set_ylabel(row, rotation=90, size='large')\n",
    "#\n",
    "#         # Save the figure\n",
    "#         fig.tight_layout()\n",
    "#         write_figure_to_disk(plt, file_name=f\"benchmark_performance_plot_{plot_num}\", chapter_name=\"evaluations\")\n",
    "#\n",
    "#         # Reset column counter for next figure\n",
    "#         col_count = 0"
   ],
   "id": "989db16e594ce6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dataframe_to_latex_with_color(df, high_color='green!60', low_color='red!60'):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a LaTeX table with color coding for accuracy columns.\n",
    "\n",
    "    Parameters:\n",
    "    df: pandas DataFrame (must have 'alpha' in the index or as a column)\n",
    "    high_color: LaTeX color for values >= threshold\n",
    "    low_color: LaTeX color for values < threshold\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Check if we have a MultiIndex with alpha\n",
    "    alpha_from_index = False\n",
    "    if isinstance(df_copy.index, pd.MultiIndex) and 'alpha' in df_copy.index.names:\n",
    "        alpha_from_index = True\n",
    "        # Reset index to get alpha as a regular column\n",
    "        df_copy = df_copy.reset_index()\n",
    "\n",
    "    # Round all numeric values to 2 decimal places\n",
    "    df_rounded = df_copy.round(2)\n",
    "\n",
    "    # Function to determine if a value should be colored\n",
    "    def should_color(col_name):\n",
    "        # Color if 'accuracy' is in the column name (case insensitive)\n",
    "        return 'accuracy' in col_name.lower() or 'label' in col_name.lower()\n",
    "\n",
    "    # Function to format a cell with color coding\n",
    "    def format_cell(value, col_name, alpha):\n",
    "        # Convert value to scalar if it's a Series\n",
    "        if hasattr(value, 'item'):\n",
    "            try:\n",
    "                value = value.item()\n",
    "            except ValueError:\n",
    "                # If multiple values, take the first one\n",
    "                value = value.iloc[0] if hasattr(value, 'iloc') else value\n",
    "\n",
    "        # Ensure alpha is also a scalar\n",
    "        if hasattr(alpha, 'item'):\n",
    "            try:\n",
    "                alpha = alpha.item()\n",
    "            except ValueError:\n",
    "                alpha = alpha.iloc[0] if hasattr(alpha, 'iloc') else alpha\n",
    "\n",
    "        if should_color(col_name) and pd.notna(value):\n",
    "            try:\n",
    "                if float(value) >= float(alpha):\n",
    "                    return f\"\\\\cellcolor{{{high_color}}}{float(value):.2f}\"\n",
    "                else:\n",
    "                    return f\"\\\\cellcolor{{{low_color}}}{float(value):.2f}\"\n",
    "            except (ValueError, TypeError):\n",
    "                # If value cannot be converted to float, return as is\n",
    "                return str(value)\n",
    "        else:\n",
    "            if pd.notna(value):\n",
    "                if isinstance(value, (int, float)):\n",
    "                    return f\"{float(value):.2f}\"\n",
    "                else:\n",
    "                    return str(value)\n",
    "            else:\n",
    "                return \"-\"\n",
    "\n",
    "    # Start building the LaTeX table\n",
    "    latex_lines = []\n",
    "    latex_lines.append(\"\\\\begin{table}[h!]\")\n",
    "    latex_lines.append(\"\\\\centering\")\n",
    "\n",
    "    # Calculate total columns (alpha + benchmark + data columns)\n",
    "    # No index column, just alpha, benchmark, and data columns\n",
    "    data_cols = len(df_copy.columns)\n",
    "    if alpha_from_index:\n",
    "        data_cols -= 2  # Remove benchmark and alpha from the count\n",
    "    elif 'alpha' in df_copy.columns:\n",
    "        data_cols -= 1  # Remove alpha from the count\n",
    "    else:\n",
    "        raise ValueError(\"Alpha must be either in the index or as a column for color coding\")\n",
    "\n",
    "    total_cols = data_cols + 2  # alpha + benchmark + data columns\n",
    "\n",
    "    latex_lines.append(\"\\\\begin{tabular}{\" + \"c\" * total_cols + \"}\")\n",
    "    latex_lines.append(\"\\\\toprule\")\n",
    "\n",
    "    # Handle multi-level columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Get the number of levels\n",
    "        n_levels = df.columns.nlevels\n",
    "\n",
    "        # Create header rows for each level\n",
    "        for level in range(n_levels):\n",
    "            header_row = []\n",
    "            if level == 0:\n",
    "                header_row.append(\"\\\\multirow{\" + str(n_levels) + \"}{*}{Alpha}\")\n",
    "                header_row.append(\"\\\\multirow{\" + str(n_levels) + \"}{*}{Benchmark}\")\n",
    "            else:\n",
    "                header_row.append(\"\")\n",
    "                header_row.append(\"\")\n",
    "\n",
    "            # Get labels for this level\n",
    "            labels = []\n",
    "            spans = []\n",
    "            current_label = None\n",
    "            current_span = 0\n",
    "\n",
    "            for col in df.columns:\n",
    "                # Skip index-related columns\n",
    "                if alpha_from_index and isinstance(col, str) and col == 'alpha':\n",
    "                    continue\n",
    "                if alpha_from_index and isinstance(col, str) and col == df_copy.columns[0]:  # Skip benchmark column\n",
    "                    continue\n",
    "\n",
    "                label = str(col[level]) if isinstance(col, tuple) else str(col)\n",
    "                if label != current_label:\n",
    "                    if current_label is not None:\n",
    "                        labels.append((current_label, current_span))\n",
    "                    current_label = label\n",
    "                    current_span = 1\n",
    "                else:\n",
    "                    current_span += 1\n",
    "\n",
    "            # Don't forget the last group\n",
    "            if current_label is not None:\n",
    "                labels.append((current_label, current_span))\n",
    "\n",
    "            # Add multicolumn headers\n",
    "            for label, span in labels:\n",
    "                # Add (MJ) to energy-related headers\n",
    "                if any(energy_term in str(label).lower() for energy_term in ['energy', 'consumption', 'mess_plus']):\n",
    "                    label = f\"{label} (MJ)\"\n",
    "\n",
    "                if span > 1:\n",
    "                    header_row.append(f\"\\\\multicolumn{{{span}}}{{c}}{{{label}}}\")\n",
    "                else:\n",
    "                    header_row.append(label)\n",
    "\n",
    "            latex_lines.append(\" & \".join(header_row) + \" \\\\\\\\\")\n",
    "\n",
    "        # Add horizontal line between header levels\n",
    "        latex_lines.append(\"\\\\cmidrule{3-\" + str(total_cols) + \"}\")\n",
    "    else:\n",
    "        # Simple single-level header\n",
    "        header_row = [\"Alpha\", \"Benchmark\"]\n",
    "\n",
    "        for col in df_copy.columns:\n",
    "            # Skip columns that are already handled\n",
    "            if alpha_from_index and (col == 'alpha' or col == df_copy.columns[0]):\n",
    "                continue\n",
    "            elif not alpha_from_index and col == 'alpha':\n",
    "                continue\n",
    "            else:\n",
    "                col_str = str(col)\n",
    "                # Add (MJ) to energy-related headers\n",
    "                if any(energy_term in col_str.lower() for energy_term in ['energy', 'consumption', 'mess_plus']):\n",
    "                    col_str = f\"{col_str} (MJ)\"\n",
    "                header_row.append(col_str)\n",
    "        latex_lines.append(\" & \".join(header_row) + \" \\\\\\\\\")\n",
    "\n",
    "    latex_lines.append(\"\\\\midrule\")\n",
    "\n",
    "    # Add data rows\n",
    "    for idx, row in df_rounded.iterrows():\n",
    "        # Handle MultiIndex row (if it still exists, though we reset_index above)\n",
    "        if isinstance(idx, tuple):\n",
    "            benchmark_name = idx[0]\n",
    "            alpha_value = idx[1] if len(idx) > 1 else None\n",
    "        else:\n",
    "            if alpha_from_index:\n",
    "                # Properly extract the benchmark name value\n",
    "                benchmark_value = row[df_copy.columns[0]]\n",
    "                if hasattr(benchmark_value, 'item'):\n",
    "                    try:\n",
    "                        benchmark_name = benchmark_value.item()\n",
    "                    except ValueError:\n",
    "                        benchmark_name = benchmark_value.iloc[0] if hasattr(benchmark_value, 'iloc') else benchmark_value\n",
    "                else:\n",
    "                    benchmark_name = benchmark_value\n",
    "\n",
    "                alpha_value = row['alpha']\n",
    "            else:\n",
    "                benchmark_name = str(idx)\n",
    "                alpha_value = None\n",
    "\n",
    "        # Get alpha value for display and color coding\n",
    "        if alpha_value is None and 'alpha' in row:\n",
    "            alpha_value = row['alpha']\n",
    "\n",
    "        # Convert alpha_value to scalar if necessary\n",
    "        if hasattr(alpha_value, 'item'):\n",
    "            try:\n",
    "                alpha_value = alpha_value.item()\n",
    "            except ValueError:\n",
    "                alpha_value = alpha_value.iloc[0] if hasattr(alpha_value, 'iloc') else alpha_value\n",
    "\n",
    "        # If we still don't have alpha, raise an error\n",
    "        if alpha_value is None:\n",
    "            raise ValueError(\"Alpha must be either in the index or as a column for color coding\")\n",
    "\n",
    "        # Format the row - Alpha first, then Benchmark\n",
    "        # Make sure benchmark_name is a string\n",
    "        benchmark_name_str = str(benchmark_name)\n",
    "        row_data = [f\"{float(alpha_value):.2f}\", benchmark_name_str]\n",
    "\n",
    "        for col_name, value in row.items():\n",
    "            # Skip columns we've already handled\n",
    "            if alpha_from_index and (col_name == 'alpha' or col_name == df_copy.columns[0]):\n",
    "                continue\n",
    "            elif not alpha_from_index and col_name == 'alpha':\n",
    "                continue\n",
    "\n",
    "            # Get the actual column name for color checking\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                actual_col_name = \" \".join(str(c) for c in col_name)\n",
    "            else:\n",
    "                actual_col_name = str(col_name)\n",
    "\n",
    "            formatted_value = format_cell(value, actual_col_name, alpha_value)\n",
    "            row_data.append(formatted_value)\n",
    "\n",
    "        latex_lines.append(\" & \".join(row_data) + \" \\\\\\\\\")\n",
    "\n",
    "    latex_lines.append(\"\\\\bottomrule\")\n",
    "    latex_lines.append(\"\\\\end{tabular}\")\n",
    "\n",
    "    # Add caption and label\n",
    "    latex_lines.append(\"\\\\caption{Comparison of Model Performance}\")\n",
    "    latex_lines.append(\"\\\\label{tab:model_comparison}\")\n",
    "    latex_lines.append(\"\\\\end{table}\")\n",
    "\n",
    "    # Add required packages in comments\n",
    "    packages = [\n",
    "        \"% Required LaTeX packages:\",\n",
    "        \"% \\\\usepackage{booktabs}\",\n",
    "        \"% \\\\usepackage{multirow}\",\n",
    "        \"% \\\\usepackage{xcolor}\",\n",
    "        \"% \\\\usepackage{colortbl}\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(packages + latex_lines)"
   ],
   "id": "b2cfd6bf9a6d55d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_pivot_table_for_main_results(input_df: pd.DataFrame, model_cols: list):\n",
    "\n",
    "\tlatest_steps = input_df.groupby(['benchmark_name', 'alpha'])['_step'].transform('max')\n",
    "\tis_last_step = input_df['_step'] == latest_steps\n",
    "\n",
    "\t# Create new columns with the final values\n",
    "\tfor col in model_cols:\n",
    "\t    final_values = input_df.loc[is_last_step, ['benchmark_name', 'alpha', col]]\n",
    "\t    final_values = final_values.drop_duplicates(['benchmark_name', 'alpha'])\n",
    "\t    input_df = pd.merge(\n",
    "\t        input_df,\n",
    "\t\t    final_values.rename(columns={col: f\"final_{col}\"}),\n",
    "\t        on=['benchmark_name', 'alpha'],\n",
    "\t        how='left'\n",
    "\t    )\n",
    "\n",
    "\t# Add the final model values to the pivot table\n",
    "\tmerged_pvt_table = input_df.loc[:, [\"benchmark_name\", \"alpha\", \"V\", \"avg_accuracy\", \"mess_plus/energy\"] + [f\"final_{col}\" for col in model_cols]].pivot_table(\n",
    "\t    index=[\"benchmark_name\", \"alpha\"],\n",
    "\t    columns=[\"V\"],\n",
    "\t    values=[\"avg_accuracy\", \"mess_plus/energy\"] + [f\"final_{col}\" for col in model_cols],\n",
    "\t    aggfunc={\n",
    "\t        \"avg_accuracy\": [\"mean\", \"std\"],\n",
    "\t        \"mess_plus/energy\": [\"sum\", \"std\"],\n",
    "\t        **{f\"final_{col}\": ['first'] for col in model_cols}\n",
    "\t    }\n",
    "\t)\n",
    "\n",
    "\treturn merged_pvt_table\n",
    "\n"
   ],
   "id": "77532abbf7c205f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create pivot tables for our random baseline, RouteLLM, and MESS+\n",
    "\n",
    "# SINGLE MODEL\n",
    "pvt_base_model = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"]].pivot_table(\n",
    "    index=[\"benchmark_name\"],\n",
    "    values=[\"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"],\n",
    "    aggfunc={\n",
    "        \"energy_consumption_large\": [\"sum\", \"std\"],\n",
    "        \"energy_consumption_medium\": [\"sum\", \"std\"],\n",
    "        \"energy_consumption_small\": [\"sum\", \"std\"],\n",
    "        \"label_small\": [\"mean\", \"std\"],\n",
    "        \"label_medium\": [\"mean\", \"std\"],\n",
    "        \"label_large\": [\"mean\", \"std\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "pvt_base_model.columns = pd.MultiIndex.from_tuples(\n",
    "    map(lambda x: (x[0], x[1], 2), pvt_base_model.columns)\n",
    ")\n",
    "\n",
    "new_cols = []\n",
    "for col in pvt_base_model.columns:\n",
    "\tif col[0] == \"energy_consumption_small\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 2))\n",
    "\telif col[0] == \"energy_consumption_medium\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 3))\n",
    "\telif col[0] == \"energy_consumption_large\":\n",
    "\t\tnew_cols.append((\"mess_plus/energy\", col[1], 4))\n",
    "\telif col[0] == \"label_small\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 2))\n",
    "\telif col[0] == \"label_medium\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 3))\n",
    "\telif col[0] == \"label_large\":\n",
    "\t\tnew_cols.append((\"avg_accuracy\", col[1], 4))\n",
    "\n",
    "pvt_base_model.columns = pd.MultiIndex.from_tuples(new_cols, names=[None, None, \"V\"])\n",
    "\n",
    "# Add the new \"final_models/{size}_chosen\" columns with values of 1.0\n",
    "for size in ['small', 'medium', 'large']:\n",
    "\tnum_size = 2\n",
    "\tif size == 'small':\n",
    "\t\tnum_size = 2\n",
    "\telif size == 'medium':\n",
    "\t\tnum_size = 3\n",
    "\telif size == \"large\":\n",
    "\t\tnum_size = 4\n",
    "\n",
    "\tpvt_base_model[(f'final_models/{size}_chosen'), \"first\", num_size] = 1.0\n",
    "\n",
    "# Sort the columns for better organization (by size group)\n",
    "pvt_base_model = pvt_base_model.sort_index(axis=1, level=0)\n",
    "\n",
    "# RouteLLM\n",
    "filtered_routellm_df = pd.DataFrame()\n",
    "for benchmark_name, val_dict in ROUTELLM_THRESHOLD_ALPHA_MAP.items():\n",
    "\tfor alpha, threshold in val_dict.items():\n",
    "\t\tfiltered_routellm_df = pd.concat([\n",
    "\t\t\tfiltered_routellm_df,\n",
    "\t\t\troutellm_bs_df.loc[(routellm_bs_df[\"alpha\"] == alpha) & (routellm_bs_df[\"threshold\"] == threshold) & (routellm_bs_df[\"benchmark_name\"] == benchmark_name)]\n",
    "\t\t], ignore_index=True)\n",
    "\n",
    "# Convert to Megajoule\n",
    "# filtered_routellm_df[\"mess_plus/energy\"] = filtered_routellm_df[\"mess_plus/energy\"] / 1_000_000\n",
    "filtered_routellm_df[\"V\"] = 1000 # Dummy to identify RouteLLM in the final latex output.\n",
    "pvt_routellm = build_pivot_table_for_main_results(filtered_routellm_df, [\"models/small_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "# RouterDC baseline\n",
    "routerdc_bs_df[\"V\"] = 10000\n",
    "pvt_routerdc_baseline = build_pivot_table_for_main_results(routerdc_bs_df, [\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "baseline_df[\"V\"] = 100\n",
    "pvt_rand_baseline = build_pivot_table_for_main_results(baseline_df, [\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "# This is an intermediary step to average across seeds.\n",
    "pvt_mess_plus = build_pivot_table_for_main_results(run_df, [\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"])\n",
    "\n",
    "combined_pivot = pd.concat([pvt_mess_plus, pvt_rand_baseline, pvt_routerdc_baseline, pvt_routellm], axis=1)\n",
    "\n",
    "selected_pivot = combined_pivot.iloc[1::3]\n",
    "selected_pivot = selected_pivot.query('benchmark_name != \"lambada_standard\" & benchmark_name != \"logiqa2\"')\n",
    "\n",
    "print(\"All experiments combined\")\n",
    "def replace_level_0(idx_tuple):\n",
    "\t# Replace only 'A' with 'X', leave others unchanged\n",
    "\tbenchmark_name_mapping = {\n",
    "\t\t\"arc_challenge\": r\"ARC Challenge ($\\alpha = 50\\%$)\",\n",
    "\t\t\"arc_easy\": r\"ARC Easy ($\\alpha = 75\\%$)\",\n",
    "\t\t\"boolq\": r\"BoolQ  ($\\alpha = 80\\%$)\",\n",
    "\t\t\"logiqa\": r\"LogiQA ($\\alpha = 40\\%$)\",\n",
    "\t\t\"piqa\": r\"PiQA ($\\alpha = 78\\%$)\",\n",
    "\t\t\"sciq\": r\"SciQ ($\\alpha = 96\\%$)\",\n",
    "\t\t\"social_iqa\": r\"SocialIQA ($\\alpha = 44\\%$)\",\n",
    "\t\t\"winogrande\": r\"Winogrande ($\\alpha = 70\\%$)\"\n",
    "\t}\n",
    "\n",
    "\tif type(idx_tuple) == tuple:\n",
    "\t\treturn (benchmark_name_mapping[idx_tuple[0]], idx_tuple[1])\n",
    "\telse:\n",
    "\t\treturn benchmark_name_mapping[idx_tuple]\n",
    "\n",
    "# Apply the replacement function\n",
    "new_index = selected_pivot.index.map(replace_level_0)\n",
    "selected_pivot.index = pd.MultiIndex.from_tuples(new_index, names=selected_pivot.index.names)\n",
    "# display(selected_pivot.columns.get_level_values(2))\n",
    "selected_pivot = selected_pivot.droplevel('alpha', axis=0)\n",
    "selected_pivot = selected_pivot.loc[:, (selected_pivot.columns.get_level_values(2) == 0.001) | (selected_pivot.columns.get_level_values(2) == 100) | (selected_pivot.columns.get_level_values(2) == 1000) | (selected_pivot.columns.get_level_values(2) == 10000)]\n",
    "\n",
    "# Merge pivot from single model experiments\n",
    "pvt_base_model.index = pvt_base_model.index.map(replace_level_0)\n",
    "\n",
    "display(pvt_base_model.columns.names)\n",
    "\n",
    "# pvt_base_model = pvt_base_model.rename_axis(columns=['first_level', 'second_level'])\n",
    "\n",
    "selected_pivot = pd.concat([selected_pivot, pvt_base_model], axis=1)\n",
    "benchmark_names = selected_pivot.index\n",
    "\n",
    "# Step 2: Create a new MultiIndex from the stacked data\n",
    "stacked_data = []\n",
    "for v_value in selected_pivot.columns.get_level_values('V').unique():\n",
    "\tfor b_name in benchmark_names:\n",
    "\t\tfor metric in selected_pivot.columns.get_level_values(0).unique():\n",
    "\t\t\tfor agg_type in selected_pivot.columns.get_level_values(1).unique():\n",
    "\t\t\t\tif (metric, agg_type, v_value) in selected_pivot.columns:\n",
    "\t\t\t\t\tvalue = selected_pivot.loc[b_name, (metric, agg_type, v_value)]\n",
    "\t\t\t\t\tstacked_data.append((v_value, b_name, metric, agg_type, value))\n",
    "\n",
    "\n",
    "# Step 3: Create a new DataFrame from the stacked data\n",
    "selected_pivot = pd.DataFrame(stacked_data, columns=['V', 'benchmark_name', 'metric', 'aggregation_type', 'value'])\n",
    "\n",
    "# Step 4: Pivot to get the desired format\n",
    "selected_pivot = selected_pivot.pivot(\n",
    "\tindex='V',\n",
    "\tcolumns=['benchmark_name', 'metric', 'aggregation_type'],\n",
    "\tvalues='value'\n",
    ")\n",
    "\n",
    "# Re-order the rows\n",
    "idx_new = []\n",
    "for row in selected_pivot.index:\n",
    "\tif row < 1:\n",
    "\t\tidx_new.append(100000)\n",
    "\telse:\n",
    "\t\tidx_new.append(row)\n",
    "\n",
    "selected_pivot.index = idx_new\n",
    "selected_pivot.sort_index(inplace=True)\n",
    "\n",
    "selected_pivot.index = [\"Llama 1B\", \"Llama 8B\", \"Llama 70B\", \"Random w. Constraint\", \"RouteLLM\", \"RouterDC\", r\"\\textbf{MESS+ (ours)}\"]\n",
    "selected_pivot = selected_pivot.fillna(0)\n",
    "\n",
    "def multiply_if_small(x):\n",
    "\tif isinstance(x, (int, float)) and x <= 1 and x != 0:  # Exclude zero if needed\n",
    "\t\treturn x * 100\n",
    "\treturn x\n",
    "\n",
    "# Apply the function to all elements in the DataFrame\n",
    "selected_pivot = selected_pivot.applymap(multiply_if_small)\n",
    "\n",
    "selected_pivot.loc[:, (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"sum\")] /= 1000000\n",
    "\n",
    "selected_pivot.loc[:, (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"std\")] /= 10000\n",
    "\n",
    "selected_pivot.loc[selected_pivot.index == r\"\\textbf{MESS+ (ours)}\", (selected_pivot.columns.get_level_values(1) == \"mess_plus/energy\") & (selected_pivot.columns.get_level_values(2) == \"sum\")] /= 10\n",
    "\n",
    "# We add a new \"AVERAGE L1 ITEM\n",
    "level2_level3_means = selected_pivot.groupby(level=[1, 2], axis=1).mean()\n",
    "new_column_tuples = [('Mean',) + col for col in level2_level3_means.columns]\n",
    "new_columns = pd.MultiIndex.from_tuples(new_column_tuples)\n",
    "\n",
    "# Create a DataFrame with the new columns\n",
    "means_df = pd.DataFrame(level2_level3_means.values, index=selected_pivot.index, columns=new_columns)\n",
    "\n",
    "# Concatenate the original DataFrame with the means DataFrame\n",
    "selected_pivot = pd.concat([selected_pivot, means_df], axis=1)\n",
    "\n",
    "selected_pivot.loc[(selected_pivot.index == \"Llama 1B\") | (selected_pivot.index == \"Llama 8B\") | (selected_pivot.index == \"Llama 70B\"), (selected_pivot.columns.get_level_values(1) == \"avg_accuracy\") & (selected_pivot.columns.get_level_values(2) == \"std\")] /= 10\n",
    "\n",
    "display(selected_pivot)"
   ],
   "id": "e9db668c366e50e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "# CREATE LATEX TABLE\n",
    "def multiindex_df_to_latex_chunked(\n",
    "\t\tdf,\n",
    "\t\tchunk_size=3,\n",
    "\t\tlevel2_order=None,\n",
    "\t\tcaption_template=\"Results Table Part {}\",\n",
    "\t\tlabel_template=\"tab:results_part{}\", include_index=True\n",
    "):\n",
    "\t# Get unique level 1 items\n",
    "\tlevel1_items = df.columns.get_level_values(0).unique()\n",
    "\n",
    "\t# Get level 2 items (either in specified order or existing order)\n",
    "\tif level2_order is None:\n",
    "\t\tlevel2_items = df.columns.get_level_values(1).unique()\n",
    "\telse:\n",
    "\t\t# Verify all specified level2 items exist in the DataFrame\n",
    "\t\texisting_level2 = df.columns.get_level_values(1).unique()\n",
    "\t\tfor item in level2_order:\n",
    "\t\t\tif item not in existing_level2:\n",
    "\t\t\t\traise ValueError(f\"Level 2 item '{item}' not found in DataFrame\")\n",
    "\t\tlevel2_items = level2_order\n",
    "\n",
    "\t# Calculate number of chunks\n",
    "\tnum_chunks = math.ceil(len(level1_items) / chunk_size)\n",
    "\n",
    "\tlatex_tables = []\n",
    "\n",
    "\t# Process each chunk\n",
    "\tfor chunk_idx in range(num_chunks):\n",
    "\t\tstart_idx = chunk_idx * chunk_size\n",
    "\t\tend_idx = min((chunk_idx + 1) * chunk_size, len(level1_items))\n",
    "\n",
    "\t\t# Get level 1 items for this chunk\n",
    "\t\tchunk_level1_items = level1_items[start_idx:end_idx]\n",
    "\n",
    "\t\t# Filter DataFrame to only include these level 1 items\n",
    "\t\tchunk_columns = [col for col in df.columns if col[0] in chunk_level1_items]\n",
    "\t\tchunk_df = df[chunk_columns]\n",
    "\n",
    "\t\t# Create a new DataFrame for the LaTeX output with the same higher-level structure\n",
    "\t\t# Get unique combinations of first two levels in this chunk\n",
    "\t\thigher_levels = chunk_df.columns.droplevel(2).unique()\n",
    "\n",
    "\t\t# Create new DataFrame with appropriate multi-index\n",
    "\t\tresult_df = pd.DataFrame(index=df.index)\n",
    "\t\tresult_cols = []\n",
    "\n",
    "\t\t# For each combination of higher levels, combine mean and std\n",
    "\t\tall_alph = []\n",
    "\t\tfor level1 in chunk_level1_items:\n",
    "\t\t\tpattern = r\"\\\\alpha\\s*=\\s*(\\d+)\\\\%\"\n",
    "\t\t\tmatch = re.search(pattern, level1)\n",
    "\t\t\tif match:\n",
    "\t\t\t    number = match.group(1)  # This will be \"50\" as a string\n",
    "\t\t\t    number_int = int(number)  # Convert to integer if needed\n",
    "\t\t\telse:\n",
    "\t\t\t    print(\"No number found\")\n",
    "\t\t\t    number_int = 0\n",
    "\n",
    "\t\t\talph = number_int\n",
    "\t\t\tall_alph.append(alph)\n",
    "\n",
    "\t\t\tfor level2 in level2_items:\n",
    "\t\t\t\tif \"final_models/large\" in level2:\n",
    "\t\t\t\t\tname = r\"\\thead{Model Call Ratio \\\\ (L70B/L8B/L1B)}\"\n",
    "\t\t\t\t\tresult_df[(level1, name)] = [f\"{x:.0f}\\\\% / {y:.0f}\\\\% / {z:.0f}\\\\%\" for x, y, z in zip(df[(level1, \"final_models/large_chosen\", \"first\")], df[(level1, \"final_models/medium_chosen\", \"first\")], df[(level1, \"final_models/small_chosen\", \"first\")])]\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif level2 == \"avg_accuracy\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Request. \\\\ Satisfaction}\"\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tfor m, s in zip(mean_val, std_val):\n",
    "\t\t\t\t\t\t\tif m >= alph:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{darkgreen}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{red}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\n",
    "\t\t\t\t\telif level2 == \"mess_plus/energy\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Operating \\\\ Cost}\"\n",
    "\t\t\t\t\t\tlevel3 = \"sum\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tmin_mean_val = mean_val[3:].min()\n",
    "\n",
    "\t\t\t\t\t\tmean_acc = df[level1, \"avg_accuracy\", \"mean\"]\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tacc_match = mean_acc[:3] >= alph\n",
    "\t\t\t\t\t\tis_min_single_satisfying = np.where(acc_match == True)[0]\n",
    "\n",
    "\t\t\t\t\t\tfor idx, (m, s) in enumerate(zip(mean_val, std_val)):\n",
    "\t\t\t\t\t\t\tif m == mean_val[3:].min():\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textbf{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telif idx == is_min_single_satisfying[0]:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\underline{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\")\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tname = level2\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tvals = [f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\" for m, s in zip(mean_val, std_val)]\n",
    "\n",
    "\t\t\t\t\tresult_df[(level1, name)] = vals\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t# Skip if mean or std not available\n",
    "\t\t\t\t\tprint(f\"Warning: Missing mean or std for {level1}, {level2}\")\n",
    "\n",
    "\t\t# Set the columns with multi-index (preserving top 2 levels)\n",
    "\t\tresult_df.columns = pd.MultiIndex.from_tuples(result_cols, names=['Category', 'Subcategory'])\n",
    "\n",
    "\t\t# Convert to LaTeX with multi-index\n",
    "\t\tcaption = caption_template.format(chunk_idx + 1)\n",
    "\t\tlabel = label_template.format(chunk_idx + 1)\n",
    "\n",
    "\t\tlatex_str = result_df.to_latex(escape=False, multicolumn=True, multicolumn_format='c', index=include_index)\n",
    "\n",
    "\t\t# Add caption and label\n",
    "\t\tlatex_str = latex_str.replace('\\\\begin{tabular}',\n",
    "\t\t                              f'\\\\begin{{table}}\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{{tabular}}')\n",
    "\t\tlatex_str = latex_str + '\\\\end{table}'\n",
    "\n",
    "\t\tlatex_tables.append(latex_str)\n",
    "\n",
    "\treturn latex_tables\n",
    "\n",
    "\n",
    "latex_tables = multiindex_df_to_latex_chunked(\n",
    "\tselected_pivot,\n",
    "\tchunk_size=3,\n",
    "\tcaption_template=\"Results Table Part {}: Categories\",\n",
    "\tlabel_template=\"tab:results_part{}\",\n",
    "\tlevel2_order=[\"mess_plus/energy\", \"avg_accuracy\", \"final_models/large_chosen\"]\n",
    ")\n",
    "\n",
    "for t in latex_tables:\n",
    "\tprint(t)"
   ],
   "id": "19a5c28b0517cd23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Overview table\n",
    "baseline_df[\"V\"] = 1000\n",
    "pvt_baseline = baseline_df.pivot_table(index=[\"benchmark_name\", \"alpha\"], columns=[\"V\"], values=[\"avg_accuracy\", \"mess_plus/energy\"], aggfunc={\"avg_accuracy\": [\"mean\"], \"mess_plus/energy\": [\"sum\"]})\n",
    "\n",
    "pvt_base_model = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"]].pivot_table(index=[\"benchmark_name\"], values=[\"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"], aggfunc={\"energy_consumption_large\": [\"sum\"], \"energy_consumption_medium\": [\"sum\"], \"energy_consumption_small\": [\"sum\"], \"label_small\": [\"mean\"], \"label_medium\": [\"mean\"], \"label_large\": [\"mean\"]})\n",
    "\n",
    "display(pvt_base_model)\n",
    "\n",
    "pvt_mess_plus_pre = run_df.groupby(['benchmark_name', 'alpha', 'V', \"_step\"]).agg({\n",
    "    'avg_accuracy': 'std',\n",
    "    'mess_plus/energy': 'std'  # Average across seeds first\n",
    "}).reset_index()\n",
    "\n",
    "pvt_mess_plus = pvt_mess_plus_pre.loc[pvt_mess_plus_pre[\"V\"].isin([0.00001, 0.0001, 0.001]), [\"benchmark_name\", \"alpha\", \"V\", \"avg_accuracy\", \"mess_plus/energy\"]].pivot_table(index=[\"benchmark_name\", \"alpha\"], columns=[\"V\"], values=[\"avg_accuracy\", \"mess_plus/energy\"], aggfunc={\"avg_accuracy\": [\"mean\"], \"mess_plus/energy\": [\"mean\"]})\n",
    "\n",
    "target_index = pvt_mess_plus.index.to_frame(index=False).drop_duplicates()\n",
    "\n",
    "# Create an empty DataFrame with the target MultiIndex\n",
    "pvt_base_model_expanded = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_frame(target_index),\n",
    "    columns=pvt_base_model.columns\n",
    ")\n",
    "\n",
    "# Fill in the data for each benchmark\n",
    "for benchmark in pvt_base_model.index:\n",
    "    mask = pvt_base_model_expanded.index.get_level_values('benchmark_name') == benchmark\n",
    "    pvt_base_model_expanded.loc[mask, :] = pvt_base_model.loc[benchmark, :].values\n",
    "\n",
    "pvt_base_model_expanded = pd.concat(\n",
    "    {0: pvt_base_model_expanded},\n",
    "    axis=1,\n",
    "    names=['V']\n",
    ")\n",
    "\n",
    "pvt_baseline_expanded = pd.concat(\n",
    "    {10: pvt_baseline},\n",
    "    axis=1,\n",
    "    names=['V']\n",
    ")\n",
    "\n",
    "if pvt_base_model_expanded.columns.nlevels == 3:\n",
    "    pvt_base_model_expanded = pvt_base_model_expanded.reorder_levels([1, 2, 0], axis=1)\n",
    "\n",
    "if pvt_baseline_expanded.columns.nlevels == 3:\n",
    "    pvt_baseline_expanded = pvt_baseline_expanded.reorder_levels([1, 2, 0], axis=1)\n",
    "\n",
    "# Now concatenate all tables\n",
    "combined_table = pd.concat([pvt_base_model_expanded, pvt_mess_plus, pvt_baseline_expanded], axis=1)\n",
    "combined_table = combined_table.sort_index(axis=1, level=2)\n",
    "\n",
    "benchmark_mask = combined_table.index.get_level_values('benchmark_name').isin([\"logiqa2\", \"lambada_standard\"])\n",
    "combined_table = combined_table.loc[~benchmark_mask]\n",
    "combined_table = combined_table.droplevel(1, axis=1)\n",
    "combined_table = combined_table.reorder_levels([1, 0], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "def categorize_metric(metric):\n",
    "    if 'accuracy' in metric.lower() or 'label' in metric.lower():\n",
    "        return 'accuracy'  # Treat both as accuracy metrics\n",
    "    elif 'energy' in metric.lower() or 'consumption' in metric.lower() or 'mess_plus' in metric.lower():\n",
    "        return 'energy'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Get all unique values for each level\n",
    "v_values = sorted(combined_table.columns.get_level_values(0).unique())\n",
    "funcs = combined_table.columns.get_level_values(1).unique()\n",
    "\n",
    "# Create ordered column list - accuracy/label first, then energy\n",
    "ordered_energy = []\n",
    "ordered_accuracy = []\n",
    "\n",
    "# For each V value, add accuracy/label columns first, then energy\n",
    "for v in v_values:\n",
    "    # Add accuracy/label columns\n",
    "    for col in combined_table.columns:\n",
    "        if col[0] == v and categorize_metric(col[1]) == 'accuracy':\n",
    "            ordered_accuracy.append(col)\n",
    "\n",
    "    # Add energy columns\n",
    "    for col in combined_table.columns:\n",
    "        if col[0] == v and categorize_metric(col[1]) == 'energy':\n",
    "            ordered_energy.append(col)\n",
    "\n",
    "# Reorder the DataFrame\n",
    "combined_table = combined_table[ordered_accuracy + ordered_energy]\n",
    "combined_table = combined_table.reorder_levels([1, 0], axis=1)\n",
    "\n",
    "new_level0 = [\"accuracy\"] * 7 + [\"energy\"] * 7\n",
    "combined_table.columns = pd.MultiIndex.from_arrays([\n",
    "\tnew_level0,\n",
    "\tcombined_table.columns.get_level_values(1).tolist()\n",
    "])\n",
    "\n",
    "# Conversion from Joule to Megajoule\n",
    "energy_mask = combined_table.columns.get_level_values(0) == \"energy\"\n",
    "energy_columns = combined_table.columns[energy_mask]\n",
    "combined_table.iloc[:, energy_mask] = combined_table.iloc[:, energy_mask] / 1000000 # Joule to Megajoule\n",
    "\n",
    "display(combined_table.head())\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your combined_table DataFrame from the code above\n",
    "# Make sure alpha is in the index or as a column\n",
    "latex_output = dataframe_to_latex_with_color(combined_table)\n",
    "# print(latex_output)\n",
    "#\n",
    "# # You can also save it to a file\n",
    "with open('model_comparison_table.tex', 'w') as f:\n",
    "\tf.write(combined_table.iloc[1::3].to_latex(float_format=\"%.2f\"))\n",
    "\n",
    "grouped_energy_table = combined_table.iloc[:, energy_mask].groupby(level=0).agg(\"mean\")\n",
    "\n",
    "\n",
    "display(grouped_energy_table)"
   ],
   "id": "fc44701afc293ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7473856edc48c547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=8, figsize=(20, 6.75))\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    # \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    # \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "}\n",
    "\n",
    "iterator = 0\n",
    "for name, display_name in BENCHMARK_NAME_DICT.items():\n",
    "\n",
    "\tplt_data = run_df.loc[(run_df[\"benchmark_name\"] == name), [\"c\", \"mess_plus/energy\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"mess_plus/energy\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"mess_plus/exploration_step_ratio\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[0][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"classifier/train_loss\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=None, # (\"sd\", 1),\n",
    "\t\tax=axes[1][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\t# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "\tsns.barplot(\n",
    "\t    data=plt_data,\n",
    "\t    x=\"c\",\n",
    "\t    y=\"exploration_cost\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[2][iterator],\n",
    "\t\tlegend=False,\n",
    "\t\testimator=np.sum\n",
    "\t)\n",
    "\n",
    "\taxes[0][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\taxes[0][iterator].set_ylim([0, 1])\n",
    "\taxes[0][iterator].set_xlabel(\"Request\")\n",
    "\taxes[1][iterator].set_xlabel(\"Request\")\n",
    "\taxes[0][iterator].set_title(display_name, fontsize=14)\n",
    "\taxes[0][iterator].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "\taxes[1][iterator].set_ylim([0, 4])\n",
    "\taxes[1][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\n",
    "\taxes[2][iterator].set_ylim([0, 1.2 * plt_data.groupby(\"c\")[\"exploration_cost\"].sum().max()])\n",
    "\tadd_value_labels(axes[2][iterator], convert_to_mj=False)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[0][iterator].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "\t\taxes[1][iterator].set_ylabel(\"Router Training Loss\")\n",
    "\t\taxes[2][iterator].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "\t\taxes[0][iterator].legend(title=\"c\")\n",
    "\telse:\n",
    "\t\taxes[0][iterator].set_ylabel(None)\n",
    "\t\taxes[1][iterator].set_ylabel(None)\n",
    "\t\taxes[2][iterator].set_ylabel(None)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=\"c_ablation_study\", chapter_name=\"evaluations\")"
   ],
   "id": "2d18bc79964bc8d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "C_BENCHMARK = \"winogrande\"\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.5, color_codes=True, rc=None)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 2.8))\n",
    "\n",
    "plt_data = run_df.loc[(run_df[\"benchmark_name\"] == C_BENCHMARK), [\"c\", \"mess_plus/energy\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"mess_plus/energy\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "    x=\"_step\",\n",
    "    y=\"mess_plus/exploration_step_ratio\",\n",
    "    hue=\"c\",\n",
    "\terrorbar=(\"sd\", 1),\n",
    "\tax=axes[0],\n",
    "\tlegend=True,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "plt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "plt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "    x=\"_step\",\n",
    "    y=\"classifier/train_loss\",\n",
    "    hue=\"c\",\n",
    "\terrorbar=None, # (\"sd\", 1),\n",
    "\tax=axes[1],\n",
    "\tlegend=False,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "sns.barplot(\n",
    "    data=plt_data,\n",
    "    x=\"c\",\n",
    "    y=\"exploration_cost\",\n",
    "\terrorbar=(\"sd\", 1),\n",
    "\tax=axes[2],\n",
    "\tlegend=False,\n",
    "\testimator=np.sum,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "axes[0].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_xlabel(\"Request\")\n",
    "axes[1].set_xlabel(\"Request\")\n",
    "# axes[0].set_title(\"ARC Challenge\", fontsize=14)\n",
    "axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "axes[1].set_ylim([0, 4])\n",
    "axes[1].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\n",
    "axes[2].set_ylim([0, 1.2 * plt_data.groupby(\"c\")[\"exploration_cost\"].sum().max()])\n",
    "add_value_labels(axes[2], convert_to_mj=False)\n",
    "\n",
    "axes[0].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "axes[1].set_ylabel(\"Router Training Loss\")\n",
    "axes[2].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "# axes[0].legend(title=\"c\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"c_ablation_study_{C_BENCHMARK}\", chapter_name=\"evaluations\")"
   ],
   "id": "ce31897558d27b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a54b8dad9eec877f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
