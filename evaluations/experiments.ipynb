{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "from plots.utils.plotting import write_figure_to_disk\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"experiments.ipynb\").absolute().parent\n",
    "\n",
    "DATA_DIR = f\"{NOTEBOOK_PATH}/data/online\"\n",
    "\n",
    "BENCHMARK_NAMES = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\"]\n",
    "# BENCHMARK_NAMES = [\"winogrande\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess-plus_3-models_online_vFINAL\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")"
   ],
   "id": "440a498da98688a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(run_summary_df)\n",
    "run_df = load_all_histories_to_dataframe(DATA_DIR)\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"benchmark_name\"] = name\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"] = run_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"].str.replace(f\"{name}_\", \"\")\n",
    "\n",
    "run_df[[\"V\", \"alpha\", \"c\", \"seed\"]] = run_df[\"run_name\"].str.split(\"_\", expand=True)\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].str.replace(\"a=\", \"\")\n",
    "run_df[\"V\"] = run_df[\"V\"].str.replace(\"V=\", \"\")\n",
    "run_df[\"c\"] = run_df[\"c\"].str.replace(\"c=\", \"\")\n",
    "run_df[\"seed\"] = run_df[\"seed\"].str.replace(\"seed=\", \"\")\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].astype(float)\n",
    "run_df[\"V\"] = run_df[\"V\"].astype(float)\n",
    "run_df[\"c\"] = run_df[\"c\"].astype(float)\n",
    "run_df[\"seed\"] = run_df[\"seed\"].astype(int)\n",
    "\n",
    "run_df[\"models/small_chosen\"] = run_df[\"models/small_chosen\"].astype(float)\n",
    "run_df[\"models/medium_chosen\"] = run_df[\"models/medium_chosen\"].astype(float)\n",
    "run_df[\"models/large_chosen\"] = run_df[\"models/large_chosen\"].astype(float)\n",
    "\n",
    "display(run_df.head())"
   ],
   "id": "52264c7618a8265f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_df = run_df.loc[(run_df[\"c\"] == 1.0) & (run_df[\"benchmark_name\"] == \"winogrande\")].pivot_table(index=[\"benchmark_name\", \"alpha\", \"V\", \"c\"], values=[\"avg_accuracy\", \"mess_plus/energy\", \"mess_plus/q_length\", \"total_runtime\"], aggfunc={\"avg_accuracy\": \"mean\", \"mess_plus/energy\": \"sum\", \"mess_plus/q_length\": \"mean\", \"total_runtime\": \"max\"})",
   "id": "e20b51f6fc67780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_value_labels(axx, spacing=5, convert_to_mj: bool = True):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in axx.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        if convert_to_mj:\n",
    "            label = f'{y_value / 1_000_000:.1f}' # MJ conversion\n",
    "        else:\n",
    "            label = f'{y_value:.1f}'\n",
    "\n",
    "        # Create annotation\n",
    "        axx.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "def fmt_to_megajoules(x, pos):\n",
    "    return f'{(x / 1_000_000):.0f}'\n"
   ],
   "id": "a38483bf31450f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load raw inference data\n",
    "\n",
    "infer_df = pd.DataFrame()\n",
    "def get_inference_data(benchmark_name):\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{NOTEBOOK_PATH.parent}/data/inference_outputs/{benchmark_name}\")\n",
    "\t\tinput_df[\"idx_original\"] = input_df.index\n",
    "\t\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\treturn input_df\n",
    "\texcept ValueError:\n",
    "\t\treturn pd.DataFrame()\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\tinfer_df = pd.concat([infer_df, get_inference_data(name)], ignore_index=True)\n",
    "\n",
    "infer_df.reset_index(inplace=True)\n",
    "\n",
    "# Get baseline dataframe\n",
    "BASELINE_DATA_DIR = f\"{NOTEBOOK_PATH}/data/random_baseline_v02\"\n",
    "baseline_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess_plus_random_baseline_with_constraint_v02\",\n",
    "    save_dir=BASELINE_DATA_DIR,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "baseline_df = load_all_histories_to_dataframe(BASELINE_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"] = baseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_alpha=\", \"\")\n",
    "\tbaseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"alpha\"] = baseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"run_name\"]\n",
    "\n",
    "\tbaseline_df[\"alpha\"] = baseline_df[\"alpha\"].astype(float)\n",
    "\n",
    "\n",
    "print(baseline_df.head())\n"
   ],
   "id": "ec68ca6f1bfad7d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "v_values_per_benchmark = {\n",
    "    \"arc_challenge\": [0.001, 0.0001, 0.00001],\n",
    "    \"arc_easy\": [0.01, 0.001, 0.0001],\n",
    "    \"boolq\": [0.01, 0.001, 0.0001],\n",
    "    # \"lambada_standard\": [0.01, 0.001, 0.0001],\n",
    "    \"logiqa\": [0.001, 0.0001, 0.00001],\n",
    "    # \"logiqa2\": [0.01, 0.001, 0.0001],\n",
    "    \"piqa\": [0.01, 0.001, 0.0001],\n",
    "    \"sciq\": [0.0001, 0.00001, 0.000001],\n",
    "    \"social_iqa\": [0.001, 0.0001, 0.00001],\n",
    "    \"winogrande\": [0.01, 0.001, 0.0001],\n",
    "}"
   ],
   "id": "57657572dfc3321f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(24, 8.5), gridspec_kw={'width_ratios': [2.3, 2.3, 1, 1, 1, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"arc_challenge\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# alpha = target_alpha_per_benchmark[name]\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\taxes[iterator][0].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.01, color='gray', fontsize=12, ha=\"left\")\n",
    "\taxes[iterator][0].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.01, color='gray', fontsize=12, ha=\"center\")\n",
    "\taxes[iterator][0].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.01, color='gray', fontsize=12, ha=\"right\")\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][0],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\n",
    "\t)\n",
    "\n",
    "\taxes[iterator][0].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][0].legend(ncols=2, title=\"V\")\n",
    "\n",
    "\taxes[iterator][0].axhline(y=alpha, color='red', linestyle='-', label=\"alpha\")\n",
    "\taxes[iterator][0].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\taxes[iterator][0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator][0].text(s=r\"This row: $ \\alpha = {alpha_val} $ (red line in this plot)\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=1.15 * raw_inference_accuracies_per_model[\"label_large\"] - 0.04, color='red', fontsize=14, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\t# Q Plot for SLA violations\n",
    "\tsubset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\tbaseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"sla_violations\"] = baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"mess_plus/q_length\"] / baseline_df.loc[(baseline_df[\"alpha\"] == alpha), \"_step\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"sla_violations\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][1],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\n",
    "\t)\n",
    "\n",
    "\taxes[iterator][1].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"sla_violations\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\taxes[iterator][1].set_ylim([0, 0.2])\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][1].legend(ncols=2, title=\"V\")\n",
    "\n",
    "\t# if iterator == 0:\n",
    "\t# \taxes[iterator][1].legend(ncols=3, loc='upper center', bbox_to_anchor=(0.5, 1.55), fontsize=12, title_fontsize=12, title=\"V\", labelspacing =0.1)\n",
    "\n",
    "\t# Energy consumption plot\n",
    "\t# random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "\t# random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "\t# random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "\t# random_baseline_energy.reset_index(inplace=True)\n",
    "\t#\n",
    "\t# raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "\t# raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "\t# raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "\t# raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "\t# raw_inference_energy_data.reset_index(inplace=True)\n",
    "\t#\n",
    "\t# raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"Llama 70B\", \"energy_consumption_medium\": \"Llama 8B\", \"energy_consumption_small\": \"Llama 1B\"}, inplace=False)\n",
    "\t#\n",
    "\t# raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "\t#\n",
    "\t# energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "\t#\n",
    "\t# energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "\t#\n",
    "\t# energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "\t# energy_data.reset_index(inplace=True)\n",
    "\t# energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "\t#\n",
    "\t# sns.barplot(\n",
    "\t#     data=energy_data,\n",
    "\t#     x=\"V\",\n",
    "\t#     y=\"mess_plus/energy\",\n",
    "\t# \tax=axes[iterator][2],\n",
    "\t# \terrorbar=(\"ci\", 0.95)\n",
    "\t# )\n",
    "\t#\n",
    "\t# add_value_labels(axes[iterator][2])\n",
    "\t# axes[iterator][2].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "\t# axes[iterator][2].set(ylim=[0, 1.4 * energy_data[\"mess_plus/energy\"].max()])\n",
    "\t# axes[iterator][2].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "\t# Classifier training loss plot\n",
    "\t# sns.lineplot(\n",
    "\t#     data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t#     x=\"_step\",\n",
    "\t#     y=\"classifier/train_loss\",\n",
    "\t#     hue=\"V\",\n",
    "\t# \terrorbar=None,\n",
    "\t# \tax=axes[3][iterator],\n",
    "\t# \tlegend=False,\n",
    "\t# )\n",
    "\n",
    "\t# Stackplot for Model Call Ratio\n",
    "\tv_values_per_benchmark[name] = sorted(v_values_per_benchmark[name], reverse=False)\n",
    "\t# v_values_per_benchmark[name].reverse()\n",
    "\tfor jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "\t\tstack_df = subset.loc[\n",
    "\t\t\t(run_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(run_df[\"V\"] == V) &\n",
    "\t\t\t(subset[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"V\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t\tx = stack_df[\"_step\"]\n",
    "\t\ty = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\t\ty_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "\t\taxes[iterator][2 + jdx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=1.0)\n",
    "\t\taxes[iterator][2 + jdx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=1.0)\n",
    "\t\taxes[iterator][2 + jdx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=1.0)\n",
    "\t\taxes[iterator][2 + jdx].set(xlabel=f\"Requests @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "\t\taxes[iterator][2 + jdx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t\taxes[iterator][2 + jdx].set(xlim=[0, stack_df[\"_step\"].max()])\n",
    "\n",
    "\t\t# if jdx > 0:\n",
    "\t\t# \taxes[iterator][3 + jdx].get_yaxis().set_visible(False)\n",
    "\n",
    "\t# Add area plot for random baseline with constraint.\n",
    "\tbaseline_stack_df = baseline_df.loc[\n",
    "\t\t\t(baseline_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(baseline_df[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\tx_base = baseline_stack_df[\"_step\"]\n",
    "\ty_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\ty_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "\taxes[iterator][5].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\taxes[iterator][5].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "\taxes[iterator][5].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\taxes[iterator][5].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\taxes[iterator][5].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\taxes[iterator][5].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\t# axes[iterator][6].get_yaxis().set_visible(False)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][5].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"], ncols=1, loc='upper center', title=\"Model\", fontsize=14)\n",
    "\n",
    "\n",
    "\taxes[iterator][0].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][1].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\t# axes[iterator][2].set(xlabel=\"\")\n",
    "\t# axes[3][iterator].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\n",
    "\taxes[iterator][0].set(ylabel=None)\n",
    "\taxes[iterator][1].set(ylabel=None)\n",
    "\t# axes[iterator][2].set(ylabel=None)\n",
    "\t\t# axes[3][iterator].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes[iterator], [r\"Avg. User Satisfaction Rate Over Time (varying $\\alpha$)\".format(alpha_val=alpha), \"Avg. SLA Violations Over Time\", \"Total Cost (in MJ)\", \"Model Call Ratio (MCR)\", \"\", \"\"]):\n",
    "\n",
    "\t\tif iterator == 1:\n",
    "\t\t\tax.set_ylabel(col, rotation=90, size=18)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_all_alpha\", chapter_name=\"evaluations\")\n"
   ],
   "id": "7045943bad636e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(infer_df.columns)\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_large\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_medium\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_small\"].mean())"
   ],
   "id": "3ef80eb1dca358dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot generator\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    # \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    # \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "}\n",
    "\n",
    "# Create a list of all benchmark-alpha combinations\n",
    "benchmark_alpha_combinations = []\n",
    "for name in v_values_per_benchmark.keys():\n",
    "    config_path = Path(f\"{NOTEBOOK_PATH.parent}/config/online/{name}.yaml\")\n",
    "    with config_path.open(\"r\") as f:\n",
    "        import yaml\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    algorithm_config = CONFIG[\"algorithm\"]\n",
    "    for alpha in algorithm_config[\"alpha_values\"]:\n",
    "        benchmark_alpha_combinations.append((name, alpha))\n",
    "\n",
    "# Initialize plotting variables\n",
    "plot_num = 0\n",
    "col_count = 0\n",
    "\n",
    "# Iterate through all benchmark-alpha combinations\n",
    "for combo_idx, (name, alpha) in enumerate(benchmark_alpha_combinations):\n",
    "\n",
    "    # Create new figure every 6 columns\n",
    "    if col_count == 0:\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        fig, axes = plt.subplots(nrows=7, ncols=6, figsize=(20, 12))\n",
    "        plot_num += 1\n",
    "\n",
    "    # Get current column index\n",
    "    col_idx = col_count\n",
    "\n",
    "    # Skip if this benchmark doesn't have V values configured\n",
    "    if name not in v_values_per_benchmark.keys():\n",
    "        continue\n",
    "\n",
    "    # Filter data for current benchmark and alpha\n",
    "    subset = run_df.loc[(run_df[\"benchmark_name\"] == name) &\n",
    "                       (run_df[\"c\"] == 0.1) &\n",
    "                       (run_df[\"V\"].isin(v_values_per_benchmark[name])) &\n",
    "                       (run_df[\"_step\"] > 10) &\n",
    "                       (run_df[\"alpha\"] == alpha)]\n",
    "\n",
    "    v_values = subset[\"V\"].unique().tolist()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    raw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.025, color='gray', fontsize=8, ha=\"left\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.025, color='gray', fontsize=8, ha=\"center\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.025, color='gray', fontsize=8, ha=\"right\")\n",
    "    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"avg_accuracy\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[0][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "    )\n",
    "\n",
    "    axes[0][col_idx].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "    axes[0][col_idx].axhline(y=alpha, color='red', linestyle='-')\n",
    "    axes[0][col_idx].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=8, ha=\"right\")\n",
    "\n",
    "    axes[0][col_idx].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[0][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Q Plot for SLA violations\n",
    "    subset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"sla_violations\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[1][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "    )\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[1][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Energy consumption plot\n",
    "    random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "    random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "    random_baseline_energy.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "    raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "    raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "    raw_inference_energy_data.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"70B\", \"energy_consumption_medium\": \"8B\", \"energy_consumption_small\": \"1B\"}, inplace=False)\n",
    "\n",
    "    raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "    energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "\n",
    "    energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "\n",
    "    energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "    energy_data.reset_index(inplace=True)\n",
    "    energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "\n",
    "    sns.barplot(\n",
    "        data=energy_data,\n",
    "        x=\"V\",\n",
    "        y=\"mess_plus/energy\",\n",
    "        ax=axes[2][col_idx],\n",
    "        errorbar=(\"ci\", 0.95),\n",
    "    )\n",
    "\n",
    "    add_value_labels(axes[2][col_idx])\n",
    "    axes[2][col_idx].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "    axes[2][col_idx].set(ylim=[0, 2 * energy_data[\"mess_plus/energy\"].max()])\n",
    "    axes[2][col_idx].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "    # Stackplot for Model Call Ratio\n",
    "    for jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "        stack_df = subset.loc[\n",
    "            (run_df[\"benchmark_name\"] == name) &\n",
    "            (run_df[\"V\"] == V) &\n",
    "            (subset[\"alpha\"] == alpha),\n",
    "            [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "        x = stack_df[\"_step\"]\n",
    "        y = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "        y_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "        axes[3 + jdx][col_idx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].set(xlabel=f\"Request @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "        axes[3 + jdx][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "        if jdx == 0 and col_idx == 0:\n",
    "            axes[3 + jdx][col_idx].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"])\n",
    "\n",
    "    # Add area plot for random baseline with constraint.\n",
    "    baseline_stack_df = baseline_df.loc[\n",
    "            (baseline_df[\"benchmark_name\"] == name) &\n",
    "            (baseline_df[\"alpha\"] == alpha),\n",
    "            [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "    x_base = baseline_stack_df[\"_step\"]\n",
    "    y_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "    y_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "    axes[6][col_idx].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "    axes[6][col_idx].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "    axes[6][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axes[6][col_idx].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\n",
    "    # Set axis properties\n",
    "    axes[0][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[1][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[2][col_idx].set(xlabel=\"\")\n",
    "\n",
    "    # Remove y-labels for columns after the first\n",
    "    if col_idx > 0:\n",
    "        axes[0][col_idx].set(ylabel=None)\n",
    "        axes[1][col_idx].set(ylabel=None)\n",
    "        axes[2][col_idx].set(ylabel=None)\n",
    "\n",
    "    # Set title for each column\n",
    "    axes[0][col_idx].set_title(r\"{bm_name} ($\\alpha = {alpha_val} $)\".format(bm_name=BENCHMARK_NAME_DICT[name], alpha_val=alpha))\n",
    "\n",
    "    # Increment column counter\n",
    "    col_count += 1\n",
    "\n",
    "    # Check if we need to save the current figure and start a new one\n",
    "    if col_count == 6 or combo_idx == len(benchmark_alpha_combinations) - 1:\n",
    "        # Add row labels\n",
    "        for idx, (ax, row) in enumerate(zip(axes[:,0], [\"User Satisfaction\", \"SLA Violations\", \"Cost (in MJ energy)\", \"\", \"\", \"\", \"\"])):\n",
    "            if idx == 5:\n",
    "                fig.text(0.003, 0.225, \"Model Call Ratio (MCR)\", ha=\"center\", rotation='vertical', fontsize=plt.rcParams['axes.labelsize'])\n",
    "            else:\n",
    "                ax.set_ylabel(row, rotation=90, size='large')\n",
    "\n",
    "        # Save the figure\n",
    "        fig.tight_layout()\n",
    "        write_figure_to_disk(plt, file_name=f\"benchmark_performance_plot_{plot_num}\", chapter_name=\"evaluations\")\n",
    "\n",
    "        # Reset column counter for next figure\n",
    "        col_count = 0"
   ],
   "id": "989db16e594ce6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def dataframe_to_latex_with_color(df, high_color='green!60', low_color='red!60'):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a LaTeX table with color coding for accuracy columns.\n",
    "\n",
    "    Parameters:\n",
    "    df: pandas DataFrame (must have 'alpha' in the index or as a column)\n",
    "    high_color: LaTeX color for values >= threshold\n",
    "    low_color: LaTeX color for values < threshold\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Check if we have a MultiIndex with alpha\n",
    "    alpha_from_index = False\n",
    "    if isinstance(df_copy.index, pd.MultiIndex) and 'alpha' in df_copy.index.names:\n",
    "        alpha_from_index = True\n",
    "        # Reset index to get alpha as a regular column\n",
    "        df_copy = df_copy.reset_index()\n",
    "\n",
    "    # Round all numeric values to 2 decimal places\n",
    "    df_rounded = df_copy.round(2)\n",
    "\n",
    "    # Function to determine if a value should be colored\n",
    "    def should_color(col_name):\n",
    "        # Color if 'accuracy' is in the column name (case insensitive)\n",
    "        return 'accuracy' in col_name.lower() or 'label' in col_name.lower()\n",
    "\n",
    "    # Function to format a cell with color coding\n",
    "    def format_cell(value, col_name, alpha):\n",
    "        # Convert value to scalar if it's a Series\n",
    "        if hasattr(value, 'item'):\n",
    "            try:\n",
    "                value = value.item()\n",
    "            except ValueError:\n",
    "                # If multiple values, take the first one\n",
    "                value = value.iloc[0] if hasattr(value, 'iloc') else value\n",
    "\n",
    "        # Ensure alpha is also a scalar\n",
    "        if hasattr(alpha, 'item'):\n",
    "            try:\n",
    "                alpha = alpha.item()\n",
    "            except ValueError:\n",
    "                alpha = alpha.iloc[0] if hasattr(alpha, 'iloc') else alpha\n",
    "\n",
    "        if should_color(col_name) and pd.notna(value):\n",
    "            try:\n",
    "                if float(value) >= float(alpha):\n",
    "                    return f\"\\\\cellcolor{{{high_color}}}{float(value):.2f}\"\n",
    "                else:\n",
    "                    return f\"\\\\cellcolor{{{low_color}}}{float(value):.2f}\"\n",
    "            except (ValueError, TypeError):\n",
    "                # If value cannot be converted to float, return as is\n",
    "                return str(value)\n",
    "        else:\n",
    "            if pd.notna(value):\n",
    "                if isinstance(value, (int, float)):\n",
    "                    return f\"{float(value):.2f}\"\n",
    "                else:\n",
    "                    return str(value)\n",
    "            else:\n",
    "                return \"-\"\n",
    "\n",
    "    # Start building the LaTeX table\n",
    "    latex_lines = []\n",
    "    latex_lines.append(\"\\\\begin{table}[h!]\")\n",
    "    latex_lines.append(\"\\\\centering\")\n",
    "\n",
    "    # Calculate total columns (alpha + benchmark + data columns)\n",
    "    # No index column, just alpha, benchmark, and data columns\n",
    "    data_cols = len(df_copy.columns)\n",
    "    if alpha_from_index:\n",
    "        data_cols -= 2  # Remove benchmark and alpha from the count\n",
    "    elif 'alpha' in df_copy.columns:\n",
    "        data_cols -= 1  # Remove alpha from the count\n",
    "    else:\n",
    "        raise ValueError(\"Alpha must be either in the index or as a column for color coding\")\n",
    "\n",
    "    total_cols = data_cols + 2  # alpha + benchmark + data columns\n",
    "\n",
    "    latex_lines.append(\"\\\\begin{tabular}{\" + \"c\" * total_cols + \"}\")\n",
    "    latex_lines.append(\"\\\\toprule\")\n",
    "\n",
    "    # Handle multi-level columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Get the number of levels\n",
    "        n_levels = df.columns.nlevels\n",
    "\n",
    "        # Create header rows for each level\n",
    "        for level in range(n_levels):\n",
    "            header_row = []\n",
    "            if level == 0:\n",
    "                header_row.append(\"\\\\multirow{\" + str(n_levels) + \"}{*}{Alpha}\")\n",
    "                header_row.append(\"\\\\multirow{\" + str(n_levels) + \"}{*}{Benchmark}\")\n",
    "            else:\n",
    "                header_row.append(\"\")\n",
    "                header_row.append(\"\")\n",
    "\n",
    "            # Get labels for this level\n",
    "            labels = []\n",
    "            spans = []\n",
    "            current_label = None\n",
    "            current_span = 0\n",
    "\n",
    "            for col in df.columns:\n",
    "                # Skip index-related columns\n",
    "                if alpha_from_index and isinstance(col, str) and col == 'alpha':\n",
    "                    continue\n",
    "                if alpha_from_index and isinstance(col, str) and col == df_copy.columns[0]:  # Skip benchmark column\n",
    "                    continue\n",
    "\n",
    "                label = str(col[level]) if isinstance(col, tuple) else str(col)\n",
    "                if label != current_label:\n",
    "                    if current_label is not None:\n",
    "                        labels.append((current_label, current_span))\n",
    "                    current_label = label\n",
    "                    current_span = 1\n",
    "                else:\n",
    "                    current_span += 1\n",
    "\n",
    "            # Don't forget the last group\n",
    "            if current_label is not None:\n",
    "                labels.append((current_label, current_span))\n",
    "\n",
    "            # Add multicolumn headers\n",
    "            for label, span in labels:\n",
    "                # Add (MJ) to energy-related headers\n",
    "                if any(energy_term in str(label).lower() for energy_term in ['energy', 'consumption', 'mess_plus']):\n",
    "                    label = f\"{label} (MJ)\"\n",
    "\n",
    "                if span > 1:\n",
    "                    header_row.append(f\"\\\\multicolumn{{{span}}}{{c}}{{{label}}}\")\n",
    "                else:\n",
    "                    header_row.append(label)\n",
    "\n",
    "            latex_lines.append(\" & \".join(header_row) + \" \\\\\\\\\")\n",
    "\n",
    "        # Add horizontal line between header levels\n",
    "        latex_lines.append(\"\\\\cmidrule{3-\" + str(total_cols) + \"}\")\n",
    "    else:\n",
    "        # Simple single-level header\n",
    "        header_row = [\"Alpha\", \"Benchmark\"]\n",
    "\n",
    "        for col in df_copy.columns:\n",
    "            # Skip columns that are already handled\n",
    "            if alpha_from_index and (col == 'alpha' or col == df_copy.columns[0]):\n",
    "                continue\n",
    "            elif not alpha_from_index and col == 'alpha':\n",
    "                continue\n",
    "            else:\n",
    "                col_str = str(col)\n",
    "                # Add (MJ) to energy-related headers\n",
    "                if any(energy_term in col_str.lower() for energy_term in ['energy', 'consumption', 'mess_plus']):\n",
    "                    col_str = f\"{col_str} (MJ)\"\n",
    "                header_row.append(col_str)\n",
    "        latex_lines.append(\" & \".join(header_row) + \" \\\\\\\\\")\n",
    "\n",
    "    latex_lines.append(\"\\\\midrule\")\n",
    "\n",
    "    # Add data rows\n",
    "    for idx, row in df_rounded.iterrows():\n",
    "        # Handle MultiIndex row (if it still exists, though we reset_index above)\n",
    "        if isinstance(idx, tuple):\n",
    "            benchmark_name = idx[0]\n",
    "            alpha_value = idx[1] if len(idx) > 1 else None\n",
    "        else:\n",
    "            if alpha_from_index:\n",
    "                # Properly extract the benchmark name value\n",
    "                benchmark_value = row[df_copy.columns[0]]\n",
    "                if hasattr(benchmark_value, 'item'):\n",
    "                    try:\n",
    "                        benchmark_name = benchmark_value.item()\n",
    "                    except ValueError:\n",
    "                        benchmark_name = benchmark_value.iloc[0] if hasattr(benchmark_value, 'iloc') else benchmark_value\n",
    "                else:\n",
    "                    benchmark_name = benchmark_value\n",
    "\n",
    "                alpha_value = row['alpha']\n",
    "            else:\n",
    "                benchmark_name = str(idx)\n",
    "                alpha_value = None\n",
    "\n",
    "        # Get alpha value for display and color coding\n",
    "        if alpha_value is None and 'alpha' in row:\n",
    "            alpha_value = row['alpha']\n",
    "\n",
    "        # Convert alpha_value to scalar if necessary\n",
    "        if hasattr(alpha_value, 'item'):\n",
    "            try:\n",
    "                alpha_value = alpha_value.item()\n",
    "            except ValueError:\n",
    "                alpha_value = alpha_value.iloc[0] if hasattr(alpha_value, 'iloc') else alpha_value\n",
    "\n",
    "        # If we still don't have alpha, raise an error\n",
    "        if alpha_value is None:\n",
    "            raise ValueError(\"Alpha must be either in the index or as a column for color coding\")\n",
    "\n",
    "        # Format the row - Alpha first, then Benchmark\n",
    "        # Make sure benchmark_name is a string\n",
    "        benchmark_name_str = str(benchmark_name)\n",
    "        row_data = [f\"{float(alpha_value):.2f}\", benchmark_name_str]\n",
    "\n",
    "        for col_name, value in row.items():\n",
    "            # Skip columns we've already handled\n",
    "            if alpha_from_index and (col_name == 'alpha' or col_name == df_copy.columns[0]):\n",
    "                continue\n",
    "            elif not alpha_from_index and col_name == 'alpha':\n",
    "                continue\n",
    "\n",
    "            # Get the actual column name for color checking\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                actual_col_name = \" \".join(str(c) for c in col_name)\n",
    "            else:\n",
    "                actual_col_name = str(col_name)\n",
    "\n",
    "            formatted_value = format_cell(value, actual_col_name, alpha_value)\n",
    "            row_data.append(formatted_value)\n",
    "\n",
    "        latex_lines.append(\" & \".join(row_data) + \" \\\\\\\\\")\n",
    "\n",
    "    latex_lines.append(\"\\\\bottomrule\")\n",
    "    latex_lines.append(\"\\\\end{tabular}\")\n",
    "\n",
    "    # Add caption and label\n",
    "    latex_lines.append(\"\\\\caption{Comparison of Model Performance}\")\n",
    "    latex_lines.append(\"\\\\label{tab:model_comparison}\")\n",
    "    latex_lines.append(\"\\\\end{table}\")\n",
    "\n",
    "    # Add required packages in comments\n",
    "    packages = [\n",
    "        \"% Required LaTeX packages:\",\n",
    "        \"% \\\\usepackage{booktabs}\",\n",
    "        \"% \\\\usepackage{multirow}\",\n",
    "        \"% \\\\usepackage{xcolor}\",\n",
    "        \"% \\\\usepackage{colortbl}\",\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(packages + latex_lines)"
   ],
   "id": "b2cfd6bf9a6d55d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Overview table\n",
    "pvt_baseline = baseline_df.pivot_table(index=[\"benchmark_name\", \"alpha\"], values=[\"avg_accuracy\", \"mess_plus/energy\"], aggfunc={\"avg_accuracy\": [\"mean\"], \"mess_plus/energy\": [\"sum\"]})\n",
    "\n",
    "pvt_base_model = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"]].pivot_table(index=[\"benchmark_name\"], values=[\"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\", \"label_small\", \"label_medium\", \"label_large\"], aggfunc={\"energy_consumption_large\": [\"sum\"], \"energy_consumption_medium\": [\"sum\"], \"energy_consumption_small\": [\"sum\"], \"label_small\": [\"mean\"], \"label_medium\": [\"mean\"], \"label_large\": [\"mean\"]})\n",
    "\n",
    "pvt_mess_plus_pre = run_df.groupby(['benchmark_name', 'alpha', 'V', \"_step\"]).agg({\n",
    "    'avg_accuracy': 'std',\n",
    "    'mess_plus/energy': 'std'  # Average across seeds first\n",
    "}).reset_index()\n",
    "\n",
    "pvt_mess_plus = pvt_mess_plus_pre.loc[pvt_mess_plus_pre[\"V\"].isin([0.00001, 0.0001, 0.001]), [\"benchmark_name\", \"alpha\", \"V\", \"avg_accuracy\", \"mess_plus/energy\"]].pivot_table(index=[\"benchmark_name\", \"alpha\"], columns=[\"V\"], values=[\"avg_accuracy\", \"mess_plus/energy\"], aggfunc={\"avg_accuracy\": [\"mean\"], \"mess_plus/energy\": [\"mean\"]})\n",
    "\n",
    "target_index = pvt_mess_plus.index.to_frame(index=False).drop_duplicates()\n",
    "\n",
    "# Create an empty DataFrame with the target MultiIndex\n",
    "pvt_base_model_expanded = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_frame(target_index),\n",
    "    columns=pvt_base_model.columns\n",
    ")\n",
    "\n",
    "# Fill in the data for each benchmark\n",
    "for benchmark in pvt_base_model.index:\n",
    "    mask = pvt_base_model_expanded.index.get_level_values('benchmark_name') == benchmark\n",
    "    pvt_base_model_expanded.loc[mask, :] = pvt_base_model.loc[benchmark, :].values\n",
    "\n",
    "pvt_base_model_expanded = pd.concat(\n",
    "    {0: pvt_base_model_expanded},\n",
    "    axis=1,\n",
    "    names=['V']\n",
    ")\n",
    "\n",
    "pvt_baseline_expanded = pd.concat(\n",
    "    {10: pvt_baseline},\n",
    "    axis=1,\n",
    "    names=['V']\n",
    ")\n",
    "\n",
    "if pvt_base_model_expanded.columns.nlevels == 3:\n",
    "    pvt_base_model_expanded = pvt_base_model_expanded.reorder_levels([1, 2, 0], axis=1)\n",
    "\n",
    "if pvt_baseline_expanded.columns.nlevels == 3:\n",
    "    pvt_baseline_expanded = pvt_baseline_expanded.reorder_levels([1, 2, 0], axis=1)\n",
    "\n",
    "# Now concatenate all tables\n",
    "combined_table = pd.concat([pvt_base_model_expanded, pvt_mess_plus, pvt_baseline_expanded], axis=1)\n",
    "combined_table = combined_table.sort_index(axis=1, level=2)\n",
    "\n",
    "benchmark_mask = combined_table.index.get_level_values('benchmark_name').isin([\"logiqa2\", \"lambada_standard\"])\n",
    "combined_table = combined_table.loc[~benchmark_mask]\n",
    "combined_table = combined_table.droplevel(1, axis=1)\n",
    "combined_table = combined_table.reorder_levels([1, 0], axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "def categorize_metric(metric):\n",
    "    if 'accuracy' in metric.lower() or 'label' in metric.lower():\n",
    "        return 'accuracy'  # Treat both as accuracy metrics\n",
    "    elif 'energy' in metric.lower() or 'consumption' in metric.lower() or 'mess_plus' in metric.lower():\n",
    "        return 'energy'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "# Get all unique values for each level\n",
    "v_values = sorted(combined_table.columns.get_level_values(0).unique())\n",
    "funcs = combined_table.columns.get_level_values(1).unique()\n",
    "\n",
    "# Create ordered column list - accuracy/label first, then energy\n",
    "ordered_energy = []\n",
    "ordered_accuracy = []\n",
    "\n",
    "# For each V value, add accuracy/label columns first, then energy\n",
    "for v in v_values:\n",
    "    # Add accuracy/label columns\n",
    "    for col in combined_table.columns:\n",
    "        if col[0] == v and categorize_metric(col[1]) == 'accuracy':\n",
    "            ordered_accuracy.append(col)\n",
    "\n",
    "    # Add energy columns\n",
    "    for col in combined_table.columns:\n",
    "        if col[0] == v and categorize_metric(col[1]) == 'energy':\n",
    "            ordered_energy.append(col)\n",
    "\n",
    "# Reorder the DataFrame\n",
    "combined_table = combined_table[ordered_accuracy + ordered_energy]\n",
    "combined_table = combined_table.reorder_levels([1, 0], axis=1)\n",
    "\n",
    "new_level0 = [\"accuracy\"] * 7 + [\"energy\"] * 7\n",
    "combined_table.columns = pd.MultiIndex.from_arrays([\n",
    "\tnew_level0,\n",
    "\tcombined_table.columns.get_level_values(1).tolist()\n",
    "])\n",
    "\n",
    "# Conversion from Joule to Megajoule\n",
    "energy_mask = combined_table.columns.get_level_values(0) == \"energy\"\n",
    "energy_columns = combined_table.columns[energy_mask]\n",
    "combined_table.iloc[:, energy_mask] = combined_table.iloc[:, energy_mask] / 1000000 # Joule to Megajoule\n",
    "\n",
    "display(combined_table.head())\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your combined_table DataFrame from the code above\n",
    "# Make sure alpha is in the index or as a column\n",
    "latex_output = dataframe_to_latex_with_color(combined_table)\n",
    "# print(latex_output)\n",
    "#\n",
    "# # You can also save it to a file\n",
    "with open('model_comparison_table.tex', 'w') as f:\n",
    "\tf.write(combined_table.iloc[1::3].to_latex(float_format=\"%.2f\"))\n",
    "\n",
    "grouped_energy_table = combined_table.iloc[:, energy_mask].groupby(level=0).agg(\"mean\")\n",
    "\n",
    "\n",
    "display(grouped_energy_table)"
   ],
   "id": "fc44701afc293ba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7473856edc48c547",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=8, figsize=(20, 6.75))\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    # \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    # \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "}\n",
    "\n",
    "iterator = 0\n",
    "for name, display_name in BENCHMARK_NAME_DICT.items():\n",
    "\n",
    "\tplt_data = run_df.loc[(run_df[\"benchmark_name\"] == name), [\"c\", \"mess_plus/energy\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"mess_plus/energy\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"mess_plus/exploration_step_ratio\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[0][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"classifier/train_loss\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=None, # (\"sd\", 1),\n",
    "\t\tax=axes[1][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\t# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "\tsns.barplot(\n",
    "\t    data=plt_data,\n",
    "\t    x=\"c\",\n",
    "\t    y=\"exploration_cost\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[2][iterator],\n",
    "\t\tlegend=False,\n",
    "\t\testimator=np.sum\n",
    "\t)\n",
    "\n",
    "\taxes[0][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\taxes[0][iterator].set_ylim([0, 1])\n",
    "\taxes[0][iterator].set_xlabel(\"Request\")\n",
    "\taxes[1][iterator].set_xlabel(\"Request\")\n",
    "\taxes[0][iterator].set_title(display_name, fontsize=14)\n",
    "\taxes[0][iterator].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "\taxes[1][iterator].set_ylim([0, 4])\n",
    "\taxes[1][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\n",
    "\taxes[2][iterator].set_ylim([0, 1.2 * plt_data.groupby(\"c\")[\"exploration_cost\"].sum().max()])\n",
    "\tadd_value_labels(axes[2][iterator], convert_to_mj=False)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[0][iterator].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "\t\taxes[1][iterator].set_ylabel(\"Router Training Loss\")\n",
    "\t\taxes[2][iterator].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "\t\taxes[0][iterator].legend(title=\"c\")\n",
    "\telse:\n",
    "\t\taxes[0][iterator].set_ylabel(None)\n",
    "\t\taxes[1][iterator].set_ylabel(None)\n",
    "\t\taxes[2][iterator].set_ylabel(None)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=\"c_ablation_study\", chapter_name=\"evaluations\")"
   ],
   "id": "2d18bc79964bc8d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 3))\n",
    "\n",
    "plt_data = run_df.loc[(run_df[\"benchmark_name\"] == \"arc_challenge\"), [\"c\", \"mess_plus/energy\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"mess_plus/energy\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "    x=\"_step\",\n",
    "    y=\"mess_plus/exploration_step_ratio\",\n",
    "    hue=\"c\",\n",
    "\terrorbar=(\"sd\", 1),\n",
    "\tax=axes[0],\n",
    "\tlegend=True,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "plt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "plt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "    x=\"_step\",\n",
    "    y=\"classifier/train_loss\",\n",
    "    hue=\"c\",\n",
    "\terrorbar=None, # (\"sd\", 1),\n",
    "\tax=axes[1],\n",
    "\tlegend=False,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "sns.barplot(\n",
    "    data=plt_data,\n",
    "    x=\"c\",\n",
    "    y=\"exploration_cost\",\n",
    "\terrorbar=(\"sd\", 1),\n",
    "\tax=axes[2],\n",
    "\tlegend=False,\n",
    "\testimator=np.sum,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "axes[0].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_xlabel(\"Request\")\n",
    "axes[1].set_xlabel(\"Request\")\n",
    "# axes[0].set_title(\"ARC Challenge\", fontsize=14)\n",
    "axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "axes[1].set_ylim([0, 4])\n",
    "axes[1].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\n",
    "axes[2].set_ylim([0, 1.2 * plt_data.groupby(\"c\")[\"exploration_cost\"].sum().max()])\n",
    "add_value_labels(axes[2], convert_to_mj=False)\n",
    "\n",
    "axes[0].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "axes[1].set_ylabel(\"Router Training Loss\")\n",
    "axes[2].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "# axes[0].legend(title=\"c\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=\"c_ablation_study_arc_challenge\", chapter_name=\"evaluations\")"
   ],
   "id": "ce31897558d27b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a54b8dad9eec877f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
