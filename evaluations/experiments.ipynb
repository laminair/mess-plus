{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from rich.jupyter import display\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "from plots.utils.plotting import write_figure_to_disk\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"experiments.ipynb\").absolute().parent\n",
    "\n",
    "DATA_DIR = f\"{NOTEBOOK_PATH}/data/online\"\n",
    "\n",
    "BENCHMARK_NAMES = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\"]\n",
    "# BENCHMARK_NAMES = [\"winogrande\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess-plus_3-models_online_vFINAL\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")"
   ],
   "id": "440a498da98688a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "display(run_summary_df)\n",
    "run_df = load_all_histories_to_dataframe(DATA_DIR)\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"benchmark_name\"] = name\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"] = run_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"].str.replace(f\"{name}_\", \"\")\n",
    "\n",
    "run_df[[\"V\", \"alpha\", \"c\", \"seed\"]] = run_df[\"run_name\"].str.split(\"_\", expand=True)\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].str.replace(\"a=\", \"\")\n",
    "run_df[\"V\"] = run_df[\"V\"].str.replace(\"V=\", \"\")\n",
    "run_df[\"c\"] = run_df[\"c\"].str.replace(\"c=\", \"\")\n",
    "run_df[\"seed\"] = run_df[\"seed\"].str.replace(\"seed=\", \"\")\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].astype(float)\n",
    "run_df[\"V\"] = run_df[\"V\"].astype(float)\n",
    "run_df[\"c\"] = run_df[\"c\"].astype(float)\n",
    "run_df[\"seed\"] = run_df[\"seed\"].astype(int)\n",
    "\n",
    "run_df[\"models/small_chosen\"] = run_df[\"models/small_chosen\"].astype(float)\n",
    "run_df[\"models/medium_chosen\"] = run_df[\"models/medium_chosen\"].astype(float)\n",
    "run_df[\"models/large_chosen\"] = run_df[\"models/large_chosen\"].astype(float)\n",
    "\n",
    "display(run_df.head())"
   ],
   "id": "52264c7618a8265f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "analysis_df = run_df.loc[(run_df[\"c\"] == 1.0) & (run_df[\"benchmark_name\"] == \"winogrande\")].pivot_table(index=[\"benchmark_name\", \"alpha\", \"V\", \"c\"], values=[\"avg_accuracy\", \"mess_plus/energy\", \"mess_plus/q_length\", \"total_runtime\"], aggfunc={\"avg_accuracy\": \"mean\", \"mess_plus/energy\": \"sum\", \"mess_plus/q_length\": \"mean\", \"total_runtime\": \"max\"})",
   "id": "e20b51f6fc67780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_value_labels(axx, spacing=5):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in axx.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.2f}\".format(y_value / 1_000_000) # MJ conversion\n",
    "\n",
    "        # Create annotation\n",
    "        axx.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "def fmt_to_megajoules(x, pos):\n",
    "    return f'{(x / 1_000_000):.0f}'\n"
   ],
   "id": "a38483bf31450f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load raw inference data\n",
    "\n",
    "infer_df = pd.DataFrame()\n",
    "def get_inference_data(benchmark_name):\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{NOTEBOOK_PATH.parent}/data/inference_outputs/{benchmark_name}\")\n",
    "\t\tinput_df[\"idx_original\"] = input_df.index\n",
    "\t\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\treturn input_df\n",
    "\texcept ValueError:\n",
    "\t\treturn pd.DataFrame()\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\tinfer_df = pd.concat([infer_df, get_inference_data(name)], ignore_index=True)\n",
    "\n",
    "infer_df.reset_index(inplace=True)\n",
    "\n",
    "# Get baseline dataframe\n",
    "BASELINE_DATA_DIR = f\"{NOTEBOOK_PATH}/data/random_baseline\"\n",
    "baseline_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess_plus_random_baseline_with_constraint_v01\",\n",
    "    save_dir=BASELINE_DATA_DIR,\n",
    "    batch_size=5\n",
    ")\n",
    "\n",
    "baseline_df = load_all_histories_to_dataframe(BASELINE_DATA_DIR)\n",
    "\n",
    "for benchmark in BENCHMARK_NAMES:\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"benchmark_name\"] = benchmark\n",
    "\tbaseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"] = baseline_df.loc[baseline_df[\"run_name\"].str.contains(benchmark), \"run_name\"].str.replace(f\"{benchmark}_alpha=\", \"\")\n",
    "\tbaseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"alpha\"] = baseline_df.loc[baseline_df[\"benchmark_name\"] == benchmark, \"run_name\"]\n",
    "\n",
    "\tbaseline_df[\"alpha\"] = baseline_df[\"alpha\"].astype(float)\n",
    "\n",
    "\n",
    "print(baseline_df.head())\n"
   ],
   "id": "ec68ca6f1bfad7d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=7, figsize=(24, 8.5), gridspec_kw={'width_ratios': [2.3, 2.3, 2.3, 1, 1, 1, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"arc_challenge\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# alpha = target_alpha_per_benchmark[name]\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\taxes[iterator][0].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.01, color='gray', fontsize=12, ha=\"left\")\n",
    "\taxes[iterator][0].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=accuracies_per_model[name][1] + 0.01, color='gray', fontsize=12, ha=\"center\")\n",
    "\taxes[iterator][0].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.01, color='gray', fontsize=12, ha=\"right\")\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=accuracies_per_model[name][1], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][0],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=\"dark:#5A9_r\",\n",
    "\t)\n",
    "\n",
    "\taxes[iterator][0].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "\t# if iterator == 0:\n",
    "\t# \taxes[iterator][0].legend(ncols=4, loc='upper center', bbox_to_anchor=(0.5, 1.55), fontsize=12, title_fontsize=12, title=\"V\", labelspacing =0.1)\n",
    "\n",
    "\taxes[iterator][0].axhline(y=alpha, color='red', linestyle='-', label=\"alpha\")\n",
    "\taxes[iterator][0].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=12, ha=\"right\")\n",
    "\n",
    "\taxes[iterator][0].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\taxes[iterator][0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "\n",
    "\t# Q Plot for SLA violations\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"mess_plus/q_length\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][1],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=\"dark:#5A9_r\",\n",
    "\t)\n",
    "\n",
    "\t# if iterator == 0:\n",
    "\t# \taxes[iterator][1].legend(ncols=3, loc='upper center', bbox_to_anchor=(0.5, 1.55), fontsize=12, title_fontsize=12, title=\"V\", labelspacing =0.1)\n",
    "\n",
    "\t# Energy consumption plot\n",
    "\trandom_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "\trandom_baseline_energy[\"V\"] = \"Rand.\"\n",
    "\trandom_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "\trandom_baseline_energy.reset_index(inplace=True)\n",
    "\n",
    "\traw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "\traw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "\traw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "\traw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "\traw_inference_energy_data.reset_index(inplace=True)\n",
    "\n",
    "\traw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"Llama 70B\", \"energy_consumption_medium\": \"Llama 8B\", \"energy_consumption_small\": \"Llama 1B\"}, inplace=False)\n",
    "\n",
    "\traw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "\n",
    "\tenergy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "\n",
    "\tenergy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "\n",
    "\tenergy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "\tenergy_data.reset_index(inplace=True)\n",
    "\tenergy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "\n",
    "\tsns.barplot(\n",
    "\t    data=energy_data,\n",
    "\t    x=\"V\",\n",
    "\t    y=\"mess_plus/energy\",\n",
    "\t\tax=axes[iterator][2],\n",
    "\t\terrorbar=(\"ci\", 0.95)\n",
    "\t)\n",
    "\n",
    "\tadd_value_labels(axes[iterator][2])\n",
    "\taxes[iterator][2].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "\taxes[iterator][2].set(ylim=[0, 1.4 * energy_data[\"mess_plus/energy\"].max()])\n",
    "\taxes[iterator][2].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "\t# Classifier training loss plot\n",
    "\t# sns.lineplot(\n",
    "\t#     data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t#     x=\"_step\",\n",
    "\t#     y=\"classifier/train_loss\",\n",
    "\t#     hue=\"V\",\n",
    "\t# \terrorbar=None,\n",
    "\t# \tax=axes[3][iterator],\n",
    "\t# \tlegend=False,\n",
    "\t# )\n",
    "\n",
    "\t# Stackplot for Model Call Ratio\n",
    "\tv_values_per_benchmark[name] = sorted(v_values_per_benchmark[name], reverse=False)\n",
    "\t# v_values_per_benchmark[name].reverse()\n",
    "\tfor jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "\t\tstack_df = subset.loc[\n",
    "\t\t\t(run_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(run_df[\"V\"] == V) &\n",
    "\t\t\t(subset[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"V\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t\tx = stack_df[\"_step\"]\n",
    "\t\ty = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\t\ty_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "\t\taxes[iterator][3 + jdx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=1.0)\n",
    "\t\taxes[iterator][3 + jdx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=1.0)\n",
    "\t\taxes[iterator][3 + jdx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=1.0)\n",
    "\t\taxes[iterator][3 + jdx].set(xlabel=f\"Requests @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "\t\taxes[iterator][3 + jdx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t\taxes[iterator][3 + jdx].set(xlim=[0, stack_df[\"_step\"].max()])\n",
    "\n",
    "\t\t# if jdx > 0:\n",
    "\t\t# \taxes[iterator][3 + jdx].get_yaxis().set_visible(False)\n",
    "\n",
    "\t# Add area plot for random baseline with constraint.\n",
    "\tbaseline_stack_df = baseline_df.loc[\n",
    "\t\t\t(baseline_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(baseline_df[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\tx_base = baseline_stack_df[\"_step\"]\n",
    "\ty_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\ty_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "\taxes[iterator][6].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "\taxes[iterator][6].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "\taxes[iterator][6].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "\taxes[iterator][6].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "\taxes[iterator][6].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\taxes[iterator][6].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\t# axes[iterator][6].get_yaxis().set_visible(False)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator][6].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"], ncols=1, loc='upper center', fontsize=10.5, title_fontsize=10.5, title=\"Model\")\n",
    "\n",
    "\n",
    "\taxes[iterator][0].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][1].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][2].set(xlabel=\"\")\n",
    "\t# axes[3][iterator].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\n",
    "\tif iterator > 0:\n",
    "\t\taxes[iterator][0].set(ylabel=None)\n",
    "\t\taxes[iterator][1].set(ylabel=None)\n",
    "\t\taxes[iterator][2].set(ylabel=None)\n",
    "\t\t# axes[3][iterator].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes[iterator], [r\"USF ($\\alpha = {alpha_val}$)\".format(alpha_val=alpha), \"Q Length\", \"Cost (in MJ)\", \"Model Call Ratio (MCR)\", \"\", \"\"]):\n",
    "\n",
    "\t\tif (col == \"Model Call Ratio (MCR)\" and iterator == 1) or col != \"Model Call Ratio (MCR)\":\n",
    "\t\t\tax.set_ylabel(col, rotation=90, size=18)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_all_alpha\", chapter_name=\"evaluations\")\n"
   ],
   "id": "7045943bad636e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(infer_df.columns)\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_large\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_medium\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_small\"].mean())"
   ],
   "id": "3ef80eb1dca358dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot generator\n",
    "v_values_per_benchmark = {\n",
    "    \"arc_challenge\": [0.001, 0.0001, 0.00001],\n",
    "    \"arc_easy\": [0.01, 0.001, 0.0001],\n",
    "    \"boolq\": [0.01, 0.001, 0.0001],\n",
    "    # \"lambada_standard\": [0.01, 0.001, 0.0001],\n",
    "    \"logiqa\": [0.001, 0.0001, 0.00001],\n",
    "    # \"logiqa2\": [0.01, 0.001, 0.0001],\n",
    "    \"piqa\": [0.01, 0.001, 0.0001],\n",
    "    \"sciq\": [0.0001, 0.00001, 0.000001],\n",
    "    \"social_iqa\": [0.001, 0.0001, 0.00001],\n",
    "    \"winogrande\": [0.01, 0.001, 0.0001],\n",
    "}\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "}\n",
    "\n",
    "# Create a list of all benchmark-alpha combinations\n",
    "benchmark_alpha_combinations = []\n",
    "for name in v_values_per_benchmark.keys():\n",
    "    config_path = Path(f\"{NOTEBOOK_PATH.parent}/config/online/{name}.yaml\")\n",
    "    with config_path.open(\"r\") as f:\n",
    "        import yaml\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    algorithm_config = CONFIG[\"algorithm\"]\n",
    "    for alpha in algorithm_config[\"alpha_values\"]:\n",
    "        benchmark_alpha_combinations.append((name, alpha))\n",
    "\n",
    "# Initialize plotting variables\n",
    "plot_num = 0\n",
    "col_count = 0\n",
    "\n",
    "# Iterate through all benchmark-alpha combinations\n",
    "for combo_idx, (name, alpha) in enumerate(benchmark_alpha_combinations):\n",
    "\n",
    "    # Create new figure every 6 columns\n",
    "    if col_count == 0:\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        fig, axes = plt.subplots(nrows=7, ncols=6, figsize=(20, 12))\n",
    "        plot_num += 1\n",
    "\n",
    "    # Get current column index\n",
    "    col_idx = col_count\n",
    "\n",
    "    # Skip if this benchmark doesn't have V values configured\n",
    "    if name not in v_values_per_benchmark.keys():\n",
    "        continue\n",
    "\n",
    "    # Filter data for current benchmark and alpha\n",
    "    subset = run_df.loc[(run_df[\"benchmark_name\"] == name) &\n",
    "                       (run_df[\"c\"] == 0.1) &\n",
    "                       (run_df[\"V\"].isin(v_values_per_benchmark[name])) &\n",
    "                       (run_df[\"_step\"] > 10) &\n",
    "                       (run_df[\"alpha\"] == alpha)]\n",
    "\n",
    "    v_values = subset[\"V\"].unique().tolist()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    raw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.025, color='gray', fontsize=8, ha=\"left\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=accuracies_per_model[name][1] + 0.025, color='gray', fontsize=8, ha=\"center\")\n",
    "    axes[0][col_idx].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.025, color='gray', fontsize=8, ha=\"right\")\n",
    "    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "    axes[0][col_idx].axhline(y=accuracies_per_model[name][1], color='gray', linestyle='--')\n",
    "    axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"avg_accuracy\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[0][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "    )\n",
    "\n",
    "    axes[0][col_idx].plot(\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "\t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "\t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "\t)\n",
    "\n",
    "    axes[0][col_idx].axhline(y=alpha, color='red', linestyle='-')\n",
    "    axes[0][col_idx].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=8, ha=\"right\")\n",
    "\n",
    "    axes[0][col_idx].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[0][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Q Plot for SLA violations\n",
    "    sns.lineplot(\n",
    "        data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "        x=\"_step\",\n",
    "        y=\"mess_plus/q_length\",\n",
    "        hue=\"V\",\n",
    "        errorbar=None,\n",
    "        ax=axes[1][col_idx],\n",
    "        legend=True if col_idx == 0 else False,\n",
    "\t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "    )\n",
    "\n",
    "    if col_idx == 0:\n",
    "        axes[1][col_idx].legend(ncols=2)\n",
    "\n",
    "    # Energy consumption plot\n",
    "    random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "    random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "    random_baseline_energy.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "    raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "    raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "    raw_inference_energy_data.reset_index(inplace=True)\n",
    "\n",
    "    raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"70B\", \"energy_consumption_medium\": \"8B\", \"energy_consumption_small\": \"1B\"}, inplace=False)\n",
    "\n",
    "    raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "    energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "\n",
    "    energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "\n",
    "    energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "    energy_data.reset_index(inplace=True)\n",
    "    energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "\n",
    "    sns.barplot(\n",
    "        data=energy_data,\n",
    "        x=\"V\",\n",
    "        y=\"mess_plus/energy\",\n",
    "        ax=axes[2][col_idx],\n",
    "        errorbar=(\"ci\", 0.95),\n",
    "    )\n",
    "\n",
    "    add_value_labels(axes[2][col_idx])\n",
    "    axes[2][col_idx].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "    axes[2][col_idx].set(ylim=[0, 2 * energy_data[\"mess_plus/energy\"].max()])\n",
    "    axes[2][col_idx].tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "    # Stackplot for Model Call Ratio\n",
    "    for jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "        stack_df = subset.loc[\n",
    "            (run_df[\"benchmark_name\"] == name) &\n",
    "            (run_df[\"V\"] == V) &\n",
    "            (subset[\"alpha\"] == alpha),\n",
    "            [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "        x = stack_df[\"_step\"]\n",
    "        y = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "        y_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "        axes[3 + jdx][col_idx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "        axes[3 + jdx][col_idx].set(xlabel=f\"Request @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "        axes[3 + jdx][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "        if jdx == 0 and col_idx == 0:\n",
    "            axes[3 + jdx][col_idx].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"])\n",
    "\n",
    "    # Add area plot for random baseline with constraint.\n",
    "    baseline_stack_df = baseline_df.loc[\n",
    "            (baseline_df[\"benchmark_name\"] == name) &\n",
    "            (baseline_df[\"alpha\"] == alpha),\n",
    "            [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "        ].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "    x_base = baseline_stack_df[\"_step\"]\n",
    "    y_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "    y_stack_base = np.cumsum(y_base, axis=1)\n",
    "\n",
    "    axes[6][col_idx].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "    axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "    axes[6][col_idx].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "    axes[6][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axes[6][col_idx].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "\n",
    "    # Set axis properties\n",
    "    axes[0][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[1][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "    axes[2][col_idx].set(xlabel=\"\")\n",
    "\n",
    "    # Remove y-labels for columns after the first\n",
    "    if col_idx > 0:\n",
    "        axes[0][col_idx].set(ylabel=None)\n",
    "        axes[1][col_idx].set(ylabel=None)\n",
    "        axes[2][col_idx].set(ylabel=None)\n",
    "\n",
    "    # Set title for each column\n",
    "    axes[0][col_idx].set_title(r\"{bm_name} ($\\alpha = {alpha_val} $)\".format(bm_name=BENCHMARK_NAME_DICT[name], alpha_val=alpha))\n",
    "\n",
    "    # Increment column counter\n",
    "    col_count += 1\n",
    "\n",
    "    # Check if we need to save the current figure and start a new one\n",
    "    if col_count == 6 or combo_idx == len(benchmark_alpha_combinations) - 1:\n",
    "        # Add row labels\n",
    "        for idx, (ax, row) in enumerate(zip(axes[:,0], [\"User Satisfaction\", \"Q Length\", \"Cost (in MJ energy)\", \"\", \"\", \"\", \"\"])):\n",
    "            if idx == 5:\n",
    "                fig.text(0.003, 0.225, \"Model Call Ratio (MCR)\", ha=\"center\", rotation='vertical', fontsize=plt.rcParams['axes.labelsize'])\n",
    "            else:\n",
    "                ax.set_ylabel(row, rotation=90, size='large')\n",
    "\n",
    "        # Save the figure\n",
    "        fig.tight_layout()\n",
    "        write_figure_to_disk(plt, file_name=f\"benchmark_performance_plot_{plot_num}\", chapter_name=\"evaluations\")\n",
    "\n",
    "        # Reset column counter for next figure\n",
    "        col_count = 0"
   ],
   "id": "989db16e594ce6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b2cfd6bf9a6d55d6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
