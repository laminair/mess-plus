{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append('/mnt/c/Users/devmo/Desktop/SciRes/mess-plus')\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "from plots.utils.plotting import write_figure_to_disk\n",
    "\n",
    "NOTEBOOK_PATH = Path(\"experiments.ipynb\").absolute().parent\n",
    "\n",
    "DATA_DIR = f\"{NOTEBOOK_PATH}/data/online\"\n",
    "\n",
    "BENCHMARK_NAMES = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\"]\n",
    "# BENCHMARK_NAMES = [\"winogrande\"]\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a498da98688a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_summary_df = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"mess-plus-api-pricing\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52264c7618a8265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(run_summary_df)\n",
    "run_df = load_all_histories_to_dataframe(DATA_DIR)\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"benchmark_name\"] = name\n",
    "\trun_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"] = run_df.loc[run_df[\"run_name\"].str.contains(name), \"run_name\"].str.replace(f\"{name}_\", \"\")\n",
    "\n",
    "pat = r'V=([^_]+)_a=([^_]+)_c=([^_]+)_seed=([^_]+)'\n",
    "run_df[['V', 'alpha', 'c', 'seed']] = run_df['run_name'].str.extract(pat)\n",
    "run_df[['V', 'alpha', 'c']] = run_df[['V', 'alpha', 'c']].astype(float)\n",
    "run_df['seed'] = run_df['seed'].astype(int)\n",
    "run_df[\"alpha\"] = run_df[\"alpha\"].astype(float)\n",
    "run_df[\"V\"] = run_df[\"V\"].astype(float)\n",
    "run_df[\"c\"] = run_df[\"c\"].astype(float)\n",
    "run_df[\"seed\"] = run_df[\"seed\"].astype(int)\n",
    "\n",
    "run_df[\"models/small_chosen\"] = run_df[\"models/small_chosen\"].astype(float)\n",
    "run_df[\"models/medium_chosen\"] = run_df[\"models/medium_chosen\"].astype(float)\n",
    "run_df[\"models/large_chosen\"] = run_df[\"models/large_chosen\"].astype(float)\n",
    "\n",
    "display(run_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b51f6fc67780",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = run_df.loc[(run_df[\"c\"] == 1.0) & (run_df[\"benchmark_name\"] == \"winogrande\")].pivot_table(index=[\"benchmark_name\", \"alpha\", \"V\", \"c\"], values=[\"avg_accuracy\", \"running_avg_cost_usd\", \"mess_plus/q_length\", \"total_runtime\"], aggfunc={\"avg_accuracy\": \"mean\", \"running_avg_cost_usd\": \"sum\", \"mess_plus/q_length\": \"mean\", \"total_runtime\": \"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38483bf31450f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_value_labels(axx, spacing=5, convert_to_mj: bool = True):\n",
    "    \"\"\"Add labels to the end of each bar in a bar chart.\n",
    "\n",
    "    Arguments:\n",
    "        ax (matplotlib.axes.Axes): The matplotlib object containing the axes\n",
    "            of the plot to annotate.\n",
    "        spacing (int): The distance between the labels and the bars.\n",
    "    \"\"\"\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in axx.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        if convert_to_mj:\n",
    "            label = f'{y_value / 1_000_000:.1f}' # MJ conversion\n",
    "        else:\n",
    "            label = f'{y_value:.2f}'\n",
    "\n",
    "        # Create annotation\n",
    "        axx.annotate(\n",
    "            label,                      # Use `label` as label\n",
    "            (x_value, y_value),         # Place label at end of the bar\n",
    "            xytext=(0, space),          # Vertically shift label by `space`\n",
    "            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha='center',                # Horizontally center label\n",
    "            va=va)                      # Vertically align label differently for\n",
    "                                        # positive and negative values.\n",
    "\n",
    "def fmt_to_megajoules(x, pos):\n",
    "    return f'{(x / 1_000_000):.0f}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267a961aec3b537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d99c0-9d32-4a97-a0e7-2b2e91d6a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df = pd.DataFrame()\n",
    "def get_inference_data(benchmark_name):\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{NOTEBOOK_PATH.parent}/data/inference_outputs/{benchmark_name}\")\n",
    "\t\tinput_df[\"idx_original\"] = input_df.index\n",
    "\t\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\t\treturn input_df\n",
    "\texcept ValueError:\n",
    "\t\treturn pd.DataFrame()\n",
    "\n",
    "for name in BENCHMARK_NAMES:\n",
    "\tinfer_df = pd.concat([infer_df, get_inference_data(name)], ignore_index=True)\n",
    "\n",
    "infer_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57657572dfc3321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_values_per_benchmark = {\n",
    "    \"arc_challenge\": [0.001, 0.0001, 0.00001],\n",
    "    \"arc_easy\": [0.01, 0.001, 0.0001],\n",
    "    \"boolq\": [0.01, 0.001, 0.0001],\n",
    "    # \"lambada_standard\": [0.01, 0.001, 0.0001],\n",
    "    \"logiqa\": [0.001, 0.0001, 0.00001],\n",
    "    # \"logiqa2\": [0.01, 0.001, 0.0001],\n",
    "    \"piqa\": [0.01, 0.001, 0.0001],\n",
    "    \"sciq\": [0.0001, 0.00001, 0.000001],\n",
    "    \"social_iqa\": [0.001, 0.0001, 0.00001],\n",
    "    \"winogrande\": [0.01, 0.001, 0.0001],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbf163-c716-4c26-a293-047a1b6cce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045943bad636e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.1, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(15, 5), gridspec_kw={'width_ratios': [4, 1, 1, 1, 1, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"winogrande\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# alpha = target_alpha_per_benchmark[name]\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\taxes[iterator][0].text(s=\"L1B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].text(s=\"L8B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].text(s=\"L70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.01, color='gray', fontsize=9, ha=\"right\")\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "\taxes[iterator][0].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator][0],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "    \n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator][0].text(s=r\"$ \\alpha = {alpha_val} $ (red line)\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=1.15 * raw_inference_accuracies_per_model[\"label_large\"] - 0.04, color='red', fontsize=9, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\t# Stackplot for Model Call Ratio\n",
    "\tv_values_per_benchmark[name] = sorted(v_values_per_benchmark[name], reverse=False)\n",
    "\t# v_values_per_benchmark[name].reverse()\n",
    "\tfor jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "\n",
    "\t\tstack_df = subset.loc[\n",
    "\t\t\t(run_df[\"benchmark_name\"] == name) &\n",
    "\t\t\t(run_df[\"V\"] == V) &\n",
    "\t\t\t(subset[\"alpha\"] == alpha),\n",
    "\t\t\t[\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "\t\t].groupby([\"_step\"]).mean().reset_index()\n",
    "\n",
    "\t\tx = stack_df[\"_step\"]\n",
    "\t\ty = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "\t\ty_stack = np.cumsum(y, axis=1)\n",
    "\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=1.0)\n",
    "\t\taxes[iterator][1 + jdx].set(xlabel=f\"Requests @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "\t\taxes[iterator][1 + jdx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\t\taxes[iterator][1 + jdx].set(xlim=[0, stack_df[\"_step\"].max()])\n",
    "\n",
    "\t\tif iterator == 0 and jdx == 0:\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L70B\", x=70, y=0.80, color=\"black\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L8B\", x=70, y=0.40, color=\"white\")\n",
    "\t\t\taxes[iterator][1 + jdx].text(s=\"L1B\", x=70, y=0.10, color=\"white\")\n",
    "\n",
    "\taxes[iterator][0].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\taxes[iterator][0].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes[iterator], [r\"Avg. User Satisfaction Rate Over Time (varying $\\alpha$)\".format(alpha_val=alpha), \"Model Call Ratio (MCR)\", \"\", \"\"]):\n",
    "\n",
    "\t\tif iterator == 1:\n",
    "\t\t\tax.set_ylabel(col, rotation=90, size=10)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_all_alpha\", chapter_name=\"evaluations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f9f1d3de21409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(4, 7))\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"winogrande\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# Q Plot for SLA violations\n",
    "\tsubset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"sla_violations\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=1e-5)\", \"Ours (V=0.0001)\", \"Ours (V=0.001)\"],\n",
    "\t)\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[iterator].text(s=r\"$ \\alpha = {alpha_val} $\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=0.17, color='red', fontsize=14, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[iterator].legend(ncols=1, title=\"Method\", fontsize=9, title_fontsize=9, loc='upper left')\n",
    "\n",
    "\taxes[iterator].set(xlabel=\"Requests\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "\n",
    "\taxes[iterator].set(ylabel=None)\n",
    "\n",
    "\tfor ax, col in zip(axes, [\"\", r\"Avg. SLA Violations Over Time (varying $\\alpha$ values)\", \"\"]):\n",
    "\t\tax.set_ylabel(col, rotation=90, size=18)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_sla_violations\", chapter_name=\"evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef80eb1dca358dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infer_df.columns)\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_large\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_medium\"].mean())\n",
    "print(infer_df.groupby(\"benchmark_name\")[\"energy_consumption_small\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989db16e594ce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot generator\n",
    "#\n",
    "# BENCHMARK_NAME_DICT = {\n",
    "#     \"arc_challenge\": \"ARC Challenge\",\n",
    "#     \"arc_easy\": \"ARC Easy\",\n",
    "#     \"boolq\": \"BoolQ\",\n",
    "#     # \"lambada_standard\": \"Lambada\",\n",
    "#     \"logiqa\": \"LogiQA\",\n",
    "#     # \"logiqa2\": \"LogiQA2\",\n",
    "#     \"piqa\": \"PiQA\",\n",
    "#     \"sciq\": \"SciQ\",\n",
    "#     \"social_iqa\": \"SocialIQA\",\n",
    "#     \"winogrande\": \"WinoGrande\",\n",
    "# }\n",
    "#\n",
    "# # Create a list of all benchmark-alpha combinations\n",
    "# benchmark_alpha_combinations = []\n",
    "# for name in v_values_per_benchmark.keys():\n",
    "#     config_path = Path(f\"{NOTEBOOK_PATH.parent}/config/online/{name}.yaml\")\n",
    "#     with config_path.open(\"r\") as f:\n",
    "#         import yaml\n",
    "#         CONFIG = yaml.safe_load(f)\n",
    "#\n",
    "#     algorithm_config = CONFIG[\"algorithm\"]\n",
    "#     for alpha in algorithm_config[\"alpha_values\"]:\n",
    "#         benchmark_alpha_combinations.append((name, alpha))\n",
    "#\n",
    "# # Initialize plotting variables\n",
    "# plot_num = 0\n",
    "# col_count = 0\n",
    "#\n",
    "# # Iterate through all benchmark-alpha combinations\n",
    "# for combo_idx, (name, alpha) in enumerate(benchmark_alpha_combinations):\n",
    "#\n",
    "#     # Create new figure every 6 columns\n",
    "#     if col_count == 0:\n",
    "#         sns.set(style=\"whitegrid\")\n",
    "#         fig, axes = plt.subplots(nrows=7, ncols=6, figsize=(20, 12))\n",
    "#         plot_num += 1\n",
    "#\n",
    "#     # Get current column index\n",
    "#     col_idx = col_count\n",
    "#\n",
    "#     # Skip if this benchmark doesn't have V values configured\n",
    "#     if name not in v_values_per_benchmark.keys():\n",
    "#         continue\n",
    "#\n",
    "#     # Filter data for current benchmark and alpha\n",
    "#     subset = run_df.loc[(run_df[\"benchmark_name\"] == name) &\n",
    "#                        (run_df[\"c\"] == 0.1) &\n",
    "#                        (run_df[\"V\"].isin(v_values_per_benchmark[name])) &\n",
    "#                        (run_df[\"_step\"] > 10) &\n",
    "#                        (run_df[\"alpha\"] == alpha)]\n",
    "#\n",
    "#     v_values = subset[\"V\"].unique().tolist()\n",
    "#\n",
    "#     # Accuracy Plot\n",
    "#     raw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "#\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.1 1B\", x=subset[\"_step\"].min() + 20, y=raw_inference_accuracies_per_model[\"label_small\"] + 0.025, color='gray', fontsize=8, ha=\"left\")\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.1 8B\", x=(subset[\"_step\"].min() + 1/2 * subset[\"_step\"].max()), y=raw_inference_accuracies_per_model[\"label_medium\"] + 0.025, color='gray', fontsize=8, ha=\"center\")\n",
    "#     axes[0][col_idx].text(s=\"Llama 3.3 70B\", x=subset[\"_step\"].max() - 20, y=raw_inference_accuracies_per_model[\"label_large\"] + 0.025, color='gray', fontsize=8, ha=\"right\")\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_small\"], color='gray', linestyle='--')\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_medium\"], color='gray', linestyle='--')\n",
    "#     axes[0][col_idx].axhline(y=raw_inference_accuracies_per_model[\"label_large\"], color='gray', linestyle='--')\n",
    "#\n",
    "#     sns.lineplot(\n",
    "#         data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "#         x=\"_step\",\n",
    "#         y=\"avg_accuracy\",\n",
    "#         hue=\"V\",\n",
    "#         errorbar=None,\n",
    "#         ax=axes[0][col_idx],\n",
    "#         legend=True if col_idx == 0 else False,\n",
    "# \t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "#     )\n",
    "#\n",
    "#     axes[0][col_idx].plot(\n",
    "# \t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha), \"_step\"],\n",
    "# \t\tbaseline_df.loc[(baseline_df[\"benchmark_name\"] == name) & (baseline_df[\"alpha\"] == alpha),\"avg_accuracy\"],\n",
    "# \t\tcolor=\"violet\", linestyle=\"dotted\", label=\"Rand.\"\n",
    "# \t)\n",
    "#\n",
    "#     axes[0][col_idx].axhline(y=alpha, color='red', linestyle='-')\n",
    "#     axes[0][col_idx].text(s=r\"$ \\alpha = {alpha_val} $ \".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=alpha + 0.01, color='red', fontsize=8, ha=\"right\")\n",
    "#\n",
    "#     axes[0][col_idx].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "#\n",
    "#     if col_idx == 0:\n",
    "#         axes[0][col_idx].legend(ncols=2)\n",
    "#\n",
    "#     # Q Plot for SLA violations\n",
    "#     subset.loc[(subset[\"alpha\"] == alpha), \"sla_violations\"] = subset.loc[(subset[\"alpha\"] == alpha), \"mess_plus/q_length\"] / subset.loc[(subset[\"alpha\"] == alpha), \"_step\"]\n",
    "#     sns.lineplot(\n",
    "#         data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "#         x=\"_step\",\n",
    "#         y=\"sla_violations\",\n",
    "#         hue=\"V\",\n",
    "#         errorbar=None,\n",
    "#         ax=axes[1][col_idx],\n",
    "#         legend=True if col_idx == 0 else False,\n",
    "# \t    palette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "#     )\n",
    "#\n",
    "#     if col_idx == 0:\n",
    "#         axes[1][col_idx].legend(ncols=2)\n",
    "#\n",
    "#     # Energy consumption plot\n",
    "#     random_baseline_energy = baseline_df.loc[baseline_df[\"alpha\"] == alpha, [\"benchmark_name\", \"mess_plus/energy\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "#     random_baseline_energy[\"V\"] = \"Rand.\"\n",
    "#     random_baseline_energy[\"mess_plus/energy\"] = random_baseline_energy[name]\n",
    "#     random_baseline_energy.reset_index(inplace=True)\n",
    "#\n",
    "#     raw_inference_energy_data = infer_df[[\"benchmark_name\", \"energy_consumption_large\", \"energy_consumption_medium\", \"energy_consumption_small\"]].groupby(\"benchmark_name\").sum().loc[name].to_frame()\n",
    "#     raw_inference_energy_data[\"V\"] = raw_inference_energy_data.index\n",
    "#     raw_inference_energy_data[\"mess_plus/energy\"] = raw_inference_energy_data[name]\n",
    "#     raw_inference_energy_data.rename({name: \"mess_plus/energy\"}, inplace=True)\n",
    "#     raw_inference_energy_data.reset_index(inplace=True)\n",
    "#\n",
    "#     raw_inference_energy_data[\"V\"] = raw_inference_energy_data[\"V\"].replace({\"energy_consumption_large\": \"70B\", \"energy_consumption_medium\": \"8B\", \"energy_consumption_small\": \"1B\"}, inplace=False)\n",
    "#\n",
    "#     raw_inference_energy_data.drop([name, \"index\"], inplace=True, axis=1)\n",
    "#     energy_data = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"]).agg({\"mess_plus/energy\": \"mean\"}).groupby(\"V\")[\"mess_plus/energy\"].sum().reset_index()\n",
    "#\n",
    "#     energy_data[\"V\"] = energy_data[\"V\"].apply(lambda sample: f\"V={sample}\")\n",
    "#\n",
    "#     energy_data = pd.concat([random_baseline_energy, raw_inference_energy_data, energy_data], ignore_index=True)\n",
    "#     energy_data.reset_index(inplace=True)\n",
    "#     energy_data = energy_data.sort_values(by=[\"mess_plus/energy\"], ascending=False)\n",
    "#\n",
    "#     sns.barplot(\n",
    "#         data=energy_data,\n",
    "#         x=\"V\",\n",
    "#         y=\"mess_plus/energy\",\n",
    "#         ax=axes[2][col_idx],\n",
    "#         errorbar=(\"ci\", 0.95),\n",
    "#     )\n",
    "#\n",
    "#     add_value_labels(axes[2][col_idx])\n",
    "#     axes[2][col_idx].yaxis.set_major_formatter(plt.FuncFormatter(fmt_to_megajoules))\n",
    "#     axes[2][col_idx].set(ylim=[0, 2 * energy_data[\"mess_plus/energy\"].max()])\n",
    "#     axes[2][col_idx].tick_params(axis='x', labelrotation=45)\n",
    "#\n",
    "#     # Stackplot for Model Call Ratio\n",
    "#     for jdx, V in enumerate(v_values_per_benchmark[name]):\n",
    "#\n",
    "#         stack_df = subset.loc[\n",
    "#             (run_df[\"benchmark_name\"] == name) &\n",
    "#             (run_df[\"V\"] == V) &\n",
    "#             (subset[\"alpha\"] == alpha),\n",
    "#             [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "#         ].groupby([\"_step\"]).mean().reset_index()\n",
    "#\n",
    "#         x = stack_df[\"_step\"]\n",
    "#         y = stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "#         y_stack = np.cumsum(y, axis=1)\n",
    "#\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, 0, y_stack.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 0], y_stack.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].fill_between(x, y_stack.iloc[:, 1], y_stack.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "#         axes[3 + jdx][col_idx].set(xlabel=f\"Request @ V={V}\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()], ylim=[0, 1])\n",
    "#         axes[3 + jdx][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "#\n",
    "#         if jdx == 0 and col_idx == 0:\n",
    "#             axes[3 + jdx][col_idx].legend([\"Llama 3.1 1B\", \"Llama 3.1 8B\", \"Llama 3.3 70B\"])\n",
    "#\n",
    "#     # Add area plot for random baseline with constraint.\n",
    "#     baseline_stack_df = baseline_df.loc[\n",
    "#             (baseline_df[\"benchmark_name\"] == name) &\n",
    "#             (baseline_df[\"alpha\"] == alpha),\n",
    "#             [\"_step\", \"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]\n",
    "#         ].groupby([\"_step\"]).mean().reset_index()\n",
    "#\n",
    "#     x_base = baseline_stack_df[\"_step\"]\n",
    "#     y_base = baseline_stack_df[[\"models/small_chosen\", \"models/medium_chosen\", \"models/large_chosen\"]]\n",
    "#     y_stack_base = np.cumsum(y_base, axis=1)\n",
    "#\n",
    "#     axes[6][col_idx].fill_between(x_base, 0, y_stack_base.iloc[:, 0], color=\"#2f364d\", alpha=0.95)\n",
    "#     axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 0], y_stack_base.iloc[:, 1], color=\"#3f758a\", alpha=0.95)\n",
    "#     axes[6][col_idx].fill_between(x_base, y_stack_base.iloc[:, 1], y_stack_base.iloc[:, 2], color=\"#69cf81\", alpha=0.95)\n",
    "#     axes[6][col_idx].set(xlabel=f\"Requests (Rand.)\", xlim=[0, baseline_stack_df[\"_step\"].max()], ylim=[0, 1])\n",
    "#     axes[6][col_idx].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "#     axes[6][col_idx].set(xlim=[0, baseline_stack_df[\"_step\"].max()])\n",
    "#\n",
    "#     # Set axis properties\n",
    "#     axes[0][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "#     axes[1][col_idx].set(xlabel=\"Request\", xlim=[0, subset.loc[(subset[\"alpha\"] == alpha), \"_step\"].max()])\n",
    "#     axes[2][col_idx].set(xlabel=\"\")\n",
    "#\n",
    "#     # Remove y-labels for columns after the first\n",
    "#     if col_idx > 0:\n",
    "#         axes[0][col_idx].set(ylabel=None)\n",
    "#         axes[1][col_idx].set(ylabel=None)\n",
    "#         axes[2][col_idx].set(ylabel=None)\n",
    "#\n",
    "#     # Set title for each column\n",
    "#     axes[0][col_idx].set_title(r\"{bm_name} ($\\alpha = {alpha_val} $)\".format(bm_name=BENCHMARK_NAME_DICT[name], alpha_val=alpha))\n",
    "#\n",
    "#     # Increment column counter\n",
    "#     col_count += 1\n",
    "#\n",
    "#     # Check if we need to save the current figure and start a new one\n",
    "#     if col_count == 6 or combo_idx == len(benchmark_alpha_combinations) - 1:\n",
    "#         # Add row labels\n",
    "#         for idx, (ax, row) in enumerate(zip(axes[:,0], [\"User Satisfaction\", \"SLA Violations\", \"Cost (in MJ energy)\", \"\", \"\", \"\", \"\"])):\n",
    "#             if idx == 5:\n",
    "#                 fig.text(0.003, 0.225, \"Model Call Ratio (MCR)\", ha=\"center\", rotation='vertical', fontsize=plt.rcParams['axes.labelsize'])\n",
    "#             else:\n",
    "#                 ax.set_ylabel(row, rotation=90, size='large')\n",
    "#\n",
    "#         # Save the figure\n",
    "#         fig.tight_layout()\n",
    "#         write_figure_to_disk(plt, file_name=f\"benchmark_performance_plot_{plot_num}\", chapter_name=\"evaluations\")\n",
    "#\n",
    "#         # Reset column counter for next figure\n",
    "#         col_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77532abbf7c205f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pivot_table_for_main_results(input_df: pd.DataFrame, model_cols: list):\n",
    "\n",
    "\tlatest_steps = input_df.groupby(['benchmark_name', 'alpha'])['_step'].transform('max')\n",
    "\tis_last_step = input_df['_step'] == latest_steps\n",
    "\n",
    "\tfor col in model_cols:\n",
    "\t\tinput_df = input_df.rename(columns={col: f\"final_{col}\"})\n",
    "\n",
    "\t# Create new columns with the final values\n",
    "\t# for col in model_cols:\n",
    "\t#     final_values = input_df.loc[is_last_step, ['benchmark_name', 'alpha', col]]\n",
    "\t#     final_values = final_values.drop_duplicates(['benchmark_name', 'alpha'])\n",
    "\t#     input_df = pd.merge(\n",
    "\t#         input_df,\n",
    "\t# \t    final_values.rename(columns={col: f\"final_{col}\"}),\n",
    "\t#         on=['benchmark_name', 'alpha'],\n",
    "\t#         how='left'\n",
    "\t#     )\n",
    "\t#\n",
    "\t# display(input_df.columns)\n",
    "\n",
    "\t# Add the final model values to the pivot table\n",
    "\tmerged_pvt_table = input_df.loc[:, [\"benchmark_name\", \"alpha\", \"V\", \"avg_accuracy\", \"running_avg_cost_usd\"] + [f\"final_{col}\" for col in model_cols]].pivot_table(\n",
    "\t    index=[\"benchmark_name\", \"alpha\"],\n",
    "\t    columns=[\"V\"],\n",
    "\t    values=[\"avg_accuracy\", \"running_avg_cost_usd\"] + [f\"final_{col}\" for col in model_cols],\n",
    "\t    aggfunc={\n",
    "\t        \"avg_accuracy\": [\"mean\", \"std\"],\n",
    "\t        \"running_avg_cost_usd\": [\"sum\", \"std\"],\n",
    "\t        **{f\"final_{col}\": ['mean'] for col in model_cols}\n",
    "\t    }\n",
    "\t)\n",
    "\n",
    "\treturn merged_pvt_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5c28b0517cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "# CREATE LATEX TABLE\n",
    "def multiindex_df_to_latex_chunked(\n",
    "\t\tdf,\n",
    "\t\tchunk_size=3,\n",
    "\t\tlevel2_order=None,\n",
    "\t\tcaption_template=\"Results Table Part {}\",\n",
    "\t\tlabel_template=\"tab:results_part{}\", include_index=True\n",
    "):\n",
    "\t# Get unique level 1 items\n",
    "\tlevel1_items = df.columns.get_level_values(0).unique()\n",
    "\n",
    "\t# Get level 2 items (either in specified order or existing order)\n",
    "\tif level2_order is None:\n",
    "\t\tlevel2_items = df.columns.get_level_values(1).unique()\n",
    "\telse:\n",
    "\t\t# Verify all specified level2 items exist in the DataFrame\n",
    "\t\texisting_level2 = df.columns.get_level_values(1).unique()\n",
    "\t\tfor item in level2_order:\n",
    "\t\t\tif item not in existing_level2:\n",
    "\t\t\t\traise ValueError(f\"Level 2 item '{item}' not found in DataFrame\")\n",
    "\t\tlevel2_items = level2_order\n",
    "\n",
    "\t# Calculate number of chunks\n",
    "\tnum_chunks = math.ceil(len(level1_items) / chunk_size)\n",
    "\n",
    "\tlatex_tables = []\n",
    "\n",
    "\t# Process each chunk\n",
    "\tfor chunk_idx in range(num_chunks):\n",
    "\t\tstart_idx = chunk_idx * chunk_size\n",
    "\t\tend_idx = min((chunk_idx + 1) * chunk_size, len(level1_items))\n",
    "\n",
    "\t\t# Get level 1 items for this chunk\n",
    "\t\tchunk_level1_items = level1_items[start_idx:end_idx]\n",
    "\n",
    "\t\t# Filter DataFrame to only include these level 1 items\n",
    "\t\tchunk_columns = [col for col in df.columns if col[0] in chunk_level1_items]\n",
    "\t\tchunk_df = df[chunk_columns]\n",
    "\n",
    "\t\t# Create a new DataFrame for the LaTeX output with the same higher-level structure\n",
    "\t\t# Get unique combinations of first two levels in this chunk\n",
    "\t\thigher_levels = chunk_df.columns.droplevel(2).unique()\n",
    "\n",
    "\t\t# Create new DataFrame with appropriate multi-index\n",
    "\t\tresult_df = pd.DataFrame(index=df.index)\n",
    "\t\tresult_cols = []\n",
    "\n",
    "\t\t# For each combination of higher levels, combine mean and std\n",
    "\t\tall_alph = []\n",
    "\t\tfor level1 in chunk_level1_items:\n",
    "\t\t\tpattern = r\"\\\\alpha\\s*=\\s*(\\d+)\\\\%\"\n",
    "\t\t\tmatch = re.search(pattern, level1)\n",
    "\t\t\tif match:\n",
    "\t\t\t    number = match.group(1)  # This will be \"50\" as a string\n",
    "\t\t\t    number_int = int(number)  # Convert to integer if needed\n",
    "\t\t\telse:\n",
    "\t\t\t    print(\"No number found\")\n",
    "\t\t\t    number_int = 0\n",
    "\n",
    "\t\t\talph = number_int\n",
    "\t\t\tall_alph.append(alph)\n",
    "\n",
    "\t\t\tfor level2 in level2_items:\n",
    "\t\t\t\tif \"final_models/large\" in level2:\n",
    "\t\t\t\t\tname = r\"\\thead{Model Call Ratio \\\\ (L70B/L8B/L1B)}\"\n",
    "\t\t\t\t\tresult_df[(level1, name)] = [f\"{x:.0f}\\\\% / {y:.0f}\\\\% / {z:.0f}\\\\%\" for x, y, z in zip(df[(level1, \"final_models/large_chosen\", \"mean\")], df[(level1, \"final_models/medium_chosen\", \"mean\")], df[(level1, \"final_models/small_chosen\", \"mean\")])]\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tif level2 == \"avg_accuracy\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Request. \\\\ Satisfaction}\"\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tfor m, s in zip(mean_val, std_val):\n",
    "\t\t\t\t\t\t\tif m >= alph:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{darkgreen}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textcolor{{red}}{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\n",
    "\t\t\t\t\telif level2 == \"running_avg_cost_usd\":\n",
    "\t\t\t\t\t\tname = r\"\\thead{Operating \\\\ Cost}\"\n",
    "\t\t\t\t\t\tlevel3 = \"sum\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tmin_mean_val = mean_val[3:].min()\n",
    "\n",
    "\t\t\t\t\t\tmean_acc = df[level1, \"avg_accuracy\", \"mean\"]\n",
    "\t\t\t\t\t\tif level1 == \"Mean\":\n",
    "\t\t\t\t\t\t\talph = 66.625\n",
    "\n",
    "\t\t\t\t\t\tvals = []\n",
    "\t\t\t\t\t\tacc_match = mean_acc[:3] >= alph\n",
    "\t\t\t\t\t\tis_min_single_satisfying = np.where(acc_match == True)[0]\n",
    "\n",
    "\t\t\t\t\t\tfor idx, (m, s) in enumerate(zip(mean_val, std_val)):\n",
    "\t\t\t\t\t\t\tif m == mean_val[3:].min():\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\textbf{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telif idx == is_min_single_satisfying[0]:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"\\\\underline{{{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$}}\")\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tvals.append(f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\")\n",
    "\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tname = level2\n",
    "\t\t\t\t\t\tlevel3 = \"mean\"\n",
    "\t\t\t\t\t\tmean_val = df[level1, level2, level3]\n",
    "\t\t\t\t\t\tstd_val = df[level1, level2, 'std']\n",
    "\t\t\t\t\t\tvals = [f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\" for m, s in zip(mean_val, std_val)]\n",
    "\n",
    "\t\t\t\t\tresult_df[(level1, name)] = vals\n",
    "\t\t\t\t\tresult_cols.append((level1, name))\n",
    "\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t# Skip if mean or std not available\n",
    "\t\t\t\t\tprint(f\"Warning: Missing mean or std for {level1}, {level2}\")\n",
    "\n",
    "\t\t# Set the columns with multi-index (preserving top 2 levels)\n",
    "\t\tresult_df.columns = pd.MultiIndex.from_tuples(result_cols, names=['Category', 'Subcategory'])\n",
    "\n",
    "\t\t# Convert to LaTeX with multi-index\n",
    "\t\tcaption = caption_template.format(chunk_idx + 1)\n",
    "\t\tlabel = label_template.format(chunk_idx + 1)\n",
    "\n",
    "\t\tlatex_str = result_df.to_latex(escape=False, multicolumn=True, multicolumn_format='c', index=include_index)\n",
    "\n",
    "\t\t# Add caption and label\n",
    "\t\tlatex_str = latex_str.replace('\\\\begin{tabular}',\n",
    "\t\t                              f'\\\\begin{{table}}\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{{tabular}}')\n",
    "\t\tlatex_str = latex_str + '\\\\end{table}'\n",
    "\n",
    "\t\tlatex_tables.append(latex_str)\n",
    "\n",
    "\treturn latex_tables\n",
    "\n",
    "\n",
    "latex_tables = multiindex_df_to_latex_chunked(\n",
    "\tselected_pivot,\n",
    "\tchunk_size=3,\n",
    "\tcaption_template=\"Results Table Part {}: Categories\",\n",
    "\tlabel_template=\"tab:results_part{}\",\n",
    "\tlevel2_order=[\"mess_plus/energy\", \"avg_accuracy\", \"final_models/large_chosen\"]\n",
    ")\n",
    "\n",
    "for t in latex_tables:\n",
    "\tprint(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473856edc48c547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18bc79964bc8d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=3, ncols=8, figsize=(20, 6.75))\n",
    "\n",
    "BENCHMARK_NAME_DICT = {\n",
    "    \"arc_challenge\": \"ARC Challenge\",\n",
    "    \"arc_easy\": \"ARC Easy\",\n",
    "    \"boolq\": \"BoolQ\",\n",
    "    # \"lambada_standard\": \"Lambada\",\n",
    "    \"logiqa\": \"LogiQA\",\n",
    "    # \"logiqa2\": \"LogiQA2\",\n",
    "    \"piqa\": \"PiQA\",\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"social_iqa\": \"SocialIQA\",\n",
    "    \"winogrande\": \"WinoGrande\",\n",
    "}\n",
    "\n",
    "iterator = 0\n",
    "for name, display_name in BENCHMARK_NAME_DICT.items():\n",
    "\n",
    "\tplt_data = run_df.loc[(run_df[\"benchmark_name\"] == name), [\"c\", \"running_avg_cost_usd\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"running_avg_cost_usd\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"mess_plus/exploration_step_ratio\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[0][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "\tplt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"classifier/train_loss\",\n",
    "\t    hue=\"c\",\n",
    "\t\terrorbar=None, # (\"sd\", 1),\n",
    "\t\tax=axes[1][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "\t)\n",
    "\n",
    "\t# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "\tplt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "\tsns.barplot(\n",
    "\t    data=plt_data,\n",
    "\t    x=\"c\",\n",
    "\t    y=\"exploration_cost\",\n",
    "\t\terrorbar=(\"sd\", 1),\n",
    "\t\tax=axes[2][iterator],\n",
    "\t\tlegend=False,\n",
    "\t\testimator=np.sum\n",
    "\t)\n",
    "\n",
    "\taxes[0][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\taxes[0][iterator].set_ylim([0, 1])\n",
    "\taxes[0][iterator].set_xlabel(\"Request\")\n",
    "\taxes[1][iterator].set_xlabel(\"Request\")\n",
    "\taxes[0][iterator].set_title(display_name, fontsize=14)\n",
    "\taxes[0][iterator].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "\taxes[1][iterator].set_ylim([0, 4])\n",
    "\taxes[1][iterator].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "\n",
    "\taxes[2][iterator].set_ylim([0, 1.2 * plt_data.groupby(\"c\")[\"exploration_cost\"].sum().max()])\n",
    "\tadd_value_labels(axes[2][iterator], convert_to_mj=False)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[0][iterator].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "\t\taxes[1][iterator].set_ylabel(\"Router Training Loss\")\n",
    "\t\taxes[2][iterator].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "\t\taxes[0][iterator].legend(title=\"c\")\n",
    "\telse:\n",
    "\t\taxes[0][iterator].set_ylabel(None)\n",
    "\t\taxes[1][iterator].set_ylabel(None)\n",
    "\t\taxes[2][iterator].set_ylabel(None)\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=\"c_ablation_study\", chapter_name=\"evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31897558d27b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_BENCHMARK = \"winogrande\"\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.4, color_codes=True, rc=None)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(3, 4.5))\n",
    "\n",
    "# (run_df[\"benchmark_name\"] == C_BENCHMARK)\n",
    "plt_data = run_df.loc[:, [\"c\", \"running_avg_cost_usd\", \"classifier/train_loss\", \"_step\", \"mess_plus/exploration_step_ratio\", \"mess_plus/p_t\"]]\n",
    "\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"running_avg_cost_usd\"] * plt_data[\"mess_plus/p_t\"]\n",
    "\n",
    "# sns.lineplot(\n",
    "#     data=plt_data[[\"_step\", \"mess_plus/exploration_step_ratio\", \"c\"]],\n",
    "#     x=\"_step\",\n",
    "#     y=\"mess_plus/exploration_step_ratio\",\n",
    "#     hue=\"c\",\n",
    "# \terrorbar=(\"sd\", 1),\n",
    "# \tax=axes[0],\n",
    "# \tlegend=True,\n",
    "# \tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    "# )\n",
    "\n",
    "plt_data.loc[plt_data[\"c\"] == 0.1, \"classifier/train_loss\"] /= 0.1\n",
    "plt_data.loc[plt_data[\"c\"] == 0.01, \"classifier/train_loss\"] /= 0.01\n",
    "\n",
    "sns.lineplot(\n",
    "    data=plt_data[[\"_step\", \"classifier/train_loss\", \"c\"]],\n",
    "    x=\"_step\",\n",
    "    y=\"classifier/train_loss\",\n",
    "    hue=\"c\",\n",
    "\terrorbar=None, # (\"sd\", 1),\n",
    "\tax=axes[0],\n",
    "\tlegend=True,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "# bar_data = plt_data[[\"_step\", \"mess_plus/energy\", \"c\"]].groupby([\"c\"], as_index=False).sum()\n",
    "plt_data[\"exploration_cost\"] = plt_data[\"exploration_cost\"] / 1_000_000 # convert to MJ\n",
    "sns.barplot(\n",
    "    data=plt_data.groupby([\"_step\", \"c\"]).mean(),\n",
    "    x=\"c\",\n",
    "    y=\"exploration_cost\",\n",
    "\terrorbar=(\"sd\", 1),\n",
    "\tax=axes[1],\n",
    "\tlegend=False,\n",
    "\testimator=np.sum,\n",
    "\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"]\n",
    ")\n",
    "\n",
    "# axes[0].set_xlim([0, plt_data[\"_step\"].max()])\n",
    "# axes[0].set_ylim([0, 1])\n",
    "# axes[0].set_xlabel(\"Request\")\n",
    "axes[0].set_xlabel(\"Request\")\n",
    "# axes[0].set_title(\"ARC Challenge\", fontsize=14)\n",
    "# axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "\n",
    "axes[0].set_ylim([0, 4])\n",
    "axes[0].set_xlim([0, 2000])\n",
    "\n",
    "axes[1].set_ylim([0, 1.2 * plt_data.groupby([\"_step\", \"c\"]).mean()[\"exploration_cost\"].sum().max()])\n",
    "add_value_labels(axes[1], convert_to_mj=False)\n",
    "\n",
    "# axes[0].set_ylabel(\"Exploration Ratio (\\%)\")\n",
    "axes[0].set_ylabel(\"Router Training Loss\")\n",
    "axes[1].set_ylabel(\"Exploration Cost (in MJ)\")\n",
    "# axes[0].legend(title=\"c\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "write_figure_to_disk(plt, file_name=f\"c_ablation_study_{C_BENCHMARK}\", chapter_name=\"evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54b8dad9eec877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PLOT: Report on alpha & V dynamics regarding cost and request satisfaction\n",
    "\n",
    "# Set the style for all plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(palette=\"dark:#5A9_r\")\n",
    "\n",
    "sns.set_theme(context='paper', style='whitegrid', palette='dark:#5A9_r', font='sans-serif', font_scale=1.8, color_codes=True, rc=None)\n",
    "\n",
    "# Create a figure and a grid of subplots: 4 rows, 10 columns\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 4), gridspec_kw={'height_ratios': [2, 1]})\n",
    "\n",
    "# # Flatten the 2D array of axes for easier iteration\n",
    "# axes = axes.flatten()\n",
    "\n",
    "name = \"winogrande\"\n",
    "subset = run_df.loc[(run_df[\"benchmark_name\"] == name) & (run_df[\"c\"] == 0.1) & (run_df[\"V\"].isin(v_values_per_benchmark[name])) & (run_df[\"_step\"] > 10)]\n",
    "subset = subset.sort_values(by=[\"alpha\"])\n",
    "\n",
    "def format_v_value(b):\n",
    "\tif b < 0.0001:\n",
    "\t\tb = b * 100\n",
    "\t\tb = b / 100\n",
    "\n",
    "\treturn f\"Ours (V={b})\"\n",
    "\n",
    "subset[\"V\"] = subset[\"V\"].apply(format_v_value)\n",
    "\n",
    "iterator = 0\n",
    "for alpha in subset[\"alpha\"].unique().tolist():\n",
    "\tv_values = subset[\"V\"].unique().tolist()\n",
    "\tc_values = subset[\"c\"].unique().tolist()\n",
    "\n",
    "\t# Accuracy Plot\n",
    "\traw_inference_accuracies_per_model = infer_df[[\"benchmark_name\", \"label_small\", \"label_medium\", \"label_large\"]].groupby(\"benchmark_name\").mean().loc[name]\n",
    "\n",
    "\tsns.lineplot(\n",
    "\t    data=subset.loc[(subset[\"alpha\"] == alpha)],\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"avg_accuracy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[0][iterator],\n",
    "\t\tlegend=True if iterator == 0 else False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=0.01)\", \"Ours (V=0.001)\", \"Ours (V=0.0001)\"],\n",
    "\t)\n",
    "\n",
    "\tif alpha == 0.65:\n",
    "\t\tstep = 740\n",
    "\t\ty = 0.62\n",
    "\t\toffset = 0.04\n",
    "\t\tha = \"center\"\n",
    "\telif alpha == 0.7:\n",
    "\t\tstep = 803\n",
    "\t\ty = 0.65\n",
    "\t\toffset = 0.04\n",
    "\t\tha = \"center\"\n",
    "\telse:\n",
    "\t\tstep = 994\n",
    "\t\ty = 0.65\n",
    "\t\toffset = 0.04\n",
    "\t\tha = \"right\"\n",
    "\n",
    "\taxes[0][iterator].annotate('',\n",
    "        xy=(step, alpha),         # tip of the arrow (endpoint)\n",
    "        xytext=(step + 100, y),       # start point of the arrow\n",
    "        arrowprops=dict(\n",
    "            facecolor='black',\n",
    "            shrink=0.05,      # how much to shrink the arrow from the endpoints\n",
    "            width=2,          # width of arrow in points\n",
    "            headwidth=8,      # width of arrow head in points\n",
    "            headlength=10     # length of arrow head in points\n",
    "        ))\n",
    "\n",
    "\taxes[0][iterator].text(step + 120, y - offset, f\"SLA satisfied @ step {step}\", fontsize=12, ha=ha)\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[0][iterator].legend(loc=\"center\", ncol=6, title=\"Method\", fontsize=7, title_fontsize=7)\n",
    "\n",
    "\taxes[0][iterator].axhline(y=alpha, color='red', linestyle='-', label=\"alpha\")\n",
    "\taxes[0][iterator].set(ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\taxes[0][iterator].yaxis.set_major_formatter(mtick.PercentFormatter(1.0, decimals=0))\n",
    "\n",
    "\t# Add alpha marker\n",
    "\tt1 = axes[0][iterator].text(s=r\"$ \\alpha = {alpha_val} $ (red line)\".format(alpha_val=alpha), x=subset[\"_step\"].max() - 20, y=1.15 * raw_inference_accuracies_per_model[\"label_large\"] - 0.04, color='red', fontsize=12, ha=\"right\")\n",
    "\tt1.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='black', pad=1.5))\n",
    "\n",
    "\trun_df = run_df.sort_values('_step')\n",
    "\tstep_averages = subset.loc[(subset[\"alpha\"] == alpha)].groupby([\"_step\", \"V\"], as_index=False)['running_avg_cost_usd'].mean().reset_index()\n",
    "\n",
    "\t# Step 2: Make sure the data is sorted by step\n",
    "\tstep_averages = step_averages.sort_values('_step')\n",
    "\n",
    "\t# Step 3: Calculate the cumulative sum of the seed-averaged energy values\n",
    "\tfor v in v_values:\n",
    "\t\tstep_averages.loc[(step_averages[\"V\"] == v), 'cumulative_energy_sum'] = step_averages.loc[(step_averages[\"V\"] == v), \"running_avg_cost_usd\"].cumsum()\n",
    "\n",
    "\t# Step 4: Calculate the time average (cumulative sum divided by step)\n",
    "\tstep_averages['time_average_energy'] = step_averages['cumulative_energy_sum'] / step_averages['_step']\n",
    "\n",
    "\t# Energy plot\n",
    "\tsns.lineplot(\n",
    "\t    data=step_averages,\n",
    "\t    x=\"_step\",\n",
    "\t    y=\"time_average_energy\",\n",
    "\t    hue=\"V\",\n",
    "\t\terrorbar=None,\n",
    "\t\tax=axes[1][iterator],\n",
    "\t\tlegend=False,\n",
    "\t\tpalette=[\"#2f364d\", \"#3f758a\", \"#69cf81\"],\n",
    "\t\thue_order=[\"Ours (V=0.01)\", \"Ours (V=0.001)\", \"Ours (V=0.0001)\"],\n",
    "\t)\n",
    "\taxes[0][iterator].set(xlim=[0, subset[\"_step\"].max()], ylim=[0.97 * raw_inference_accuracies_per_model[\"label_small\"], 1.15 * raw_inference_accuracies_per_model[\"label_large\"]])\n",
    "\taxes[0][iterator].set_ylabel(None)\n",
    "\taxes[1][iterator].set_ylabel(None)\n",
    "\taxes[1][iterator].set(xlim=[0, subset[\"_step\"].max()])\n",
    "\n",
    "\tif iterator == 0:\n",
    "\t\taxes[0][iterator].set(ylabel=\"Req. Satisfaction\", xlabel=None)\n",
    "\t\taxes[1][iterator].set(ylabel=\"Avg. Cost (J)\", xlabel=\"Request\")\n",
    "\n",
    "\t\tlegend = axes[0][iterator].get_legend()\n",
    "\t\tlegend.set_frame_on(True)           # Ensure the frame is visible\n",
    "\t\tlegend.get_frame().set_facecolor('white')  # Non-transparent white background\n",
    "\t\tlegend.get_frame().set_edgecolor('darkgray')  # Black frame\n",
    "\t\tlegend.get_frame().set_linewidth(1.5)      # Slightly thicker frame for a sleek look\n",
    "\t\tlegend.get_frame().set_alpha(1.0)          # Fully opaque\n",
    "\n",
    "\telse:\n",
    "\t\taxes[0][iterator].set(ylabel=None, xlabel=None)\n",
    "\t\taxes[1][iterator].set(ylabel=None, xlabel=\"Request\")\n",
    "\n",
    "\n",
    "\titerator += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Export legend\n",
    "def export_legend(legend, filename=\"legend.png\", expand=[-1,-1,1,1]): # ,\n",
    "    fig  = legend.figure\n",
    "    fig.canvas.draw()\n",
    "    bbox  = legend.get_window_extent()\n",
    "    bbox = bbox.from_extents(*(bbox.extents + np.array(expand))) #\n",
    "    bbox = bbox.transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)\n",
    "\n",
    "export_legend(axes[0][0].get_legend(), filename=\"legend_test.pdf\")\n",
    "\n",
    "write_figure_to_disk(plt, file_name=f\"{name}_alpha_v_interplay_no_legend\", chapter_name=\"evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40595dd737a85285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing overhead table\n",
    "\n",
    "run_df[\"classifier_inference_energy\"] = run_df[\"mess_plus/total_energy_incl_classifier\"] - run_df[\"mess_plus/inference_only_energy\"]\n",
    "\n",
    "overhead_df = run_df.loc[run_df[\"classifier_inference_energy\"].notna()]\n",
    "avg_overhead_df = overhead_df[[\"_step\", \"benchmark_name\", \"classifier_inference_energy\", \"running_avg_cost_usd\"]].groupby([\"_step\", \"benchmark_name\"]).mean()\n",
    "avg_overhead_df[\"routing_overhead_ratio\"] = avg_overhead_df[\"classifier_inference_energy\"] / avg_overhead_df[\"running_avg_cost_usd\"]\n",
    "\n",
    "pvt_overhead = avg_overhead_df.pivot_table(index=\"benchmark_name\", values=[\"classifier_inference_energy\", \"running_avg_cost_usd\", \"routing_overhead_ratio\"], aggfunc=[\"mean\", \"std\"])\n",
    "pvt_overhead = pvt_overhead.loc[~(pvt_overhead.index == \"lambada_standard\") & ~(pvt_overhead.index == \"logiqa2\")]\n",
    "\n",
    "pvt_overhead.loc[\"mean\"] = pvt_overhead.mean(axis=0)\n",
    "\n",
    "print(pvt_overhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cfc59235bba3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE LATEX TABLE\n",
    "def multiindex_df_to_latex_simple_for_overhead(\n",
    "\t\tdf,\n",
    "\t\tchunk_size=3,\n",
    "\t\tlevel2_order=None,\n",
    "\t\tcaption_template=\"Results Table Part {}\",\n",
    "\t\tlabel_template=\"tab:results_part{}\", include_index=True\n",
    "):\n",
    "\t# Get unique level 1 items\n",
    "\tlevel1_items = df.columns.get_level_values(0).unique()\n",
    "\n",
    "\t# Get level 2 items (either in specified order or existing order)\n",
    "\tif level2_order is None:\n",
    "\t\tlevel2_items = df.columns.get_level_values(1).unique()\n",
    "\telse:\n",
    "\t\t# Verify all specified level2 items exist in the DataFrame\n",
    "\t\texisting_level2 = df.columns.get_level_values(1).unique()\n",
    "\t\tfor item in level2_order:\n",
    "\t\t\tif item not in existing_level2:\n",
    "\t\t\t\traise ValueError(f\"Level 2 item '{item}' not found in DataFrame\")\n",
    "\t\tlevel2_items = level2_order\n",
    "\n",
    "\t# Calculate number of chunks\n",
    "\tnum_chunks = math.ceil(len(level1_items) / chunk_size)\n",
    "\n",
    "\tlatex_tables = []\n",
    "\n",
    "\t# Process each chunk\n",
    "\tfor chunk_idx in range(num_chunks):\n",
    "\t\tstart_idx = chunk_idx * chunk_size\n",
    "\t\tend_idx = min((chunk_idx + 1) * chunk_size, len(level1_items))\n",
    "\n",
    "\t\t# Get level 1 items for this chunk\n",
    "\t\tchunk_level1_items = level1_items[start_idx:end_idx]\n",
    "\n",
    "\t\t# Filter DataFrame to only include these level 1 items\n",
    "\t\tchunk_columns = [col for col in df.columns if col[0] in chunk_level1_items]\n",
    "\t\tchunk_df = df[chunk_columns]\n",
    "\n",
    "\t\t# Create new DataFrame with appropriate multi-index\n",
    "\t\tresult_df = pd.DataFrame(index=df.index)\n",
    "\t\tresult_cols = []\n",
    "\n",
    "\t\tfor level1 in chunk_level1_items:\n",
    "\n",
    "\t\t\tif level1 == \"std\":\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tfor level2 in level2_items:\n",
    "\t\t\t\tmean_val = df[level1, level2]\n",
    "\t\t\t\tstd_val = df[\"std\", level2]\n",
    "\n",
    "\t\t\t\tif level2 == \"running_avg_cost_usd\" or level2 ==  \"classifier_inference_energy\":\n",
    "\t\t\t\t\tresult_df[level2] = [f\"{m:.2f}$\\\\scriptscriptstyle\\\\pm{s:.2f}$\" for m, s in zip(mean_val, std_val)]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresult_df[level2] = [f\"{(m * 100):.2f}$\\\\scriptscriptstyle\\\\pm{(s * 100):.2f}$\" for m, s in zip(mean_val, std_val)]\n",
    "\n",
    "\t\t\t\tresult_cols.append(level2)\n",
    "\n",
    "\t\t# Set the columns with multi-index (preserving top 2 levels)\n",
    "\t\t# result_df.columns = pd.MultiIndex.from_tuples(result_cols, names=['Category', 'Subcategory'])\n",
    "\n",
    "\t\t# Convert to LaTeX with multi-index\n",
    "\t\tcaption = caption_template.format(chunk_idx + 1)\n",
    "\t\tlabel = label_template.format(chunk_idx + 1)\n",
    "\n",
    "\t\tlatex_str = result_df.to_latex(escape=False, multicolumn=True, multicolumn_format='c', index=include_index)\n",
    "\n",
    "\t\t# Add caption and label\n",
    "\t\tlatex_str = latex_str.replace('\\\\begin{tabular}',\n",
    "\t\t                              f'\\\\begin{{table}}\\n\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\\\\begin{{tabular}}')\n",
    "\t\tlatex_str = latex_str + '\\\\end{table}'\n",
    "\n",
    "\t\tlatex_tables.append(latex_str)\n",
    "\n",
    "\treturn latex_tables\n",
    "\n",
    "tables = multiindex_df_to_latex_simple_for_overhead(\n",
    "\tpvt_overhead,\n",
    "\tchunk_size=3,\n",
    "\tlevel2_order=None,\n",
    "\tcaption_template=\"Results Table Part {}\",\n",
    "\tlabel_template=\"tab:results_part{}\",\n",
    "\tinclude_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458420948e13a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tab in tables:\n",
    "\tprint(tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746cf42fb51fb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
