{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "\n",
    "BENCHMARK_NAME = \"arc_challenge\"\n",
    "# this refers to whether we want to use a pre-trained classifier or learn the classifier online while benchmarking.\n",
    "APPROACH = \"online\"  # alt: online\n",
    "# This only has an effect when APPROACH = pretrained. Make sure to adjust the minibatch size accordingly!\n",
    "NUM_PRETRAINING_STEPS = 400\n",
    "SEEDS = [42]\n",
    "NUM_CLASSIFIER_LABELS = 3\n",
    "\n",
    "PROJECT_ROOT_PATH = Path(\"mess_plus_simulator\").parent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60dc8707e7541d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def choose_llm_from_zoo(available_models: list, probs: np.array):\n",
    "\treturn np.random.choice(available_models, p=probs)"
   ],
   "id": "64b79435161c67c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_probabilities(model_accuracies, alpha):\n",
    "    \"\"\"\n",
    "    Function to compute a priori probabilities for a baseline model selection process.\n",
    "    \"\"\"\n",
    "    accuracies = np.array(model_accuracies)\n",
    "    n = len(accuracies)\n",
    "\n",
    "    # Check if possible\n",
    "    if alpha > max(accuracies):\n",
    "        raise ValueError(\"Alpha too high\")\n",
    "\n",
    "    # Method 1: Try iterative approach\n",
    "    p = np.ones(n) / n\n",
    "\n",
    "    for _ in range(5000):\n",
    "        current_acc = np.dot(p, accuracies)\n",
    "\n",
    "        if current_acc >= alpha - 1e-6:\n",
    "            return p\n",
    "\n",
    "        # Simple update\n",
    "        for i in range(n):\n",
    "            if accuracies[i] > current_acc:\n",
    "                p[i] *= 1.01  # Increase good models\n",
    "            else:\n",
    "                p[i] *= 0.99  # Decrease bad models\n",
    "\n",
    "        # Normalize\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "    # Method 2: If iterative fails, use direct calculation\n",
    "    # Sort by accuracy\n",
    "    idx_sorted = np.argsort(accuracies)[::-1]\n",
    "\n",
    "    # Calculate minimum probability for best model\n",
    "    best_acc = accuracies[idx_sorted[0]]\n",
    "    worst_acc = accuracies[idx_sorted[-1]]\n",
    "\n",
    "    # Start with minimum probabilities for all\n",
    "    min_prob = 1e-10\n",
    "    p = np.full(n, min_prob)\n",
    "    remaining = 1.0 - n * min_prob\n",
    "\n",
    "    # Distribute remaining probability\n",
    "    for i in range(n):\n",
    "        idx = idx_sorted[i]\n",
    "\n",
    "        if i == n - 1:\n",
    "            p[idx] += remaining\n",
    "        else:\n",
    "            # Give more to better models\n",
    "            weight = (accuracies[idx] - worst_acc) / (best_acc - worst_acc)\n",
    "            allocation = remaining * weight * 0.8\n",
    "            p[idx] += allocation\n",
    "            remaining -= allocation\n",
    "\n",
    "    # Final normalization\n",
    "    p = p / np.sum(p)\n",
    "\n",
    "    return p"
   ],
   "id": "e1f3f2356c2b7abc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BENCHMARKS = [\"arc_challenge\", \"arc_easy\", \"boolq\", \"lambada_standard\", \"logiqa\", \"logiqa2\", \"piqa\", \"sciq\", \"social_iqa\", \"winogrande\"]\n",
    "for BENCHMARK_NAME in BENCHMARKS:\n",
    "\n",
    "\t# Load benchmark config\n",
    "\tif APPROACH == \"pretrained\":\n",
    "\t\tconfig_path = Path(f\"{PROJECT_ROOT_PATH}/config/pretrained/{BENCHMARK_NAME}.yaml\")\n",
    "\telif APPROACH == \"online\":\n",
    "\t\tconfig_path = Path(f\"{PROJECT_ROOT_PATH}/config/online/{BENCHMARK_NAME}.yaml\")\n",
    "\t\tNUM_PRETRAINING_STEPS = 0\n",
    "\telse:\n",
    "\t\traise NotImplementedError(f\"Approach {APPROACH} not implemented.\")\n",
    "\n",
    "\twith config_path.open(\"r\") as f:\n",
    "\t\tCONFIG = yaml.safe_load(f)\n",
    "\t\tdisplay(CONFIG)\n",
    "\n",
    "\talgorithm_config = CONFIG[\"algorithm\"]\n",
    "\n",
    "\t# Load data\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{PROJECT_ROOT_PATH}/data/inference_outputs/{BENCHMARK_NAME}\")\n",
    "\texcept ValueError:\n",
    "\t\tcontinue\n",
    "\n",
    "\tinput_df[\"idx_original\"] = input_df.index\n",
    "\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\tavailable_models = [i for i in input_df.columns if \"label\" in i]\n",
    "\n",
    "\t# Get inputs for chocie probabilities\n",
    "\tmodel_performance = {}\n",
    "\tfor model in available_models:\n",
    "\t\tmodel_performance[model] = input_df[model].mean()\n",
    "\n",
    "\tmodel_keys = [i for i in model_performance.keys()]\n",
    "\tfor alpha in algorithm_config[\"alpha_values\"]:\n",
    "\t\tprobs = calculate_probabilities([i for i in model_performance.values()], alpha)\n",
    "\n",
    "\t\trun = wandb.init(\n",
    "\t\t\tproject=\"mess_plus_random_baseline_with_constraint_v01\",\n",
    "\t\t\tname=f\"{BENCHMARK_NAME}_alpha={alpha}\",\n",
    "\t\t\tconfig=CONFIG\n",
    "\t\t)\n",
    "\n",
    "\t\tMODEL_CHOICES = []\n",
    "\t\tACCURACY_LIST = []\n",
    "\t\tfor idx, row in input_df.iterrows():\n",
    "\t\t\tstatistics_dict = {}\n",
    "\t\t\tmodel_choice = choose_llm_from_zoo(model_keys, probs)\n",
    "\t\t\tmodel_name = model_choice.split(\"_\")[-1]\n",
    "\n",
    "\t\t\tMODEL_CHOICES.append(model_keys.index(model_choice))\n",
    "\t\t\tACCURACY_LIST.append(row[model_choice])\n",
    "\n",
    "\t\t\tx = np.array(MODEL_CHOICES)\n",
    "\t\t\tstatistics_dict[\"model_choice\"] = model_keys.index(model_choice)\n",
    "\t\t\tstatistics_dict[\"avg_accuracy\"] = sum(ACCURACY_LIST) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/small_chosen\"] = len(np.where(x == 0)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/medium_chosen\"] = len(np.where(x == 1)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/large_chosen\"] = len(np.where(x == 2)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"mess_plus/energy\"] = row[f\"energy_consumption_{model_name}\"]\n",
    "\n",
    "\t\t\tif wandb.run is not None:\n",
    "\t\t\t\twandb.log(statistics_dict, step=idx)\n",
    "\n",
    "\t\twandb.finish()"
   ],
   "id": "b6f1cf8080b39d9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c26287fd30998fcc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
