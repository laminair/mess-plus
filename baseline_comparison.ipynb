{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-27T00:54:44.345416Z",
     "start_time": "2025-07-27T00:54:44.330654Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from classifier.file_reader import read_files_from_folder\n",
    "from evaluations.utils.wandb_loader import download_log_data, load_all_histories_to_dataframe\n",
    "\n",
    "MODEL_FAMILY = \"qwen2\"\n",
    "BENCHMARKS = [\n",
    "\t\"arc_challenge\",\n",
    "\t\"arc_easy\",\n",
    "\t\"boolq\",\n",
    "\t# \"lambada_standard\",\n",
    "\t\"logiqa\",\n",
    "\t# \"logiqa2\",\n",
    "\t\"piqa\",\n",
    "\t\"sciq\",\n",
    "\t\"social_iqa\",\n",
    "\t\"winogrande\"\n",
    "]\n",
    "# this refers to whether we want to use a pre-trained classifier or learn the classifier online while benchmarking.\n",
    "APPROACH = \"online\"  # alt: online\n",
    "# This only has an effect when APPROACH = pretrained. Make sure to adjust the minibatch size accordingly!\n",
    "NUM_PRETRAINING_STEPS = 400\n",
    "SEEDS = [42]\n",
    "NUM_CLASSIFIER_LABELS = 3\n",
    "\n",
    "PROJECT_ROOT_PATH = Path(\"mess_plus_simulator\").parent"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:54:44.389141Z",
     "start_time": "2025-07-27T00:54:44.377912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_llm_from_zoo(available_models: list, probs: np.array):\n",
    "\treturn np.random.choice(available_models, p=probs)"
   ],
   "id": "64b79435161c67c1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:54:44.423603Z",
     "start_time": "2025-07-27T00:54:44.412968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_probabilities(model_accuracies, alpha):\n",
    "    \"\"\"\n",
    "    Function to compute a priori probabilities for a baseline model selection process.\n",
    "    \"\"\"\n",
    "    accuracies = np.array(model_accuracies)\n",
    "    n = len(accuracies)\n",
    "\n",
    "    # Check if possible\n",
    "    if alpha > max(accuracies):\n",
    "        raise ValueError(\"Alpha too high\")\n",
    "\n",
    "    # Method 1: Try iterative approach\n",
    "    p = np.ones(n) / n\n",
    "\n",
    "    for _ in range(5000):\n",
    "        current_acc = np.dot(p, accuracies)\n",
    "\n",
    "        if current_acc >= alpha - 1e-6:\n",
    "            return p\n",
    "\n",
    "        # Simple update\n",
    "        for i in range(n):\n",
    "            if accuracies[i] > current_acc:\n",
    "                p[i] *= 1.01  # Increase good models\n",
    "            else:\n",
    "                p[i] *= 0.99  # Decrease bad models\n",
    "\n",
    "        # Normalize\n",
    "        p = p / np.sum(p)\n",
    "\n",
    "    # Method 2: If iterative fails, use direct calculation\n",
    "    # Sort by accuracy\n",
    "    idx_sorted = np.argsort(accuracies)[::-1]\n",
    "\n",
    "    # Calculate minimum probability for best model\n",
    "    best_acc = accuracies[idx_sorted[0]]\n",
    "    worst_acc = accuracies[idx_sorted[-1]]\n",
    "\n",
    "    # Start with minimum probabilities for all\n",
    "    min_prob = 1e-10\n",
    "    p = np.full(n, min_prob)\n",
    "    remaining = 1.0 - n * min_prob\n",
    "\n",
    "    # Distribute remaining probability\n",
    "    for i in range(n):\n",
    "        idx = idx_sorted[i]\n",
    "\n",
    "        if i == n - 1:\n",
    "            p[idx] += remaining\n",
    "        else:\n",
    "            # Give more to better models\n",
    "            weight = (accuracies[idx] - worst_acc) / (best_acc - worst_acc)\n",
    "            allocation = remaining * weight * 0.8\n",
    "            p[idx] += allocation\n",
    "            remaining -= allocation\n",
    "\n",
    "    # Final normalization\n",
    "    p = p / np.sum(p)\n",
    "\n",
    "    return p"
   ],
   "id": "e1f3f2356c2b7abc",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for BENCHMARK_NAME in BENCHMARKS:\n",
    "\n",
    "\t# Load benchmark config\n",
    "\tconfig_path = Path(f\"{PROJECT_ROOT_PATH}/config/{MODEL_FAMILY}/online/{BENCHMARK_NAME}.yaml\")\n",
    "\tNUM_PRETRAINING_STEPS = 0\n",
    "\n",
    "\twith config_path.open(\"r\") as f:\n",
    "\t\tCONFIG = yaml.safe_load(f)\n",
    "\t\tdisplay(CONFIG)\n",
    "\n",
    "\talgorithm_config = CONFIG[\"algorithm\"]\n",
    "\n",
    "\t# Load data\n",
    "\ttry:\n",
    "\t\tinput_df = read_files_from_folder(folder_path=f\"{PROJECT_ROOT_PATH}/data/{MODEL_FAMILY}/inference_outputs/{BENCHMARK_NAME}\")\n",
    "\texcept ValueError:\n",
    "\t\tcontinue\n",
    "\n",
    "\tinput_df[\"idx_original\"] = input_df.index\n",
    "\tinput_df = input_df.sample(frac=1).reset_index(drop=True)\n",
    "\tavailable_models = [i for i in input_df.columns if \"label\" in i]\n",
    "\n",
    "\t# Get inputs for chocie probabilities\n",
    "\tmodel_performance = {}\n",
    "\tfor model in available_models:\n",
    "\t\tmodel_performance[model] = input_df[model].mean()\n",
    "\n",
    "\tmodel_keys = [i for i in model_performance.keys()]\n",
    "\tfor alpha in algorithm_config[\"alpha_values\"]:\n",
    "\t\tprobs = calculate_probabilities([i for i in model_performance.values()], alpha)\n",
    "\n",
    "\t\trun = wandb.init(\n",
    "\t\t\tproject=\"mess_plus_qwen2_random_baseline_with_constraint_v01\",\n",
    "\t\t\tname=f\"{BENCHMARK_NAME}_alpha={alpha}\",\n",
    "\t\t\tconfig=CONFIG\n",
    "\t\t)\n",
    "\n",
    "\t\tMODEL_CHOICES = []\n",
    "\t\tACCURACY_LIST = []\n",
    "\t\tQ = 0.0\n",
    "\t\tfor idx, row in input_df.iterrows():\n",
    "\t\t\tstatistics_dict = {}\n",
    "\t\t\tmodel_choice = choose_llm_from_zoo(model_keys, probs)\n",
    "\t\t\tmodel_name = model_choice.split(\"_\")[-1]\n",
    "\n",
    "\t\t\tMODEL_CHOICES.append(model_keys.index(model_choice))\n",
    "\t\t\tACCURACY_LIST.append(row[model_choice])\n",
    "\n",
    "\t\t\tx = np.array(MODEL_CHOICES)\n",
    "\t\t\tQ = max(0.0, Q + alpha - row[model_choice])\n",
    "\n",
    "\t\t\tstatistics_dict[\"model_choice\"] = model_keys.index(model_choice)\n",
    "\t\t\tstatistics_dict[\"avg_accuracy\"] = sum(ACCURACY_LIST) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/xsmall_chosen\"] = len(np.where(x == 0)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/small_chosen\"] = len(np.where(x == 1)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/medium_chosen\"] = len(np.where(x == 2)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"models/large_chosen\"] = len(np.where(x == 3)[0]) / (idx + 1)\n",
    "\t\t\tstatistics_dict[\"mess_plus/energy\"] = row[f\"energy_consumption_{model_name}\"]\n",
    "\t\t\tstatistics_dict[\"mess_plus/q_length\"] = Q\n",
    "\n",
    "\t\t\tif wandb.run is not None:\n",
    "\t\t\t\twandb.log(statistics_dict, step=idx)\n",
    "\n",
    "\t\twandb.finish()"
   ],
   "id": "b6f1cf8080b39d9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:55:48.463287Z",
     "start_time": "2025-07-27T00:55:48.460036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparison with RouteLLM\n",
    "DATA_DIR = f\"{PROJECT_ROOT_PATH}/data/routellm_raw\"\n",
    "routellm_logs = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routellm-sweepv2\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routellm_df = load_all_histories_to_dataframe(DATA_DIR)\n"
   ],
   "id": "c26287fd30998fcc",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:55:48.473158Z",
     "start_time": "2025-07-27T00:55:48.470344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "routellm_df[[\"router_model\", \"benchmark\", \"thr\", \"threshold\"]] = routellm_df[\"run_name\"].str.split(\"-\", expand=True)\n",
    "BENCHMARKS = routellm_df[\"benchmark\"].unique().tolist()\n",
    "\n",
    "print(BENCHMARKS)\n",
    "\n",
    "MODEL_CHOICE_DICT = {\n",
    "    1: \"large\",\n",
    "    0: \"small\",\n",
    "}\n",
    "\n",
    "for benchmark in BENCHMARKS:\n",
    "    # Load benchmark config\n",
    "    config_path = Path(f\"{PROJECT_ROOT_PATH}/config/online/{benchmark}.yaml\")\n",
    "    NUM_PRETRAINING_STEPS = 0\n",
    "\n",
    "    with config_path.open(\"r\") as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    algorithm_config = CONFIG[\"algorithm\"]\n",
    "\n",
    "    try:\n",
    "        benchmark_raw_df = read_files_from_folder(f\"{PROJECT_ROOT_PATH}/data/inference_outputs/{benchmark}\")\n",
    "    except ValueError:\n",
    "\t    continue\n",
    "\n",
    "    THRESHOLDS = routellm_df.loc[routellm_df[\"benchmark\"] == benchmark, \"threshold\"].unique().tolist()\n",
    "\n",
    "    display(f\"BENCHMARK: {benchmark} - {benchmark_raw_df['label_large'].mean()}\")\n",
    "\n",
    "    for alpha in algorithm_config[\"alpha_values\"]:\n",
    "        for threshold in THRESHOLDS:\n",
    "\n",
    "            run = wandb.init(\n",
    "                project=\"routellm_baseline_v01\",\n",
    "                name=f\"{benchmark}_alpha={alpha}_thres={threshold}\",\n",
    "                config=CONFIG\n",
    "            )\n",
    "\n",
    "            MODEL_CHOICES = []\n",
    "            ACCURACY_LIST = []\n",
    "            Q = 0.0\n",
    "\n",
    "            iterator = 0\n",
    "            for idx, row in routellm_df.loc[(routellm_df[\"benchmark\"] == benchmark) & (routellm_df[\"threshold\"] == threshold)].iterrows():\n",
    "                statistics_dict = {}\n",
    "                model_choice_id = row[\"model_choice\"]\n",
    "\n",
    "                # Get accuracy from raw evaluations\n",
    "                chosen_model = MODEL_CHOICE_DICT[model_choice_id]\n",
    "                row_benchmark_data = benchmark_raw_df.loc[(benchmark_raw_df.index == row[\"document_id\"])]\n",
    "\n",
    "                result_data = row_benchmark_data[f\"label_{chosen_model}\"].values\n",
    "                try:\n",
    "                    result = result_data[0]\n",
    "                except IndexError as e:\n",
    "                    print(f\"Error encountered for {benchmark}: #{iterator}\")\n",
    "                    continue\n",
    "\n",
    "                MODEL_CHOICES.append(model_choice_id)\n",
    "                ACCURACY_LIST.append(result)\n",
    "                x = np.array(MODEL_CHOICES)\n",
    "                Q = max(0.0, Q + alpha - result)\n",
    "\n",
    "                total_energy = row_benchmark_data[f\"energy_consumption_{chosen_model}\"].values[0] + row[\"energy\"]\n",
    "\n",
    "                statistics_dict[\"model_choice\"] = model_choice_id\n",
    "                statistics_dict[\"avg_accuracy\"] = sum(ACCURACY_LIST) / (iterator + 1)\n",
    "                statistics_dict[\"models/small_chosen\"] = len(np.where(x == 0)[0]) / (iterator + 1)\n",
    "                statistics_dict[\"models/large_chosen\"] = len(np.where(x == 1)[0]) / (iterator + 1)\n",
    "                statistics_dict[\"mess_plus/energy\"] = total_energy\n",
    "                statistics_dict[\"routellm/router_energy\"] = row[\"energy\"]\n",
    "                statistics_dict[\"routellm/inference_energy\"] = row_benchmark_data[f\"energy_consumption_{chosen_model}\"].values[0]\n",
    "                statistics_dict[\"mess_plus/q_length\"] = Q\n",
    "\n",
    "                if wandb.run is not None:\n",
    "                    wandb.log(statistics_dict, step=iterator)\n",
    "\n",
    "                iterator += 1\n",
    "\n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "\n",
    "            time.sleep(3)\n"
   ],
   "id": "c1bfba07e1f21aa9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:55:48.479211Z",
     "start_time": "2025-07-27T00:55:48.477436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparison with RouterDC\n",
    "DATA_DIR = f\"{PROJECT_ROOT_PATH}/data/routerdc_raw\"\n",
    "routerdc_logs = download_log_data(\n",
    "    entity=\"tum-i13\",\n",
    "    project_name=\"routerdc-sweep\",\n",
    "    save_dir=DATA_DIR,\n",
    "    batch_size=50\n",
    ")\n",
    "\n",
    "routerdc_logs = load_all_histories_to_dataframe(DATA_DIR)"
   ],
   "id": "83c077a14eac22ee",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:55:48.495482Z",
     "start_time": "2025-07-27T00:55:48.492625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "routerdc_logs[\"benchmark\"] = routerdc_logs[\"run_name\"].str.replace(\"routerdc-\", \"\")\n",
    "BENCHMARKS = routerdc_logs[\"benchmark\"].unique().tolist()\n",
    "\n",
    "print(BENCHMARKS)\n",
    "\n",
    "MODEL_CHOICE_DICT = {\n",
    "    2: \"large\",\n",
    "    1: \"medium\",\n",
    "    0: \"small\",\n",
    "}\n",
    "\n",
    "for benchmark in BENCHMARKS:\n",
    "    # Load benchmark config\n",
    "    config_path = Path(f\"{PROJECT_ROOT_PATH}/config/online/{benchmark}.yaml\")\n",
    "    NUM_PRETRAINING_STEPS = 0\n",
    "\n",
    "    with config_path.open(\"r\") as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    algorithm_config = CONFIG[\"algorithm\"]\n",
    "\n",
    "    try:\n",
    "        benchmark_raw_df = read_files_from_folder(f\"{PROJECT_ROOT_PATH}/data/inference_outputs/{benchmark}\")\n",
    "    except ValueError:\n",
    "\t    continue\n",
    "\n",
    "    display(f\"BENCHMARK: {benchmark} - {benchmark_raw_df['label_large'].mean()}\")\n",
    "\n",
    "    for alpha in algorithm_config[\"alpha_values\"]:\n",
    "\n",
    "        run = wandb.init(\n",
    "            project=\"routerdc_baseline_v01\",\n",
    "            name=f\"{benchmark}-alpha={alpha}\",\n",
    "            config=CONFIG\n",
    "        )\n",
    "\n",
    "        MODEL_CHOICES = []\n",
    "        ACCURACY_LIST = []\n",
    "        Q = 0.0\n",
    "\n",
    "        iterator = 0\n",
    "        for idx, row in routerdc_logs.loc[(routerdc_logs[\"benchmark\"] == benchmark)].iterrows():\n",
    "            statistics_dict = {}\n",
    "            model_choice_id = row[\"model_choice\"]\n",
    "\n",
    "            # Get accuracy from raw evaluations\n",
    "            chosen_model = MODEL_CHOICE_DICT[model_choice_id]\n",
    "            row_benchmark_data = benchmark_raw_df.loc[(benchmark_raw_df.index == row[\"document_id\"])]\n",
    "\n",
    "            result_data = row_benchmark_data[f\"label_{chosen_model}\"].values\n",
    "            try:\n",
    "                result = result_data[0]\n",
    "            except IndexError as e:\n",
    "                print(f\"Error encountered for {benchmark}: #{iterator}\")\n",
    "                continue\n",
    "\n",
    "            MODEL_CHOICES.append(model_choice_id)\n",
    "            ACCURACY_LIST.append(result)\n",
    "            x = np.array(MODEL_CHOICES)\n",
    "            Q = max(0.0, Q + alpha - result)\n",
    "\n",
    "            total_energy = row_benchmark_data[f\"energy_consumption_{chosen_model}\"].values[0] + row[\"energy\"]\n",
    "\n",
    "            statistics_dict[\"model_choice\"] = model_choice_id\n",
    "            statistics_dict[\"avg_accuracy\"] = sum(ACCURACY_LIST) / (iterator + 1)\n",
    "            statistics_dict[\"models/small_chosen\"] = len(np.where(x == 0)[0]) / (iterator + 1)\n",
    "            statistics_dict[\"models/medium_chosen\"] = len(np.where(x == 1)[0]) / (iterator + 1)\n",
    "            statistics_dict[\"models/large_chosen\"] = len(np.where(x == 2)[0]) / (iterator + 1)\n",
    "            statistics_dict[\"mess_plus/energy\"] = total_energy\n",
    "            statistics_dict[\"routellm/router_energy\"] = row[\"energy\"]\n",
    "            statistics_dict[\"routellm/inference_energy\"] = row_benchmark_data[f\"energy_consumption_{chosen_model}\"].values[0]\n",
    "            statistics_dict[\"mess_plus/q_length\"] = Q\n",
    "\n",
    "            if wandb.run is not None:\n",
    "                wandb.log(statistics_dict, step=iterator)\n",
    "\n",
    "            iterator += 1\n",
    "\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "\n",
    "        time.sleep(3)\n"
   ],
   "id": "4392a2289f569878",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T00:55:48.501529Z",
     "start_time": "2025-07-27T00:55:48.499503Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "78e57a2a16b16e23",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
